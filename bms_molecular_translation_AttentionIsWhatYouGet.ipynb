{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "bms_molecular_translation-AttentionIsWhatYouGet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Attention-Is-What-You-Get/blob/main/bms_molecular_translation_AttentionIsWhatYouGet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zujis6hSUc0o"
      },
      "source": [
        "# Attention is What You Get\n",
        "\n",
        "This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition.\n",
        "\n",
        "-----\n",
        "\n",
        "AUTHOR: \n",
        "\n",
        "Mo Venouziou\n",
        "\n",
        "- *Email: mvenouziou@gmail.com*\n",
        "- *LinkedIn: www.linkedin.com/in/movenouziou/*\n",
        "\n",
        "Updates:\n",
        "\n",
        " - *Original Posting: June 2, 2021*\n",
        " - *06/21/21: added TPU support*\n",
        " - *06/17/21: improved training & inference speed. Allows full AIAYN model size on TPU, faster small model training on GPU.*\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvew4SZuM33i"
      },
      "source": [
        "### Our Goal: Predict the \"InChI\" value of any given chemical compound diagram. \n",
        "\n",
        "International Chemical Identifiers (\"InChI values\") are a standardized encoding to describe chemical compounds. They take the form of a string of letters, numbers and deliminators, often between 100 - 400 characters long. \n",
        "\n",
        "The chemical diagrams are provided as PNG files, often of such low quality that it may take a human several seconds to decipher. \n",
        "\n",
        "Label length and image quality become a serious challenge here, because we must predict labels for a very large quantity of images. There are 1.6 million images in the test set abd 2.4 million images available in the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obx8tqy_ICG0"
      },
      "source": [
        "\"\"\"\n",
        "# Example (image, target label) pair\\n\\n'\n",
        "for val in train_ds.unbatch().take(1):\n",
        "    print('Example Label:\\n', val['InChI'].numpy())\n",
        "    print('\\nCorresponding Image:', plt.imshow(val['image'][:,:,0], cmap='binary'))\n",
        "### note: load datasets before running this cell\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTzwOF_khY65"
      },
      "source": [
        "## MODEL STRUCTURE: \n",
        "\n",
        "**Image CNN + Attention Features encoder --> text Attention + (optional )CNN feature layer decoder.**\n",
        "\n",
        "This is a hybrid approach with:\n",
        " \n",
        " - Image Encoder from [*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*](https://proceedings.mlr.press/v37/xuc15.pdf).  Generate image feature vectors using intermediate layer outputs from a pretrained CNN. (Here I use the more modern EfficientNet model (recommended by [*Darien Schettler*](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook)) with fixed weights and a trainable Dense layer for customization.)\n",
        " \n",
        " - T2T encoder-decoder model from [*All You Need is Attention*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Self-attention feature extraction for both encoder and decoder, joint encoder-decoder attention feature interactions, and a dense prediction output block. Includes parameters to control number of encoder / decoder blocks.\n",
        "\n",
        " - ***PLUS*** *(optional):* Decoder Output Blocks placed in Series (not stacked). Increase the number of trainable parameters without adding inference computational complexity, while also allowing decoders to specialize on different regions of the output.\n",
        " \n",
        " - ***PLUS*** *(optional):* Is attention really all you need? Add a convolutional layer to enhance text features before decoder self-attention to experiment with performance differences with and without extra convolutional layer(s). Use of CNN's in NLP comes from [*Convolutional Sequence to Sequence Learning*](http://proceedings.mlr.press/v70/gehring17a.html.)\n",
        "\n",
        " - ***PLUS*** *(optional):* Beam-Search Alternative, an extra decoding layer applied after the full logits prediction has been made. This takes the form of a bidirectional RNN with attention, applied to the full logits sequence. Because a full (initial) prediction has already been made, computations can be parallelized using statefull RNNs. (See more details below.)\n",
        "\n",
        "*Optional features can be enabled/disabled using parameters in my model definitions.*\n",
        "\n",
        "----\n",
        "\n",
        "## NEXT STEPS:\n",
        "\n",
        " - (Low priority, specific to Kaggle's TPU implementation.) Fix \"session.run()\" TPU calls on Kaggle. (It works correctly on Colab.) This severely impacts inference speed on Kaggle.\n",
        "\n",
        " - Experiment with **\"Tokens-to-Token ViT\"** in place of the image CNN. (Technique from [*Training Vision Transformers from Scratch on ImageNet*](https://arxiv.org/pdf/2101.11986.pdf)\n",
        "  \n",
        " - Train my **Beam-search Alternative**. \n",
        "\n",
        "    - Beam search is a technique to modify model predictions to reflect the (local) maximum likelihood estimate. However, it is *very* local in that computation expense increases quickly with the number of character steps taken into account. This is also a hard-coded algorithm, which is somewhat contrary to the philosophy of deep learning.\n",
        "\n",
        "    - A *Beam-search Alternative* would be an extra decoding layer applied *after* the full logits prediction has been made. This might be in the form of a stateful, bidirectional RNN that is computationally parallizable because it is applied to the full logits sequence.\n",
        "\n",
        "    - Need to revamp code to accept main model changes made for TPU support.\n",
        "\n",
        " - Treat the number of convolutional layers (decoder feature extraction) and number of decoders places in series (decoder prediction output) as **new hyperparameters** to tune.\n",
        "\n",
        " - *6/21/21: TPU Support added* ~~Implement TPU compatability.~~\n",
        "\n",
        " - *6/17/21: Increased model size and efficiency. * ~~ Implement full size model (matching AISYN) with efficient training and inference speeds for the large dataset. (TPU required. GPU doesn't have enough memory to train such a large model)~~\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htI5m-W2uJmx"
      },
      "source": [
        "### CITATIONS\n",
        "\n",
        "- \"Attention is All You Need.\" \n",
        " - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS (2017). *https://research.google/pubs/pub46201/*\n",
        "\n",
        "- \"Convolutional Sequence to Sequence Learning.\"\n",
        " \n",
        "  - Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y.N.. (2017). Convolutional Sequence to Sequence Learning. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:1243-1252, *http://proceedings.mlr.press/v70/gehring17a.html.*\n",
        "\n",
        "\n",
        "- \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\"\n",
        " \n",
        "  - Mingxing Tan, Quoc V. Le (2019). Convolutional Sequence to Sequence Learning. International Conference on Machine Learning. *http://arxiv.org/abs/1905.11946.*\n",
        "\n",
        "\n",
        "-  \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\"\n",
        "  -  Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. & Bengio, Y.. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2048-2057. *http://proceedings.mlr.press/v37/xuc15.html.* \n",
        "            \n",
        "\n",
        "- \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\"\n",
        "\n",
        "  - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Preprint (2021). *https://arxiv.org/abs/2101.11986*.\n",
        "\n",
        "- Tensorflow documentation tutorial \"Transformer model for language understanding.\" I found this after fully completing the model and found the attention mask was incorrect. My use of \"tf.linalg.band_part\" (only) is due to this tutorial. *www.tensorflow.org/text/tutorials/transformer#masking*\n",
        "\n",
        "- Special thanks to [Darien Schettler](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook.) for leading readers to the \"Show\" and \"Attention\" papers cited above, using *session.run()* to improve inference speed in distributed settings and providing detailed info on creating TF Records. This work is otherwise derived independently from his.\n",
        "\n",
        "- It is possible my idea of a Beam Search Alternative is based on a lecture video from DeepLearning.ai's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  on Coursera.\n",
        "\n",
        "- **Dataset / Kaggle Competition:** \"Bristol-Myers Squibb â€“ Molecular Translation\" competition on Kaggle (2021). *https://www.kaggle.com/c/bms-molecular-translation*\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csshw-ehbeQY"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. [Imports](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=TjuUOVXao__C&line=4&uniqifier=1)\n",
        "2. [Data Pipeline](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=lrLHKs5Ni7Sz)\n",
        "3. [Model Layers](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=W0T-u0vZamI8)\n",
        "    - [InChI Encoding](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=DYApmA2lf1hp&line=1&uniqifier=1)\n",
        "    - [Image Encoding and Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=FESofcGdEaWF&line=1&uniqifier=1)\n",
        "    - [Decoder Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=6qFDs9RTjvod&line=1&uniqifier=1)\n",
        "    - [Joint Encoder-Decoder Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=jP-t1MkKnD5L)\n",
        "    - [Decoder Head (Prediction Output)](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=38GA7wtNEhqW&line=1&uniqifier=1)\n",
        "    - [Update Mechanism](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=_2UR1DLljD0S&line=1&uniqifier=1)\n",
        "4. [Full Model](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=D6GIs3f3rpu0&line=1&uniqifier=1)\n",
        "5. [Training](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=otxdN02mf1ht&line=1&uniqifier=1)\n",
        "6. [Inference](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=Sbvzr5rdmjgs&line=5&uniqifier=1)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjuUOVXao__C"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# system management\n",
        "import os\n",
        "os.environ['TF_ENABLE_ONEDNN.OPTS'] = '1'  # Intel's TF optimization\n",
        "\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.data import TFRecordDataset\n",
        "from tensorflow.data.experimental import TFRecordWriter\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "\n",
        "# metric for Kaggle Competition\n",
        "!pip install -q leven\n",
        "from leven import levenshtein\n",
        "\n",
        "# Kaggle (for TPU)\n",
        "#from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "\n",
        "# Tensorboard Profiler\n",
        "!pip install -U -q tensorboard\n",
        "!pip install -U -q tensorboard_plugin_profile\n",
        "!pip install --upgrade -q \"cloud-tpu-profiler>=2.3.0\"\n",
        "%load_ext tensorboard\n",
        "\n",
        "\"\"\"\n",
        "# Debugger\n",
        "tf.debugging.experimental.enable_dump_debug_info('./logs/', tensor_debug_mode=\"FULL_HEALTH\", \n",
        "                                                 circular_buffer_size=-1)\n",
        "\"\"\"\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUS3jc1o1Nq8"
      },
      "source": [
        "## Model parameters\n",
        "\n",
        "The 'ModelParameters' class manages global hyperparamaters for portability between Colab and Kaggle notebook environments. Once set, all other cells will run on either platform.\n",
        "\n",
        "On Colab, connection to my personal Google Drive is required, as ModelParameters will extract the dataset from a zip file to the hosted environment. This process may take several minutes. (It would not be difficult for the reader to update the code to point to their own drive and download the zip dataset using the Kaggle API code below.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "q6FUA_lvrd4Y",
        "outputId": "3eaf88b4-f4c5-4cfa-a2c2-402ee8b0124f"
      },
      "source": [
        "\"\"\" Kaggle api for download the compressed dataset from Kaggle's servers.\n",
        "\n",
        "# imports\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "\n",
        "# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\n",
        "# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Kaggle api for download the compressed dataset from Kaggle's servers.\\n\\n# imports\\n!pip uninstall -y kaggle\\n!pip install --upgrade pip\\n!pip install kaggle==1.5.6\\n\\n# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\\n# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\\nos.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7okJ4Do3bIZQ",
        "outputId": "b95a4b6c-effa-4526-9247-7f7a77dc03c6"
      },
      "source": [
        "# check for TPU & initialize\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "    STRATEGY = tf.distribute.TPUStrategy(resolver)\n",
        "    TPU = True\n",
        "    os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"  # for TF Hub models on TPU\n",
        "    PRECISION_TYPE = 'mixed_bfloat16' \n",
        "    #PRECISION_TYPE = 'float32' \n",
        "\n",
        "    # extra imports for GCS\n",
        "    !pip install -q fsspec\n",
        "    !pip install -q gcsfs\n",
        "    import fsspec, gcsfs \n",
        "\n",
        "except:\n",
        "    TPU = False\n",
        "    STRATEGY = tf.distribute.get_strategy()\n",
        "    PRECISION_TYPE = 'mixed_float16'\n",
        "\n",
        "# enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy(PRECISION_TYPE)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.6.229.10:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.6.229.10:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU')]\n",
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTkL38FRtUfQ"
      },
      "source": [
        "class ModelParameters:\n",
        "    def __init__(self, cloud_server='kaggle'):\n",
        "               \n",
        "        # universal parameters\n",
        "        self._batch_size = 16  # used on GPU. TPU batch size increased below\n",
        "        self._padded_length = 200\n",
        "        self._image_size = (320, 320)  # shape to process images in data pipeline, matches HUB model\n",
        "        self.SOS_string = 'InChI=1S/'  # start of sentence value\n",
        "        self.EOS_string = '<EOS>'  # end of sentence value\n",
        "        self._strategy = STRATEGY\n",
        "        self._precision_type = PRECISION_TYPE\n",
        "        self._tpu = TPU\n",
        "        \n",
        "        # TPU batch size\n",
        "        if self._tpu:\n",
        "            self._batch_size = 128 * self._strategy.num_replicas_in_sync\n",
        "\n",
        "        # File Paths       \n",
        "        if cloud_server == 'colab':  # Google Colab\n",
        "            \n",
        "            # load drive for saving checkpoints\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount('/content/gdrive/') \n",
        "            except:\n",
        "                pass  # drive already mounted\n",
        "            \n",
        "            # check for TPU \n",
        "            if self._tpu: \n",
        "\n",
        "                # TPU file structure (via Kaggle GCS folder)\n",
        "                self._dataset_dir = 'gs://kds-df3031ee4e277d641d1044cc3e9386a923ca98833b0d51a2575d2932' # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "                self._prepared_files_dir = 'gs://kds-96b617b700ddb4d07bc42a47c0a7abfe3a68d4510c459b7cd7b216e6'  # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('periodic-table')\n",
        "                self._tfrec_dir = 'gs://kds-dc74fe0494d010e8c9544cd7fff86e64f08cb0cffd4c608156ff3f41'  # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('bmsshards')\n",
        "                self._checkpoint_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/checkpoints/'  # gdrive\n",
        "                self._load_checkpoint_dir = self._checkpoint_dir\n",
        "                self._csv_save_dir = './'\n",
        "\n",
        "            else:\n",
        "                # unzip data\n",
        "                if not os.path.isdir('/content/bms-molecular-translation'):\n",
        "                    !unzip -q /content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/bms-molecular-translation.zip -d '/content/bms-molecular-translation'\n",
        "                \n",
        "                # file paths\n",
        "                self._dataset_dir = 'bms-molecular-translation/'\n",
        "                self._prepared_files_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/'\n",
        "                self._checkpoint_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/checkpoints/'\n",
        "                self._load_checkpoint_dir = self._checkpoint_dir\n",
        "                self._csv_save_dir = self._prepared_files_dir \n",
        "                self._tfrec_dir = None\n",
        "                \n",
        "        elif cloud_server == 'kaggle': # Kaggle cloud notebook (CPU / GPU)\n",
        "            from kaggle_datasets import KaggleDatasets\n",
        "            \n",
        "            # check for TPU \n",
        "            if self._tpu: \n",
        "                \n",
        "                # file paths\n",
        "                self._dataset_dir = '' #KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "                self._prepared_files_dir = KaggleDatasets().get_gcs_path('periodic-table')\n",
        "                self._tfrec_dir = KaggleDatasets().get_gcs_path('bmsshards')\n",
        "                self._checkpoint_dir = './'\n",
        "                self._load_checkpoint_dir = './'\n",
        "                self._csv_save_dir = './'\n",
        "\n",
        "            # set GPU instance info\n",
        "            else:  \n",
        "                # file paths\n",
        "                self._dataset_dir = '../input/bms-molecular-translation/'\n",
        "                self._prepared_files_dir = '../input/periodic-table/'\n",
        "                self._tfrec_dir = '../input/bmsshards/'\n",
        "                self._checkpoint_dir = './'\n",
        "                self._load_checkpoint_dir = '../input/k/mvenou/bms-molecular-translation/checkpoints/'\n",
        "                self._csv_save_dir = './'\n",
        "                self._tfrec_dir = None\n",
        "\n",
        "        # common file paths\n",
        "        self._periodic_table_csv = os.path.join(self._prepared_files_dir, 'periodic_table_elements.csv')\n",
        "        self._vocab_csv = os.path.join(self._prepared_files_dir, 'vocab.csv')        \n",
        "        self._test_images_dir = os.path.join(self._dataset_dir, 'test/')\n",
        "        self._train_images_dir = os.path.join(self._dataset_dir, 'train/')\n",
        "        self._extra_labels_csv = os.path.join(self._dataset_dir, 'extra_approved_InChIs.csv')\n",
        "        self._train_labels_csv = os.path.join(self._dataset_dir, 'train_labels.csv')\n",
        "        self._sample_submission_csv = os.path.join(self._dataset_dir, 'sample_submission.csv')\n",
        "        \n",
        "    # functions to access params\n",
        "    def padded_length(self):\n",
        "        return self._padded_length\n",
        "    def mixed_precision(self):\n",
        "        return self._precision_type\n",
        "    def tpu(self):\n",
        "        return self._tpu\n",
        "    def tfrec_dir(self):\n",
        "        return self._tfrec_dir\n",
        "    def cloud_server(self):\n",
        "        return self._cloud_server\n",
        "    def strategy(self):\n",
        "        return self._strategy\n",
        "    def csv_save_dir(self):\n",
        "        return self._csv_save_dir\n",
        "    def train_labels_csv(self):\n",
        "        return self._train_labels_csv\n",
        "    def vocab_csv(self):\n",
        "        return self._vocab_csv\n",
        "    def periodic_table_csv(self):\n",
        "        return self._periodic_table_csv\n",
        "    def batch_size(self):\n",
        "        return self._batch_size  \n",
        "    def image_size(self):\n",
        "        return self._image_size    \n",
        "    def SOS(self):\n",
        "        return self.SOS_string\n",
        "    def EOS(self):\n",
        "        return self.EOS_string\n",
        "    def train_images_dir(self):\n",
        "        return self._train_images_dir\n",
        "    def test_images_dir(self):\n",
        "        return self._test_images_dir   \n",
        "    def checkpoint_dir(self):\n",
        "        return self._checkpoint_dir\n",
        "    def load_checkpoint_dir(self):\n",
        "        return self._load_checkpoint_dir\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sFuCtyTUQwh"
      },
      "source": [
        "Initialize Parameter Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a301N_Gf1hl",
        "outputId": "77918a9d-477f-4ae8-da4d-7ba451ee8119"
      },
      "source": [
        "PARAMETERS = ModelParameters(cloud_server='colab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLHKs5Ni7Sz"
      },
      "source": [
        "# **Input Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgq0W_hZUX-r"
      },
      "source": [
        "Load train labels as DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21sUOSyff1hm"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "    # Load CSV as dataframe\n",
        "    train_labels_df = pd.read_csv(PARAMETERS.train_labels_csv())\n",
        "    train_labels_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1PolKXhNOy"
      },
      "source": [
        "### InChI Text Parsing\n",
        "\n",
        "We split each InChI label into its \"vocabulary\" of logical subunits, consisting of element abbreviations numbers, common symbols and the required string 'InChI=1S/', which is at the start of every InChI label. We want to narrow down this vocabulary to the smallest set represented in our training data. The functions below provide a system for finding this minimal set, as well as preparing a new CSV file with parsed labels ready to be fed into a tokenizer layer.\n",
        "\n",
        "(For clarity and to reduce reliance on loading external files, the true code has been commented out and replaced with corresponding hard-coded values.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQiYkMHIrXmo"
      },
      "source": [
        "def inchi_parsing_regex(parameters=PARAMETERS):\n",
        "    # regex for spliting on InChi, but preserving chemical element abbreviations and three-digit numbers\n",
        "    \n",
        "    # shortcut: hard coded values\n",
        "    vocab = [parameters.EOS(), parameters.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'Br', 'B', 'Cl', 'C', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'Si', 'S', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "    vocab += [str(num) for num in reversed(range(168))]\n",
        "    vocab = [re.escape(val) for val in vocab]\n",
        "       \n",
        "    \"\"\" # to create vocab from scratch, use:\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # load list of elements we should search for within InChI strings: \n",
        "    periodic_elements = pd.read_csv(PARAMETERS.periodic_table_csv(), header=None)[1].to_list()\n",
        "    periodic_elements = periodic_elements + [val.lower() for val in periodic_elements] + [val.upper() for val in periodic_elements]\n",
        "    \n",
        "    punctuation = list(string.punctuation)\n",
        "    punctuation = [re.escape(val) for val in punctuation]   # update values with regex escape chars added as needed\n",
        "\n",
        "    three_dig_nums_list = [str(i) for i in range(1000, -1, -1)]\n",
        "\n",
        "    vocab = [SOS, EOS] + periodic_elements + three_dig_nums_list + punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    split_elements_regex = rf\"({'|'.join(vocab)})\"\n",
        "    \n",
        "    return split_elements_regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_57mCRgimRj"
      },
      "source": [
        "INCHI_PARSING_REGEX = inchi_parsing_regex()\n",
        "\n",
        "def parse_InChI(texts, parsing_regex=INCHI_PARSING_REGEX):  \n",
        "    return ' '.join(re.findall(parsing_regex, texts))\n",
        "\n",
        "\n",
        "# TF dataset map-compatible version\n",
        "def parse_InChI_py_fn(texts, parsing_regex=INCHI_PARSING_REGEX):\n",
        "    def tf_parse_InChI(texts):  \n",
        "        texts = np.char.array(texts.numpy())\n",
        "        texts = np.char.decode(texts).tolist()\n",
        "        texts = tf.constant([parse_InChI(val) for val in texts])\n",
        "        return tf.squeeze(texts)\n",
        "    return tf.py_function(func=tf_parse_InChI, inp=[texts], Tout=tf.string)\n",
        "\n",
        "\n",
        "# extracts filepath from image name\n",
        "def path_from_image_id(x, root_folder):\n",
        "    folder_a = tf.strings.substr(x, pos=0, len=1)\n",
        "    folder_b = tf.strings.substr(x, pos=1, len=1)\n",
        "    folder_c = tf.strings.substr(x, pos=2, len=1)\n",
        "    filename =  tf.strings.join([x, '.png'])\n",
        "    return tf.strings.join([root_folder, folder_a, folder_b, folder_c, filename], separator='/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcP1tFRrvyR"
      },
      "source": [
        "Tokenizer\n",
        "\n",
        "Note: This must be kept outside the model (and used in dataset prep) for TPU compatability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDY6kY8Mf1hq"
      },
      "source": [
        "def Tokenizer(parameters):\n",
        "    \"\"\" tokenizes, crops & pads to parameters.padded_length() \"\"\"\n",
        "\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    padded_length = PARAMETERS.padded_length()\n",
        "    \n",
        "    # Create vocabulary for tokenizer\n",
        "    def create_vocab():       \n",
        "        hard_coded_vocab = [PARAMETERS.EOS(), PARAMETERS.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'B', 'Br',  'C', 'Cl', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'S', 'Si', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "        numbers = [str(num) for num in range(168)]\n",
        "        \n",
        "        vocab = hard_coded_vocab + numbers\n",
        "        \n",
        "        \"\"\"\n",
        "        # get from saved file\n",
        "        vocab = pd.read_csv(PARAMETERS.vocab_csv())['vocab_value'].to_list()   \n",
        "        vocab = list(vocab)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" \n",
        "        # To create from scratch, extract all vocab elements appearing in train set:\n",
        "        df = pd.read_csv(PARAMETERS.train_labels_csv())  \n",
        "        seg_len = 250000\n",
        "        num_breaks = len(df) // seg_len\n",
        "\n",
        "        vocab = set()\n",
        "        for i in range(num_breaks):\n",
        "\n",
        "            df_i =  df['InChI'].iloc[seg_len * i: seg_len * (i+1)]\n",
        "            texts =  df_i.apply(lambda x: set(parse_InChI(x).split()))\n",
        "            texts = texts.tolist()\n",
        "\n",
        "            vocab = vocab.union(*texts)\n",
        "\n",
        "            print(f'completed {i} / {num_breaks}')\n",
        "\n",
        "        vocab = list(vocab)\n",
        "        vocab_df = pd.DataFrame({'vocab_value': vocab})\n",
        "\n",
        "        # save results\n",
        "        filename = os.path.join(PARAMETERS.csv_save_dir(), 'vocab.csv')\n",
        "        vocab_df.to_csv(filename, index=False)\n",
        "        \"\"\"\n",
        "               \n",
        "        return vocab\n",
        "\n",
        "    vocab = create_vocab()\n",
        "    \n",
        "    # create tokenizer\n",
        "    tokenizer_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        standardize=None, split=lambda x: tf.strings.split(x, sep=' ', maxsplit=-1), \n",
        "        output_mode='int', output_sequence_length=padded_length, vocabulary=vocab)\n",
        "\n",
        "    # record EOS token\n",
        "    tokenized_EOS = tokenizer_layer(tf.constant([EOS]))\n",
        "    \n",
        "    # create inverse (de-tokenizer)\n",
        "    inverse_tokenizer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=vocab, invert=True)\n",
        "\n",
        "    return tokenizer_layer, inverse_tokenizer, tokenized_EOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6ghpYe6l7W"
      },
      "source": [
        "TOKENIZER_LAYER, INVERSE_TOKENIZER, TOKENIZED_EOS = \\\n",
        "    Tokenizer(parameters=PARAMETERS)\n",
        "\n",
        "def tokenize_text(w, x, y, z):\n",
        "    # note: requires batch dim\n",
        "    y = TOKENIZER_LAYER(y)\n",
        "    return w, x, y, z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rk6HKexVmY-"
      },
      "source": [
        "Image Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4VcUWGHVB7z"
      },
      "source": [
        "# Image loaders\n",
        "def load_image(image_path):\n",
        "    image_path = tf.squeeze(image_path)\n",
        "    image = keras.layers.Lambda(lambda x: tf.io.read_file(x))(image_path)\n",
        "    return image   \n",
        "\n",
        "def decode_image(image, target_size):\n",
        "    image = keras.layers.Lambda(lambda x: tf.io.decode_image(x, channels=1, expand_animations=False))(image)\n",
        "    image = keras.layers.experimental.preprocessing.Resizing(*target_size)(image)\n",
        "    return image    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2ecfS_hR9k"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Here we create efficient tf.data.Dataset train / validation / test sets.\n",
        "\n",
        "Out data pipeline will read our prepared CSV of (image filename, parsed InChI and standard InChI) tuples. (If this file is not found, it will be created from scratch. This may take several minutes)  Iterating through the list, it will load batches of corresponding images and labels.\n",
        "\n",
        "Our datasets contain the following information, accessible by dict keys: images, image_id, InChI, parsed_InChI. (The test set uses InChI = parsed_InChI = 'InChI=1S/', the known required stating value for any InChI code.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDRJElyOf1hn"
      },
      "source": [
        "def data_generator(image_set, parameters=PARAMETERS, labels_df=None, decode_images=True):\n",
        "       \n",
        "    # get global params\n",
        "    batch_size = parameters.batch_size()\n",
        "    target_size = parameters.image_size()\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # dataset options\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_optimization.autotune_buffers = True\n",
        "    options.experimental_optimization.map_vectorization.enabled = True\n",
        "    options.experimental_optimization.apply_default_optimizations = True\n",
        "        \n",
        "    # Train & Validation Datasets\n",
        "    if image_set in ['train', 'valid']:\n",
        "        root_folder = parameters.train_images_dir()  # train / valid images\n",
        "        valid_split = 0.10\n",
        "        \n",
        "        # load labels into memory as dataframe\n",
        "        if labels_df is None:\n",
        "            labels_df = pd.read_csv(parameters.train_labels_csv())\n",
        "\n",
        "        # test / train split\n",
        "        num_valid_samples = int(valid_split * len(labels_df))\n",
        "        train_df = labels_df.iloc[num_valid_samples: ]  # get train split\n",
        "        valid_df = labels_df.iloc[: num_valid_samples]  # get validation split\n",
        "\n",
        "        # shuffle\n",
        "        train_df = train_df.sample(frac=1)\n",
        "        valid_df = valid_df.sample(frac=1)\n",
        "\n",
        "        # load into datasets  # (image_id, InChI)\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices(train_df.values)\n",
        "        valid_ds = tf.data.Dataset.from_tensor_slices(valid_df.values)\n",
        "\n",
        "        train_ds = train_ds.with_options(options)\n",
        "        valid_ds = valid_ds.with_options(options)\n",
        "\n",
        "        # update image paths  \n",
        "        def map_path(x):  # (image_path, image_id, InChI)\n",
        "            image_id = x[0]\n",
        "            image_path = path_from_image_id(image_id, root_folder)\n",
        "            return image_path, x[0], x[1]\n",
        "\n",
        "        train_ds = train_ds.map(map_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        def map_parse(x, y, z):  # (image_path, image_id, InChI)\n",
        "            parsed_InChI = parse_InChI_py_fn(z)\n",
        "            return x, y, parsed_InChI, z\n",
        "   \n",
        "        train_ds = train_ds.map(map_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                \n",
        "        # load images into dataset       \n",
        "        def open_images(w, x, y, z):\n",
        "            w = load_image(w)\n",
        "            return w, x, y, z\n",
        "        \n",
        "        train_ds = train_ds.map(open_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(open_images, num_parallel_calls=tf.data.AUTOTUNE)    \n",
        "\n",
        "        # PREFETCH dataset BEFORE decoding images\n",
        "        train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "        def decode(w, x, y, z):\n",
        "            w = decode_image(w, target_size)\n",
        "            return w, x, y, z\n",
        "\n",
        "        if decode_images:\n",
        "            train_ds = train_ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            valid_ds = valid_ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)    \n",
        "\n",
        "        # BATCH dataset AFTER decoding images (required by tf.io)\n",
        "        # should batch before other pure TF Lambda layer ops\n",
        "        train_ds = train_ds.batch(batch_size, drop_remainder=True)\n",
        "        valid_ds = valid_ds.batch(batch_size, drop_remainder=True)\n",
        "        \n",
        "        # add extra \"EOS\" values to end of parsed inchi\n",
        "        def extend_EOS(w, x, y, z):\n",
        "            y = tf.strings.join([y, EOS, EOS, EOS, EOS, EOS], separator=' ')\n",
        "            y = tf.reshape(y, [-1])\n",
        "            return w, x, y, z\n",
        "\n",
        "        train_ds = train_ds.map(extend_EOS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(extend_EOS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Tokenize parsed_inchi.  Note: ds must be batched before this step (size=1 is ok) \n",
        "        train_ds = train_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # name the elements\n",
        "        def map_names(w, x, y, z):\n",
        "            return  {'image': w, 'image_id': x, 'tokenized_InChI': y, 'InChI': z}\n",
        "        \n",
        "        train_ds = train_ds.map(map_names, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_names, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        return train_ds, valid_ds\n",
        "    \n",
        "    # Test Dataset\n",
        "    elif image_set == 'test':\n",
        "\n",
        "        # note: image resizing and batching done during this loading step\n",
        "        # other elements must be batched before combining\n",
        "        image_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            directory=parameters.test_images_dir(), labels='inferred', label_mode=None,\n",
        "            class_names=None, color_mode='grayscale', batch_size=1, \n",
        "            image_size=target_size, shuffle=False, seed=None, validation_split=None, \n",
        "            subset=None, follow_links=False)\n",
        "\n",
        "        # set filenames as label and batch\n",
        "        image_id_ds = tf.data.Dataset.from_tensor_slices(image_ds.file_paths)\n",
        "        image_id_ds = image_id_ds.map(lambda x: tf.strings.split(x, os.path.sep)[-1],\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # prepare images for TF Records creations. \n",
        "        # Note: do this step AFTER filenames step\n",
        "        if decode_images is False:  \n",
        "            # convert image to raw byte string. Note: cannot have batch dim for encoding\n",
        "            image_ds = image_ds.unbatch()\n",
        "            image_ds = image_ds.map(lambda x: tf.cast(x, dtype=tf.uint16))\n",
        "            image_ds = image_ds.map(lambda image: tf.io.encode_png(image))\n",
        "            image_ds = image_ds.map(lambda image: tf.io.serialize_tensor(image))\n",
        "            \n",
        "        # dataset consisting solely of InChI start 'InChI=1S/'\n",
        "        inchi_ds = image_id_ds.map(lambda x: tf.constant(SOS, dtype=tf.string),\n",
        "                                   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # merge datasets\n",
        "        test_ds = tf.data.Dataset.zip((image_ds, image_id_ds, inchi_ds, inchi_ds))\n",
        "        \n",
        "        # prefetch\n",
        "        test_ds = test_ds.prefetch(tf.data.AUTOTUNE)\n",
        "        test_ds = test_ds.batch(batch_size)\n",
        "\n",
        "        # Tokenize parsed_inchi.  Note: ds must be batched before this step (size=1 is ok) \n",
        "        test_ds = test_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # set key names\n",
        "        def map_names(w, x, y, z):\n",
        "            return  {'image': w, 'image_id': x, 'tokenized_InChI': y, 'InChI': z}\n",
        "        \n",
        "        test_ds = test_ds.map(map_names, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "\n",
        "\n",
        "        return test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCaO3A_5f1ho"
      },
      "source": [
        "Create Test, Train and Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvHSKTapob8V"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "    train_ds, valid_ds = data_generator('train', parameters=PARAMETERS, labels_df=train_labels_df, decode_images=True)\n",
        "    #test_ds = data_generator('test', parameters=PARAMETERS, labels_df=None, decode_images=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juAkO-oQaUml"
      },
      "source": [
        "Examine data shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKbpsKKrf1hp"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "\n",
        "    print('Train DS')\n",
        "    for val in train_ds.take(1):    \n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    print('\\nValidation DS')\n",
        "    for val in valid_ds.take(1):\n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    try:\n",
        "        print('\\nTest DS')\n",
        "        for val in test_ds.take(1):\n",
        "            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCYSm7N74i7"
      },
      "source": [
        "### TF Records Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgYgaPoO4gp"
      },
      "source": [
        "# Create TF Examples\n",
        "def make_example(image, image_id, tokenized_InChI, InChI):\n",
        "    image_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[image.numpy()])  # image provided as raw bytestring\n",
        "    )\n",
        "    image_id_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[image_id.numpy()])\n",
        "    )\n",
        "    tokenized_InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(tokenized_InChI).numpy()])\n",
        "    )\n",
        "    InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[InChI.numpy()])\n",
        "    )\n",
        "\n",
        "    features = tf.train.Features(feature={\n",
        "        'image': image_feature,\n",
        "        'image_id': image_id_feature,\n",
        "        'tokenized_InChI': tokenized_InChI_feature,\n",
        "        'InChI': InChI_feature\n",
        "    })\n",
        "    \n",
        "    example = tf.train.Example(features=features)\n",
        "\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "def make_example_py_fn(image, image_id, InChI, tokenized_InChI):\n",
        "    return tf.py_function(func=make_example, \n",
        "                   inp=[image, image_id, InChI, tokenized_InChI], \n",
        "                   Tout=tf.string)\n",
        "\n",
        "\n",
        "# Decode TF Examples\n",
        "def decode_example(example, parameters=PARAMETERS):        \n",
        "    feature_description = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'image_id': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'tokenized_InChI': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'InChI': tf.io.FixedLenFeature([], tf.string)}\n",
        "    \n",
        "    values = tf.io.parse_single_example(example, feature_description)\n",
        "    \n",
        "    \n",
        "    values['image'] = decode_image(values['image'], parameters.image_size())\n",
        "    values['tokenized_InChI'] = tf.io.parse_tensor(values['tokenized_InChI'],\n",
        "                                                  out_type=tf.int64)\n",
        "    values['tokenized_InChI'] = tf.cast(values['tokenized_InChI'], tf.int32)\n",
        "    \n",
        "    return values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hURRE1cnGOTD"
      },
      "source": [
        "def serialized_dataset_gen(parameters=PARAMETERS, set_type='train', labels_df=None):\n",
        "    \n",
        "    if set_type == 'train':\n",
        "        train_ds, valid_ds = data_generator(image_set='train', \n",
        "                                            parameters=parameters, \n",
        "                                            labels_df=train_labels_df, \n",
        "                                            decode_images=False)  # output images as bytestrings\n",
        "\n",
        "        train_ds = train_ds.unbatch()\n",
        "        valid_ds = valid_ds.unbatch()\n",
        "\n",
        "        # Create TF Examples\n",
        "        train_ds = train_ds.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['tokenized_InChI'], x['InChI']), \n",
        "                                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['tokenized_InChI'], x['InChI']), \n",
        "                                num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        return train_ds, valid_ds\n",
        "    \n",
        "    else: #test_set:\n",
        "        test_ds = data_generator(image_set='test', \n",
        "                                 parameters=parameters, \n",
        "                                 labels_df=None, \n",
        "                                 decode_images=False)  # output images as bytestrings\n",
        "        \n",
        "        test_ds = test_ds.unbatch()\n",
        "            \n",
        "        # Create TF Examples\n",
        "        test_ds = test_ds.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['tokenized_InChI'], x['InChI']), \n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        return test_ds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPDDDbqDj6BV"
      },
      "source": [
        "# Create TF Record Shards\n",
        "\"\"\"\n",
        "NOTE: Changes have been made to the other dataset pipeline functions. \n",
        "Test / Revise this for compatability before running.\n",
        "\"\"\"\n",
        "def create_records(dataset, subset, num_shards):\n",
        "    \n",
        "    folder = subset + '_tfrec'\n",
        "    \n",
        "    if subset =='train':\n",
        "        num_samples = int(.9 * len(train_labels_df))    # test / valid split\n",
        "    elif subset == 'valid':\n",
        "        num_samples = int(.1 * len(train_labels_df))\n",
        "    else:\n",
        "        num_samples = 2000000\n",
        "\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "        \n",
        "    for shard_num in range(num_shards):\n",
        "        \n",
        "        filename = os.path.join(folder, f'{subset}_shard_{shard_num+1}')\n",
        "        try:\n",
        "            this_shard = dataset.skip(shard_num * num_samples//num_shards).take(num_samples//num_shards)\n",
        "        \n",
        "            print(f'Writing shard {shard_num+1}/{num_shards} to {filename}')\n",
        "            writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "            writer.write(this_shard)\n",
        "        except:\n",
        "            break\n",
        "    return None \n",
        "    \n",
        "# Load dataset from saved TF Record Shards\n",
        "def dataset_from_records(subset, parameters=PARAMETERS):\n",
        "\n",
        "    # optimizations\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_optimization.autotune_buffers = True\n",
        "    options.experimental_optimization.map_vectorization.enabled = True\n",
        "    options.experimental_optimization.apply_default_optimizations = True\n",
        "\n",
        "    filepath = os.path.join(parameters.tfrec_dir(), \n",
        "                            subset + '_tfrec/*')\n",
        "\n",
        "    dataset = tf.data.Dataset.list_files(filepath)  # put all tf rec filenames in a ds\n",
        "    dataset = dataset.shuffle(10**6)\n",
        " \n",
        "    # merge the files\n",
        "    num_readers = parameters.strategy().num_replicas_in_sync\n",
        "    dataset = dataset.interleave(tf.data.TFRecordDataset,  \n",
        "                                 cycle_length=num_readers, block_length=1,\n",
        "                                 deterministic=False, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    dataset = dataset.shuffle(10**6)\n",
        "    \n",
        "    # decode examples\n",
        "    dataset = dataset.map(decode_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # note: tokenized InChI element spec needs help determining shape\n",
        "    for val in dataset.take(1):\n",
        "        padded_length = val['tokenized_InChI'].shape[-1]\n",
        "\n",
        "    # coerce unknown shape\n",
        "    dataset = dataset.map(lambda x: {'image':x['image'],\n",
        "                                     'image_id': x['image_id'],\n",
        "                                     'tokenized_InChI': tf.reshape(x['tokenized_InChI'], [padded_length]),\n",
        "                                     'InChI': x['InChI']},\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)  \n",
        "\n",
        "    dataset = dataset.batch(parameters.batch_size(), drop_remainder=True)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VxuZInXxj8Fn",
        "outputId": "1d7ca4a1-665d-4610-89a7-24bd48ebc41b"
      },
      "source": [
        "# To create TF_Records files\n",
        "# note: can take 8+ hours for train set alone!\n",
        "\"\"\"\n",
        "serial_train_ds, serial_valid_ds = serialized_dataset_gen(parameters=PARAMETERS, set_type='train', labels_df=train_labels_df)\n",
        "serial_test_ds = serialized_dataset_gen(parameters=PARAMETERS, set_type='test', labels_df=None)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nserial_train_ds, serial_valid_ds = serialized_dataset_gen(parameters=PARAMETERS, set_type='train', labels_df=train_labels_df)\\nserial_test_ds = serialized_dataset_gen(parameters=PARAMETERS, set_type='test', labels_df=None)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SeQrwPyh1Ok",
        "outputId": "9d04a9a7-4fb9-4b6f-8661-d1602ea93f38"
      },
      "source": [
        "# IF USING TF_RECORDS:\n",
        "#NOTE: If no dataset loads, check if Kaggle GCS directories have changed. (This happends periodically)\n",
        "if PARAMETERS.tpu():\n",
        "    with PARAMETERS.strategy().scope(): \n",
        "        train_ds = dataset_from_records('train', parameters=PARAMETERS)\n",
        "        valid_ds = dataset_from_records('valid', parameters=PARAMETERS)\n",
        "\n",
        "    print('Train DS')\n",
        "    for val in train_ds.take(1):    \n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    print('\\nValidation DS')\n",
        "    for val in valid_ds.take(1):\n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    try:\n",
        "        print('\\nTest DS')\n",
        "        for val in test_ds.take(1):\n",
        "            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train DS\n",
            "image: (1024, 320, 320, 1) image_id: (1024,) InChI: (1024,) tokenized_InChI: (1024, 200)\n",
            "\n",
            "Validation DS\n",
            "image: (1024, 320, 320, 1) image_id: (1024,) InChI: (1024,) tokenized_InChI: (1024, 200)\n",
            "\n",
            "Test DS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0T-u0vZamI8"
      },
      "source": [
        "# **Model Layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYApmA2lf1hp"
      },
      "source": [
        "## InChI Encoding\n",
        "\n",
        "Tokenizer and Embedding to convert parsed InChI strings to tensors of numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMv38YQzf1hq"
      },
      "source": [
        "InChI Input Prep Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwhCP06Wf1hq"
      },
      "source": [
        "def InchiPrep(batch_dim, padded_length, max_len, embedding_dim, vocab_size, name='InchiPrep'):\n",
        "    \"\"\" initial InChI prep step. Handles separation into input / target pair,\n",
        "    including trainable start variable and positional encoding.  \"\"\"\n",
        "        \n",
        "    # embedding layer\n",
        "    token_embedding_layer = keras.layers.Embedding(input_dim=vocab_size, \n",
        "                                                   output_dim=embedding_dim, \n",
        "                                                   mask_zero=False,\n",
        "                                                   input_length=None,  # embedding will also be using on individual token predictions\n",
        "                                                   name='TokenEmbeddingLayer')    \n",
        "    \n",
        "    # inputs\n",
        "    tokenized_inchi = keras.layers.Input([padded_length], name='tokenized_inchi')\n",
        "    positional_encoding = keras.layers.Input([max_len, embedding_dim], name='positional_encoding')\n",
        "    start_var = keras.layers.Input([1, embedding_dim], name='start_var')\n",
        "    \n",
        "    inputs = [tokenized_inchi, positional_encoding, start_var]\n",
        "    \n",
        "    # split into input / target pairs\n",
        "    inchi_target = tokenized_inchi[:, :max_len]\n",
        "    inchi_input = tokenized_inchi[:, :max_len-1]\n",
        "    \n",
        "    # input embedding\n",
        "    inchi_input = token_embedding_layer(inchi_input)\n",
        "\n",
        "    # add start variable\n",
        "    inchi_input = tf.cast(inchi_input, dtype=start_var.dtype)\n",
        "    inchi_input = keras.layers.Concatenate(-2, name='concat_start_var')([start_var, inchi_input])\n",
        "\n",
        "    # Add positional encoding\n",
        "    positional_encoding = tf.cast(positional_encoding, dtype=inchi_input.dtype)\n",
        "    inchi_input = keras.layers.Add(name='add_pos_encoding')([positional_encoding, inchi_input])\n",
        "\n",
        "    # outpus\n",
        "    outputs = [inchi_input, inchi_target]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSjRxGHqDWLV"
      },
      "source": [
        "def InchiIndividalEmbedding(token_embedding_layer, name='InchiIndividalEmbedding'):\n",
        "    \"\"\" used during character-by-character prediction generation loop \"\"\"\n",
        "        \n",
        "    # embedding layer\n",
        "    embedding_dim = token_embedding_layer.output_dim\n",
        "    vocab_size = token_embedding_layer.input_dim\n",
        "    \n",
        "    # inputs\n",
        "    tokenized_inchi = keras.layers.Input([1], name='tokenized_inchi')  #, dtype=tf.int32, name='tokenized_inchi')\n",
        "    positional_encoding_step = keras.layers.Input([embedding_dim], name='positional_encoding_step')\n",
        "\n",
        "    inputs = [tokenized_inchi, positional_encoding_step]\n",
        "   \n",
        "    # input embedding\n",
        "    inchi_input = token_embedding_layer(tokenized_inchi)\n",
        "\n",
        "    # Add positional encoding\n",
        "    inchi_input = keras.layers.Add(name='add_pos_encoding')([positional_encoding_step, inchi_input])\n",
        "    \n",
        "    # outpus\n",
        "    outputs = [inchi_input]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiiKyyo7Ev6J",
        "outputId": "8d4593de-c53b-487c-a741-73daf07e5d2f"
      },
      "source": [
        "temp_inchi_prep = InchiPrep(batch_dim=16, padded_length=TOKENIZER_LAYER.get_config()['output_sequence_length'],\n",
        "        max_len=177, embedding_dim=512, vocab_size=200, name='InchiPrep')\n",
        "temp_inchi_prep.summary()\n",
        "\n",
        "temp_token_embedding_layer = temp_inchi_prep.get_layer('TokenEmbeddingLayer')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"InchiPrep\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "tokenized_inchi (InputLayer)    [(None, 200)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_1 (Sli (None, 176)          0           tokenized_inchi[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "TokenEmbeddingLayer (Embedding) (None, 176, 512)     102400      tf.__operators__.getitem_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "positional_encoding (InputLayer [(None, 177, 512)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "start_var (InputLayer)          [(None, 1, 512)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast (TFOpLambda)            (None, 176, 512)     0           TokenEmbeddingLayer[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf.cast_1 (TFOpLambda)          (None, 177, 512)     0           positional_encoding[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "concat_start_var (Concatenate)  (None, 177, 512)     0           start_var[0][0]                  \n",
            "                                                                 tf.cast[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_pos_encoding (Add)          (None, 177, 512)     0           tf.cast_1[0][0]                  \n",
            "                                                                 concat_start_var[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem (Slici (None, 177)          0           tokenized_inchi[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 102,400\n",
            "Trainable params: 102,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs6gv3Zbf1hq",
        "outputId": "7d6c662e-9ad9-4664-83d8-1f243997dfdb"
      },
      "source": [
        "InchiIndividalEmbedding(token_embedding_layer=temp_token_embedding_layer, name='InchiIndividalEmbedding').summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"InchiIndividalEmbedding\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "tokenized_inchi (InputLayer)    [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "positional_encoding_step (Input [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "TokenEmbeddingLayer (Embedding) multiple             102400      tokenized_inchi[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_pos_encoding (Add)          (None, 1, 512)       0           positional_encoding_step[0][0]   \n",
            "                                                                 TokenEmbeddingLayer[1][0]        \n",
            "==================================================================================================\n",
            "Total params: 102,400\n",
            "Trainable params: 102,400\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FESofcGdEaWF"
      },
      "source": [
        "# Image Encoder\n",
        "\n",
        "Feature Extraction Step 1: Run the images through a pre-trained image network, extracting features as the output of an intermediate convolutional layer. [Technique from \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,\" cited at the top of this notebook.]  A dense layer is added for transfer learning and to control the dimension of the attention mechanism used later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMyUVvA9jBXp"
      },
      "source": [
        "Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTKFUFKfHeU"
      },
      "source": [
        "def TransferModel(image_shape, name='TransferModel'):\n",
        "    \n",
        "    # Note: temporarily disable mixed precision during load. (Model doesn't handle it properly)\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')  # removed mixed precision\n",
        "\n",
        "    base_transfer_model = keras.applications.EfficientNetB0(\n",
        "                            include_top=False, weights='imagenet', input_tensor=None,\n",
        "                            input_shape=(*image_shape[:2], 3), pooling=None, classes=1000,\n",
        "                            classifier_activation='softmax')\n",
        "\n",
        "    model = keras.Model(inputs=base_transfer_model.inputs, \n",
        "                        outputs=base_transfer_model.get_layer('block7a_project_bn').output, \n",
        "                        name=name)\n",
        "    \n",
        "    # revert to orig mixed precision policy\n",
        "    tf.keras.mixed_precision.set_global_policy(PARAMETERS.mixed_precision()) \n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8TJ1CkvktYy"
      },
      "source": [
        "#TransferModel((224,224,1)).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3OKI_t3f1hr"
      },
      "source": [
        "def ImageEncoder(image_shape, output_dim, use_dense_top, name='ImageEncoder'):\n",
        "\n",
        "    # transfer model\n",
        "    transfer_model = TransferModel(image_shape, name='TransferModel')\n",
        "    \n",
        "    # Input\n",
        "    image = keras.layers.Input(shape=image_shape, name='image')\n",
        "    inputs = [image]\n",
        "\n",
        "    # standardize and apply model\n",
        "    # Note: 'keras.applications.EfficientNet' models expect color image values in [0, 255] with any shape\n",
        "    if image_shape[2] == 1:  # convert to color\n",
        "        image = keras.layers.Lambda(lambda x: tf.image.grayscale_to_rgb(x),\n",
        "                                    name='grayscale_to_rgb')(image)\n",
        "    image = keras.applications.efficientnet.preprocess_input(image)  # this is only a pass-through on efficientnet\n",
        "    image = transfer_model(image)\n",
        "    \n",
        "    # extract feature vector\n",
        "    features_dim = image.shape[3]\n",
        "    num_vectors = image.shape[2] * image.shape[1]\n",
        "    image_features = tf.keras.layers.Reshape(target_shape=[num_vectors, features_dim])(image)\n",
        "    \n",
        "    # Extra customization layer    \n",
        "    if use_dense_top:    \n",
        "        image_features = keras.layers.Dense(output_dim, activation='relu',\n",
        "                                            name='dense')(image_features)\n",
        "\n",
        "    outputs = [image_features]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyEeXgr5f1hr"
      },
      "source": [
        "# ImageEncoder(image_shape=(320, 320,1), output_dim=256, use_dense_top=True).summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGYH_nP3Efsk"
      },
      "source": [
        "## Encoder Attention\n",
        "\n",
        "Feature Extraction Step 2: Now that we have basic feature vectors, we use self-attention to generate more complex features. This is the encoding step used in \"Attention is All You Need,\" cited above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmq0PjxxjayJ"
      },
      "source": [
        "def EncoderAttention(num_blocks, encoder_feature_dim, num_att_elems, name='EncoderAttention'):\n",
        "\n",
        "    \"\"\" note: use num_blocks=6 to match \"Attention is All You Need\" \"\"\"\n",
        "\n",
        "    # inputs\n",
        "    encoder_vectors = keras.layers.Input([num_att_elems, encoder_feature_dim], name='encoder_vectors')   # from image encoder\n",
        "    inputs = [encoder_vectors]\n",
        "\n",
        "    # attention (uses \"Attention is All You Need\" structure, \n",
        "    # except without positional encoding because image feature vectors are unordered)\n",
        "\n",
        "    # shared params (follows 'num_heads * key_dim = units' from paper)\n",
        "    num_heads = 8\n",
        "    key_dim = encoder_feature_dim // 8\n",
        "    \n",
        "    for i in range(num_blocks):\n",
        "\n",
        "        # self-attention block\n",
        "        attention = tf.keras.layers.MultiHeadAttention(\n",
        "                                            num_heads=num_heads, \n",
        "                                            key_dim=key_dim, \n",
        "                                            dropout=.1,\n",
        "                                            name=f'encoder_attention_{i}')(  \n",
        "                                query=encoder_vectors, value=encoder_vectors)\n",
        "        \n",
        "        attention = keras.layers.Add()([encoder_vectors, attention])\n",
        "        attention = keras.layers.LayerNormalization(axis=[1,2], epsilon=1e-6)(attention)    \n",
        "\n",
        "        # Feed Forward Block\n",
        "        encoder_vectors = keras.layers.Dense(encoder_feature_dim * 4, 'relu',\n",
        "                                                name=f'dense_encodeR_{i}')(attention)   \n",
        "        encoder_vectors = keras.layers.Dense(encoder_feature_dim, activation=None,\n",
        "                                                )(encoder_vectors) \n",
        "        encoder_vectors = keras.layers.Dropout(rate=.1)(encoder_vectors)\n",
        "        encoder_vectors = keras.layers.Add()([attention, encoder_vectors])\n",
        "        encoder_vectors = keras.layers.LayerNormalization(\n",
        "                                            axis=[1,2], epsilon=1e-6)(encoder_vectors)     \n",
        "\n",
        "    # output\n",
        "    outputs = [encoder_vectors]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g8x4_cpkJ9A",
        "outputId": "71d158f3-28ed-42e0-fd51-8de2511333cb"
      },
      "source": [
        "EncoderAttention(num_blocks=1, encoder_feature_dim=512, num_att_elems=100).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"EncoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_vectors (InputLayer)    [(None, 100, 512)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_0 (MultiHeadA (None, 100, 512)     1050624     encoder_vectors[0][0]            \n",
            "                                                                 encoder_vectors[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 100, 512)     0           encoder_vectors[0][0]            \n",
            "                                                                 encoder_attention_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization (LayerNorma (None, 100, 512)     102400      add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_encodeR_0 (Dense)         (None, 100, 2048)    1050624     layer_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 100, 512)     1049088     dense_encodeR_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 100, 512)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 100, 512)     0           layer_normalization[0][0]        \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_1 (LayerNor (None, 100, 512)     102400      add_1[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 3,355,136\n",
            "Trainable params: 3,355,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFDs9RTjvod"
      },
      "source": [
        "## Decoder Attention\n",
        "\n",
        "Text Feature extraction + Encoder/Decoder Joint Attention interaction.\n",
        "\n",
        "With use_covolutions set to False, this is the decoder self-attention feature-extraction step from \"Attention is All You Need,\" cited above (with learned positional encoding). \n",
        "\n",
        "Includes an (optional) parameter to add a small convolutional layer for feature enhancement before the attention layer. This is included for experimentation / verification that attention really is all you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m927YThukFs-"
      },
      "source": [
        "def DecoderAttention(num_blocks, encoder_units, decoder_units, num_encoder_vectors, max_len, \n",
        "                     use_convolutions, name='DecoderAttention'):\n",
        "    \n",
        "    \"\"\" note: \"Attention is All You Need\" \"uses num_blocks = 6 \"\"\"\n",
        "\n",
        "    # Inputs\n",
        "    encoder_features = keras.layers.Input([num_encoder_vectors, encoder_units], \n",
        "                                          name='encoder_features')   # from image\n",
        "    decoder_features = keras.layers.Input([max_len, decoder_units], \n",
        "                                          name='decoder_features')   # from known text\n",
        "    \n",
        "    inputs = [encoder_features, decoder_features]\n",
        "\n",
        "    \"\"\"\n",
        "    # (Optional convolution feature extraction, for experimentation) \n",
        "    if use_convolutions:\n",
        "        \n",
        "        # crop to masked input\n",
        "        step = tf.math.argmin(mask[0, :, 0])\n",
        "        decoder_features = decoder_features[:, :step, :]\n",
        "\n",
        "        # apply convolutions\n",
        "        decoder_features = tf.keras.layers.Conv1D(filters=decoder_units, kernel_size=3, \n",
        "                    strides=1, padding='same', groups=1)(decoder_features)\n",
        "\n",
        "        # pad back to full size for (masked) Attention input\n",
        "        decoder_features = tf.pad(decoder_features, [[0,0], [0, max_len - step], [0,0]])\n",
        "    \"\"\"\n",
        "\n",
        "    # shared params (follows 'num_heads * key_dim = units' from paper)\n",
        "    num_heads = 8\n",
        "    key_dim = decoder_units // 8\n",
        "\n",
        "    # create look-ahead mask\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((max_len, max_len)), 0, -1)\n",
        "    \n",
        "    for i in range(num_blocks):\n",
        "      \n",
        "        # decoder self-attention block\n",
        "        decoder_attention = tf.keras.layers.MultiHeadAttention(\n",
        "                                            num_heads=num_heads, \n",
        "                                            key_dim=key_dim, \n",
        "                                            dropout=.1,\n",
        "                                            name=f'decoder_attention_{i}')(  \n",
        "            query=decoder_features, value=decoder_features, attention_mask=mask)\n",
        "        \n",
        "        decoder_features = keras.layers.Add()([decoder_features, decoder_attention])\n",
        "        decoder_features = keras.layers.LayerNormalization(\n",
        "                                            axis=[1,2], epsilon=1e-6)(decoder_features)    \n",
        "\n",
        "        # joint attention block\n",
        "        joint_attention = tf.keras.layers.MultiHeadAttention(\n",
        "                                            num_heads=num_heads, \n",
        "                                            key_dim=key_dim, \n",
        "                                            dropout=.1,\n",
        "                                            name=f'joint_attention_{i}')(  \n",
        "            query=decoder_features, value=encoder_features)\n",
        "        \n",
        "        joint_features = keras.layers.Add()([decoder_features, joint_attention])\n",
        "        joint_features = keras.layers.LayerNormalization(\n",
        "                                            axis=[1,2], epsilon=1e-6)(joint_features)    \n",
        "\n",
        "        # Reshape -- (note: Required for XLA / TPU support only) \n",
        "        # Has no effect other than allow XLA to properly infer all shapes\n",
        "        decoder_features = keras.layers.Reshape([max_len, decoder_units])(decoder_features)  \n",
        "        joint_features = keras.layers.Reshape([max_len, decoder_units])(joint_features)  \n",
        "\n",
        "        # Feed Forward Block\n",
        "        decoder_features = keras.layers.Dense(decoder_units * 4, activation='relu',\n",
        "                                                name=f'dense_decoder_{i}')(joint_features)   \n",
        "        decoder_features = keras.layers.Dense(decoder_units, activation=None,\n",
        "                                                )(decoder_features) \n",
        "        decoder_features = keras.layers.Dropout(rate=.1)(decoder_features)\n",
        "\n",
        "        # Reshape -- (note: Required for XLA / TPU support only) \n",
        "        decoder_features = keras.layers.Reshape([max_len, decoder_units])(decoder_features)  \n",
        "        joint_features = keras.layers.Reshape([max_len, decoder_units])(joint_features)  \n",
        "\n",
        "        decoder_features = keras.layers.Add()([joint_features, decoder_features])\n",
        "        decoder_features = keras.layers.LayerNormalization(\n",
        "                                        axis=[1,2], epsilon=1e-6)(decoder_features)     \n",
        "\n",
        "\n",
        "    outputs = [decoder_features]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcaTvpPWkqgO",
        "outputId": "a7a7ad32-97bf-4973-f0a3-963e9286f0cf"
      },
      "source": [
        "DecoderAttention(num_blocks=2, encoder_units = 208, decoder_units=512, num_encoder_vectors=64, \n",
        "                 max_len=200, use_convolutions=False, name='DecoderAttention').summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_features (InputLayer)   [(None, 200, 512)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_0 (MultiHeadA (None, 200, 512)     1050624     decoder_features[0][0]           \n",
            "                                                                 decoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 200, 512)     0           decoder_features[0][0]           \n",
            "                                                                 decoder_attention_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_2 (LayerNor (None, 200, 512)     204800      add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_features (InputLayer)   [(None, 64, 208)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_0 (MultiHeadAtt (None, 200, 512)     739328      layer_normalization_2[0][0]      \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 200, 512)     0           layer_normalization_2[0][0]      \n",
            "                                                                 joint_attention_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_3 (LayerNor (None, 200, 512)     204800      add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 200, 512)     0           layer_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_decoder_0 (Dense)         (None, 200, 2048)    1050624     reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200, 512)     1049088     dense_decoder_0[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 200, 512)     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_3 (Reshape)             (None, 200, 512)     0           reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 200, 512)     0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 200, 512)     0           reshape_3[0][0]                  \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_4 (LayerNor (None, 200, 512)     204800      add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_1 (MultiHeadA (None, 200, 512)     1050624     layer_normalization_4[0][0]      \n",
            "                                                                 layer_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 200, 512)     0           layer_normalization_4[0][0]      \n",
            "                                                                 decoder_attention_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_5 (LayerNor (None, 200, 512)     204800      add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_1 (MultiHeadAtt (None, 200, 512)     739328      layer_normalization_5[0][0]      \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 200, 512)     0           layer_normalization_5[0][0]      \n",
            "                                                                 joint_attention_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_6 (LayerNor (None, 200, 512)     204800      add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "reshape_5 (Reshape)             (None, 200, 512)     0           layer_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dense_decoder_1 (Dense)         (None, 200, 2048)    1050624     reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 200, 512)     1049088     dense_decoder_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 200, 512)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape_7 (Reshape)             (None, 200, 512)     0           reshape_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "reshape_6 (Reshape)             (None, 200, 512)     0           dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 200, 512)     0           reshape_7[0][0]                  \n",
            "                                                                 reshape_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "layer_normalization_7 (LayerNor (None, 200, 512)     204800      add_7[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 9,008,128\n",
            "Trainable params: 9,008,128\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GA7wtNEhqW"
      },
      "source": [
        "## Decoder Head (Prediction Output)\n",
        "\n",
        "This is where we use what was learned in the encoder-decoder attention to output predicted labels. It is the prediction step from \"Attention is All You Need.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2LMbmclf1hs"
      },
      "source": [
        "def DecoderHead(decoder_units, vocab_size, max_len, use_dual_heads, \n",
        "                split_char_num, name='DecoderHead'):\n",
        "    \n",
        "    decoder_features = keras.layers.Input([max_len, decoder_units], \n",
        "                                          name='decoder_features')  # from Decoder Attention layer\n",
        "\n",
        "    inputs = [decoder_features]\n",
        "\n",
        "    # Prediction Block               \n",
        "    # include activation dtype on final output layer for overriding mixed precision policies\n",
        "    if not use_dual_heads:\n",
        "        logits = keras.layers.Dense(vocab_size, activation=None, \n",
        "            kernel_initializer= tf.keras.initializers.HeNormal())(decoder_features)  \n",
        "    \n",
        "    else:\n",
        "        decoder_features_0 = decoder_features[:, :split_char_num, :]\n",
        "        decoder_features_1 = decoder_features[:, split_char_num: , :]\n",
        "        \n",
        "        logits_0 = keras.layers.Dense(vocab_size, activation=None, \n",
        "            kernel_initializer= tf.keras.initializers.HeNormal())(decoder_features_0)  \n",
        "\n",
        "        logits_1 = keras.layers.Dense(vocab_size, activation=None, \n",
        "            kernel_initializer= tf.keras.initializers.HeNormal())(decoder_features_1)  \n",
        "\n",
        "        logits = keras.layers.Concatenate(1)([logits_0, logits_1])\n",
        "\n",
        "\n",
        "    probs = keras.layers.Activation('softmax', dtype=tf.float32, name='probs')(logits)  \n",
        "\n",
        "    outputs = [probs]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js6yvxIGf1hs",
        "outputId": "b2a0fe61-4ade-4208-fd41-62391c83f85f"
      },
      "source": [
        "DecoderHead(decoder_units=512, vocab_size=199, max_len=200,\n",
        "            use_dual_heads=True, split_char_num=50).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderHead\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_features (InputLayer)   [(None, 200, 512)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_2 (Sli (None, 50, 512)      0           decoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_3 (Sli (None, 150, 512)     0           decoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 50, 199)      102087      tf.__operators__.getitem_2[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 150, 199)     102087      tf.__operators__.getitem_3[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 200, 199)     0           dense_3[0][0]                    \n",
            "                                                                 dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "probs (Activation)              (None, 200, 199)     0           concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 204,174\n",
            "Trainable params: 204,174\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2UR1DLljD0S"
      },
      "source": [
        "## Update Mechanism (Optional)\n",
        "\n",
        "*Note: this is fully coded but I have not had time to train parameters with it. I leave that as a future opportunity for exploration.*\n",
        "\n",
        "NLP technicques typically output logits to find the highest likelhood token prediction. This can be improved to a (local) maximum likelihood selection using a \"beam step\" that ay override the initial prediction choice. \n",
        "\n",
        "This layer is an alternative system for updating predictions. Unlike \"beam,\" it is trainable and includes longer-range dependencies (instead of the very \"local\" beam step.) The entire original prediction is passed through a bidirectional RNN (decoder feature extraction) followed by AIAYN stye attention blocks. No masking is needed since we have the full RNN output to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpM3FS5zZ-3Y"
      },
      "source": [
        "def BeamUpdate(num_beam_blocks, num_att_blocks,\n",
        "               num_encoder_vectors, encoder_units, \n",
        "               decoder_units, max_len, vocab_size, name='BeamUpdate'):\n",
        "    \n",
        "    # update to required GRU model dtypes\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    \n",
        "    # layers\n",
        "    # note: GRU doesn't appear to be compatible with reduced precision\n",
        "    BeamUnit = keras.layers.GRU(decoder_units, return_sequences=True, \n",
        "                    return_state=True, go_backwards=True,\n",
        "                    dtype=tf.keras.mixed_precision.Policy('float32'))  \n",
        "\n",
        "    use_convolutions = False\n",
        "    BeamDecoderAttention = DecoderAttention(num_att_blocks, encoder_units, decoder_units, \n",
        "                                            num_encoder_vectors, max_len, \n",
        "                                            use_convolutions, name='BeamDecoderAttention')\n",
        "\n",
        "    BeamDecoderHead = DecoderHead(decoder_units, vocab_size, max_len, \n",
        "                                  use_dual_heads=False, split_char_num=None,\n",
        "                                  name='BeamDecoderHead')\n",
        "\n",
        "    \n",
        "    # Inputs\n",
        "    beam_input = keras.layers.Input([max_len, vocab_size], name='beam_input') \n",
        "    hidden_state = keras.layers.Input([decoder_units], name='hidden_state')\n",
        "    encoder_features = keras.layers.Input([num_encoder_vectors, encoder_units], name='encoder_features')   # from image \n",
        "    mask = keras.layers.Input([max_len, max_len], name='mask')   # should pass in all 1's, i.e. no masking\n",
        "    \n",
        "    inputs = [beam_input, hidden_state, encoder_features, mask]\n",
        "\n",
        "    # create initial hidden state for RNN dim = decoder_units\n",
        "    beam_hidden_state = tf.reduce_mean(encoder_features, -2)\n",
        "    beam_hidden_state = keras.layers.Dense(decoder_units, activation='relu')(beam_hidden_state)\n",
        "    \n",
        "    # Decoder encoding using 1 or more Beam layers\n",
        "    for i in range(num_beam_blocks):\n",
        "        beam_out, beam_hidden_state = \\\n",
        "            BeamUnit(beam_input, initial_state=[beam_hidden_state])\n",
        "\n",
        "    # Attention & Prediction (uses \"Attention is All You Need\" structure)\n",
        "    decoder_features = BeamDecoderAttention([encoder_features, beam_out, mask])\n",
        "\n",
        "    probs = BeamDecoderHead([decoder_features])  \n",
        "\n",
        "    outputs = [probs]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnHn_q5xcsB7",
        "outputId": "2cadfee6-0880-4b44-9610-28e49cc9d931"
      },
      "source": [
        "BeamUpdate(num_beam_blocks=1, num_att_blocks=1, num_encoder_vectors=50, encoder_units=256, \n",
        "            decoder_units=512, max_len=200, vocab_size=160, name='BeamUpdate').summary()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BeamUpdate\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_features (InputLayer)   [(None, 50, 256)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.reduce_mean (TFOpLambda (None, 256)          0           encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "beam_input (InputLayer)         [(None, 200, 160)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_9 (Dense)                 (None, 512)          131584      tf.math.reduce_mean[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "gru_2 (GRU)                     [(None, 200, 512), ( 1035264     beam_input[0][0]                 \n",
            "                                                                 dense_9[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "mask (InputLayer)               [(None, 200, 200)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "BeamDecoderAttention (Functiona (None, 200, 512)     4553216     encoder_features[0][0]           \n",
            "                                                                 gru_2[0][0]                      \n",
            "                                                                 mask[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "hidden_state (InputLayer)       [(None, 512)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "BeamDecoderHead (Functional)    (None, 200, 160)     82080       BeamDecoderAttention[0][0]       \n",
            "==================================================================================================\n",
            "Total params: 5,802,144\n",
            "Trainable params: 5,802,144\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6GIs3f3rpu0"
      },
      "source": [
        "# **Full Model**\n",
        "\n",
        "All the components are combined into a full encoder/decoder model. This is implemented using the subclassing API with custom call, train,  evaluation and prediction steps. Once initialized, the models have full access to high-level model.fit(), model.compile() and model.save_weights() methods.\n",
        "\n",
        "An extra features implemented is having Decoder() elements in *series* (not stacked). This adds more trainable parameters without affecting inference speed, and allows decoders to specialize more on different regions of the text.\n",
        "\n",
        "BaseTrainer() model has the BeamUpdate mechanism disabled. InchiGenerator() models include the BeamUpdate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcEV3ifLf1hs"
      },
      "source": [
        "class BaseTrainer(keras.Model):\n",
        "    \n",
        "    def __init__(self, image_dense_output_dim, decoder_units, \n",
        "                 num_encoder_blocks, num_decoder_blocks, \n",
        "                 use_dense_encoder_top, use_convolutions, use_dual_decoders, \n",
        "                 max_len, parameters, name='BaseTrainer', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        \"\"\" Beam updates turned off. Training conducted with teach-fed inputs.\n",
        "        note: dataset provided as (image, image_id, tokenized_InChI, InChI) \"\"\"\n",
        "        \n",
        "        # tokenizer info\n",
        "        \n",
        "        tokenizer_layer, self.inverse_tokenizer, self.tokenized_EOS = \\\n",
        "                Tokenizer(parameters=parameters)\n",
        "        self.vocab_size = len(tokenizer_layer.get_vocabulary())\n",
        "        \n",
        "        # save params\n",
        "        self.max_len = max_len  # max number of token prediction length\n",
        "        self.image_dense_output_dim = image_dense_output_dim\n",
        "        self.decoder_units = decoder_units\n",
        "        self.num_encoder_blocks = num_encoder_blocks\n",
        "        self.num_decoder_blocks = num_decoder_blocks\n",
        "        self.use_dense_encoder_top = tf.constant(use_dense_encoder_top, tf.bool)\n",
        "        self.use_convolutions = tf.constant(use_convolutions, tf.bool)\n",
        "        self.use_dual_decoders = tf.constant(use_dual_decoders, tf.bool)\n",
        "        self.parameters = parameters\n",
        "        self.SOS = parameters.SOS()\n",
        "        self.EOS = parameters.EOS()\n",
        "        self.embedding_dim = self.decoder_units  # required for consistency\n",
        "\n",
        "        #  Initialize positional encoding. (Batch dim added during build)\n",
        "        initializer = tf.random_normal_initializer()(shape=[self.max_len, self.embedding_dim])\n",
        "        self.positional_encoding = tf.Variable(initializer, trainable=True,\n",
        "                                               dtype=tf.float32,\n",
        "                                               name='positional_encoding', )\n",
        "        self.positional_encoding = tf.expand_dims(self.positional_encoding, axis=0)\n",
        "    \n",
        "        # trainable start variable  (Batch dim added during build)\n",
        "        initializer = tf.random_normal_initializer()(shape=[1, self.embedding_dim])\n",
        "        self.start_var = tf.Variable(initializer, trainable=True,\n",
        "                                     dtype=tf.float32,\n",
        "                                     name='start_variable')\n",
        "        self.start_var = tf.expand_dims(self.start_var, axis=0)\n",
        "\n",
        "        # Layers (note: additional layers initialized within '.build()')\n",
        "        split_char_num = 50  # controls dual decoder head switch (if applicable)\n",
        "        self.decoder_head = DecoderHead(decoder_units=self.decoder_units, \n",
        "                                        vocab_size=self.vocab_size, \n",
        "                                        max_len=self.max_len, \n",
        "                                        use_dual_heads=self.use_dual_decoders,\n",
        "                                        split_char_num=split_char_num,\n",
        "                                        name='DecoderHead')\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'image_dense_output_dim': self.image_dense_output_dim,\n",
        "                  'decoder_units': self.decoder_units,\n",
        "                  'num_encoder_blocks': self.num_encoder_blocks,\n",
        "                  'num_decoder_blocks': self.num_decoder_blocks,\n",
        "                  'use_dense_encoder_top': self.use_dense_encoder_top,\n",
        "                  'use_convolutions': use_convolutions,\n",
        "                  'use_dual_decoders': self.use_dual_decoders,\n",
        "                  'max_len': self.max_len,\n",
        "                  'parameters': self.parameters}\n",
        "        return config \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # note: dataset prepared with dict keys (image, tokenized_InChI, image_id, InChI)\n",
        "        self.batch_size = input_shape[0][0]  # 'image' batch size\n",
        "        self.padded_length = input_shape[1][-1]  # 'tokenized_InChI' token sequ. length\n",
        "\n",
        "        # InChI Encoders\n",
        "        self.inchi_prep = InchiPrep(batch_dim=self.batch_size,\n",
        "                                    padded_length=self.padded_length,\n",
        "                                    max_len=self.max_len, \n",
        "                                    embedding_dim=self.embedding_dim, \n",
        "                                    vocab_size=self.vocab_size,\n",
        "                                    name='InchiPrep')\n",
        "        \n",
        "        # Token Embedding Layers\n",
        "        self.token_embedding_layer = self.inchi_prep.get_layer('TokenEmbeddingLayer')               \n",
        "        self.inchi_indiv_embedding = InchiIndividalEmbedding(\n",
        "            token_embedding_layer=self.token_embedding_layer, name='InchiIndividalEmbedding')\n",
        "        \n",
        "        # Image Encoders\n",
        "        self.image_shape = input_shape[0][1:]  # drops batch dims from 'image' shape   \n",
        "        self.image_encoder = ImageEncoder(image_shape=self.image_shape, \n",
        "                                          output_dim=self.image_dense_output_dim,\n",
        "                                          use_dense_top=self.use_dense_encoder_top,\n",
        "                                          name='ImageEncoder')   \n",
        "        # collect params\n",
        "        self.num_encoder_vectors = self.image_encoder.output_shape[1]\n",
        "        self.image_features_dim = self.image_encoder.output_shape[2]\n",
        "        \n",
        "        # image encoding\n",
        "        self.encoder_attention = EncoderAttention(num_blocks=self.num_encoder_blocks, \n",
        "                                                  encoder_feature_dim=self.image_features_dim, \n",
        "                                                  num_att_elems=self.num_encoder_vectors,\n",
        "                                                  name='EncoderAttention')\n",
        "\n",
        "        # Decoders\n",
        "        self.decoder_attention = DecoderAttention(num_blocks=self.num_decoder_blocks,\n",
        "                                                  encoder_units=self.image_features_dim,\n",
        "                                                  decoder_units=self.decoder_units, \n",
        "                                                  num_encoder_vectors=self.num_encoder_vectors,\n",
        "                                                  max_len=self.max_len,\n",
        "                                                  use_convolutions=self.use_convolutions,\n",
        "                                                  name='DecoderAttention') \n",
        "        \n",
        "        # positional encoding and start variables: add batch dim\n",
        "        self.positional_encoding = tf.tile(self.positional_encoding, [self.batch_size, 1, 1])\n",
        "        self.start_var = tf.tile(self.start_var, [self.batch_size, 1, 1])\n",
        "\n",
        "        # save Transfer Model (for controlling trainability)\n",
        "        self.transfer_model = self.image_encoder.get_layer('TransferModel')\n",
        "\n",
        "    def encoding_step(self, image, tokenized_InChI, training):\n",
        "        \n",
        "        # get inputs / target pairs\n",
        "        inchi_vectors, targets = self.inchi_prep([tokenized_InChI, self.positional_encoding, self.start_var]) \n",
        "\n",
        "        # for inference: zero out all inchi vectors except the initial value\n",
        "        if not training:  \n",
        "            zeros = tf.zeros_like(inchi_vectors[:, 1:, :], dtype=inchi_vectors.dtype)\n",
        "            inchi_vectors = tf.concat([inchi_vectors[:, 0:1, :], zeros], axis=1)\n",
        "\n",
        "        # image encoding\n",
        "        image = self.image_encoder(image)\n",
        "        image_encoder_vectors = self.encoder_attention(image)\n",
        "\n",
        "        return image_encoder_vectors, inchi_vectors, targets\n",
        "    \n",
        "    @tf.function\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        # inputs\n",
        "        image = inputs[0]\n",
        "        tokenized_InChI = inputs[1]\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_vectors, inchi_vectors, targets = \\\n",
        "            self.encoding_step(image, tokenized_InChI, training)\n",
        "\n",
        "        # Decoder (char generation loop using generated predictions)\n",
        "        # inference loop\n",
        "        if not training:  # inference loop\n",
        "            predicted_probs = self.generation_loop(encoder_vectors=encoder_vectors, \n",
        "                                                   inchi_vectors=inchi_vectors)\n",
        "        \n",
        "        # teacher-fed training\n",
        "        else:  \n",
        "            decoder_attention = self.decoder_attention([encoder_vectors, inchi_vectors])\n",
        "            predicted_probs = self.decoder_head([decoder_attention])\n",
        "        \n",
        "        return predicted_probs\n",
        "     \n",
        "    @tf.function(jit_compile=False)\n",
        "    def tokens_to_string(self, tokens):\n",
        "\n",
        "        parsed_string_vals = self.inverse_tokenizer(tokens)\n",
        "        string_vals = keras.layers.Lambda(\n",
        "            lambda x: tf.strings.reduce_join(x, axis=-1))(parsed_string_vals)\n",
        "\n",
        "        # remove first EOS generated and everything after\n",
        "        pattern = ''.join([self.EOS, '.*$'])\n",
        "        string_vals = tf.strings.regex_replace(string_vals, pattern, rewrite='', \n",
        "                                               replace_global=True, name='remove_EOS')   \n",
        "\n",
        "        return string_vals\n",
        "\n",
        "    # Full Generation Loop\n",
        "    def generation_loop(self, encoder_vectors, inchi_vectors):\n",
        "\n",
        "        # containers\n",
        "        # note: use fixed size arrays for XLA / JIT\n",
        "        probs_array = tf.TensorArray(size=self.max_len, \n",
        "                                     dtype=tf.float32, \n",
        "                                     dynamic_size=False, \n",
        "                                     element_shape=[None, self.vocab_size],\n",
        "                                     tensor_array_name='probs_array')\n",
        "        \n",
        "        # create initial (embedded) predictions\n",
        "        embedded_preds_array = tf.TensorArray(size=self.max_len, \n",
        "                                              dtype=tf.float32, \n",
        "                                              clear_after_read = False, \n",
        "                                              dynamic_size=False, \n",
        "                                              element_shape=[None, self.embedding_dim],\n",
        "                                              tensor_array_name='embedded_preds_array')\n",
        "        \n",
        "        # pre-populate embedded predictions array with InChI values\n",
        "        # (These are just start value and zeros during inference-- \n",
        "        # make sure this happened during the InChI encoding step)\n",
        "        # (note: structure below would allow for teacher-training an RNN with attention)\n",
        "        for step in range(1, self.max_len):\n",
        "            embedded_preds_array = embedded_preds_array.write(\n",
        "                            step, tf.cast(inchi_vectors[:, step, :], tf.float32))\n",
        "       \n",
        "        # character generation loop (tf.while_loop)\n",
        "        def loop_fn(step, continue_cond, inchi_vectors, probs_array, embedded_preds_array):\n",
        "\n",
        "            # attention update\n",
        "            decoder_attention = self.decoder_attention([encoder_vectors, inchi_vectors])\n",
        "            \n",
        "            # get current step's probabilities and save results\n",
        "            probs = self.decoder_head([decoder_attention])\n",
        "            probs = probs[:, step, :]\n",
        "            probs_array = probs_array.write(step, tf.cast(probs, dtype=probs_array.dtype))\n",
        "\n",
        "            # make prediction\n",
        "            predictions = tf.argmax(probs, axis=-1)  # shape: (batch_size)\n",
        "            positional_encoding_step = self.positional_encoding[:, step, :]\n",
        "\n",
        "            # generated next iteration's input\n",
        "            embedded_preds = self.inchi_indiv_embedding(\n",
        "                                [predictions, positional_encoding_step])\n",
        "\n",
        "            # increment counter and check stopping conditions\n",
        "            step = tf.math.add(step, 1)\n",
        "\n",
        "            # check: final step reached\n",
        "            continue_cond = tf.math.less(step, self.max_len)\n",
        "           \n",
        "            # check: all batch elements reached EOS\n",
        "            def true_fn_2(predictions):\n",
        "                predictions = tf.expand_dims(predictions, axis=1)\n",
        "                continue_cond = tf.math.reduce_any(predictions != self.tokenized_EOS)\n",
        "                predictions = tf.squeeze(predictions, axis=1)\n",
        "                return continue_cond\n",
        "\n",
        "            continue_cond = tf.cond(tf.math.equal(continue_cond, True), \n",
        "                                    lambda: true_fn_2(predictions),\n",
        "                                    lambda: continue_cond)\n",
        "            \n",
        "            # save input embedding\n",
        "            def true_fn_3(embedded_preds_array):\n",
        "                embedded_preds_array = embedded_preds_array.write(\n",
        "                    step, tf.cast(tf.squeeze(embedded_preds), dtype=embedded_preds_array.dtype))  # match array dtype\n",
        "                return embedded_preds_array\n",
        "\n",
        "            embedded_preds_array = tf.cond(tf.math.equal(continue_cond, True), \n",
        "                                           lambda: true_fn_3(embedded_preds_array),\n",
        "                                           lambda: embedded_preds_array)\n",
        "\n",
        "            # prepare padded input for next iteration\n",
        "            def true_fn_4(embedded_preds_array, orig_dtype): #), orig_dtype):\n",
        "                inchi_vectors = embedded_preds_array.stack()\n",
        "                inchi_vectors = tf.transpose(inchi_vectors, perm=[1, 0, 2])  \n",
        "                \n",
        "                # cast back to original dtype\n",
        "                inchi_vectors = tf.cast(inchi_vectors, dtype=orig_dtype)  \n",
        "                return inchi_vectors\n",
        "\n",
        "            orig_dtype = inchi_vectors.dtype  # (careful casting needed for mixed precision)\n",
        "            inchi_vectors = tf.cond(tf.math.equal(continue_cond, True), \n",
        "                                    lambda: true_fn_4(embedded_preds_array, orig_dtype),\n",
        "                                    lambda: inchi_vectors)\n",
        "\n",
        "            return [step, continue_cond, inchi_vectors, probs_array, embedded_preds_array]\n",
        "\n",
        "        # stopping condition function\n",
        "        def cond_fn(step, continue_cond, inchi_vectors, probs_array, embedded_preds_array):\n",
        "            return continue_cond\n",
        "\n",
        "        # generation loop\n",
        "        # initial loop values\n",
        "        step = 0\n",
        "        continue_cond = tf.constant(True, dtype=tf.bool)\n",
        "\n",
        "        step, continue_cond, inchi_vectors, probs_array, embedded_preds_array \\\n",
        "            = tf.while_loop(\n",
        "                    maximum_iterations=self.max_len-1,  \n",
        "                    cond=cond_fn, \n",
        "                    body=loop_fn, \n",
        "                    loop_vars=[step, continue_cond, inchi_vectors,  \n",
        "                               probs_array, embedded_preds_array],               \n",
        "                    shape_invariants=\n",
        "                        [tf.TensorShape([]), # step\n",
        "                         tf.TensorShape([]), # continue_cond\n",
        "                         tf.TensorShape([None, self.max_len, \n",
        "                                         self.embedding_dim]), # inchi_vectors\n",
        "                         None,  # probs_array\n",
        "                         None],  # embedded_preds_array\n",
        "                )\n",
        "        \n",
        "        # unpack probs_array\n",
        "        predicted_probs = probs_array.stack()  # predicted logits\n",
        "        predicted_probs = tf.squeeze(predicted_probs)\n",
        "        predicted_probs = tf.transpose(predicted_probs, perm=[1, 0, 2])  \n",
        "\n",
        "        return predicted_probs"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5TuwjWo0RRB"
      },
      "source": [
        "class InchiGenerator(BaseTrainer):\n",
        "    \"\"\"\n",
        "    NOTE: Updates have been make to the main BaseTrainer model class since \n",
        "    InChiGenerator was last revised.  Test / Revise this class before using.\n",
        "\n",
        "    Beam updates turned on, training conducted using generated preds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, num_beam_att_blocks, name='BeamInchiTrainer', **kwargs):\n",
        "        \n",
        "        super().__init__(image_dense_output_dim=base_model.image_dense_output_dim,\n",
        "                         use_dense_encoder_top=base_model.use_dense_encoder_top,\n",
        "                         decoder_units=base_model.decoder_units,\n",
        "                         num_decoder_blocks=base_model.num_decoder_blocks,\n",
        "                         num_encoder_blocks=base_model.num_encoder_blocks,\n",
        "                         use_convolutions=base_model.use_convolutions,\n",
        "                         use_dual_decoders= base_model.use_dual_decoders,\n",
        "                         max_len = base_model.max_len,\n",
        "                         parameters=base_model.parameters, \n",
        "                         name=name, **kwargs)\n",
        "        \n",
        "        self.num_beam_att_blocks = num_beam_att_blocks\n",
        "            \n",
        "    def get_config(self):\n",
        "        config = {'num_beam_att_blocks': self.num_beam_att_blocks,\n",
        "                  'image_dense_output_dim': self.image_dense_output_dim,\n",
        "                  'decoder_units': self.decoder_units,\n",
        "                  'num_decoder_blocks': self.num_decoder_blocks,\n",
        "                  'num_encoder_blocks': self.num_encoder_blocks,\n",
        "                  'use_dense_encoder_top': self.use_dense_encoder_top,\n",
        "                  'use_convolutions': use_convolutions,\n",
        "                  'max_len': self.max_len,\n",
        "                  'use_dual_decoders': self.use_dual_decoders,\n",
        "                  'parameters': self.parameters}\n",
        "        return config \n",
        "\n",
        "    def build(self, input_shape):\n",
        "\n",
        "        super().build(input_shape)\n",
        "\n",
        "        self.num_encoder_vectors = super().image_encoder.output_shape[1]\n",
        "        self.image_features_dim = super().image_encoder.output_shape[2]\n",
        "\n",
        "        self.beam = BeamUpdate(num_beam_blocks=self.num_beam_att_blocks, \n",
        "                               num_att_blocks=self.num_beam_att_blocks,\n",
        "                               num_encoder_vectors=self.num_encoder_vectors,\n",
        "                               encoder_units=self.decoder_units, \n",
        "                               decoder_units=self.decoder_units, \n",
        "                               max_len=self.max_len, \n",
        "                               vocab_size=self.vocab_size, \n",
        "                               name='BeamUpdate')\n",
        "    \n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        #### first portion matches base model path:  #############\n",
        "        # inputs\n",
        "        image = inputs[0]\n",
        "        tokenized_InChI = inputs[1]\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_vectors, inchi_vectors, targets = \\\n",
        "            self.encoding_step(image, tokenized_InChI, training)\n",
        "\n",
        "        # Decoder (char generation loop using generated predictions)\n",
        "        # inference loop\n",
        "        if not training:  # inference loop\n",
        "            predicted_probs = self.generation_loop(encoder_vectors=encoder_vectors, \n",
        "                                                   inchi_vectors=inchi_vectors)\n",
        "        \n",
        "         # training steps (before gradient calculations)\n",
        "        else:  \n",
        "            decoder_attention = self.decoder_attention([encoder_vectors, inchi_vectors])\n",
        "            predicted_probs = self.decoder_head([decoder_attention])\n",
        "       \n",
        "        #### beam update #############\n",
        "        # create initial RNN state\n",
        "        initial_state = tf.math.reduce_mean(encoder_vectors, axis=1)\n",
        "\n",
        "        # get probs and predictions\n",
        "        predicted_probs = self.beam([predicted_probs, hidden_state, encoder_vectors])\n",
        "\n",
        "        return predicted_probs"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtWg2jRlI1_W"
      },
      "source": [
        "class EditDistanceMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='edit_distance', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.edit_distance = self.add_weight(name='edit_distance', initializer='zeros')\n",
        "        self.batch_counter = self.add_weight(name='batch_counter', initializer='zeros')\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.sparse.from_dense(y_true)\n",
        "        y_pred = tf.sparse.from_dense(tf.argmax(y_pred, axis=-1))  # convert probs to preds\n",
        "\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.cast(y_pred, tf.int32)\n",
        "\n",
        "        # compute edit distance (of parsed tokens)\n",
        "        edit_distance = tf.edit_distance(y_pred, y_true, normalize=False)\n",
        "        self.edit_distance.assign_add(tf.reduce_mean(edit_distance))\n",
        "\n",
        "        # update counter\n",
        "        self.batch_counter.assign_add(tf.reduce_sum(1.))\n",
        "    \n",
        "    def result(self):\n",
        "        return self.edit_distance / self.batch_counter\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.edit_distance.assign(0.0)\n",
        "        self.batch_counter.assign(0.0)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt3klX0SYnVw"
      },
      "source": [
        "# Learning rate schedule used in \"Attention is All You Need\"\n",
        "class LRScheduleAIAYN(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, scale_factor=1, warmup_steps=4000):  # defaults reflect paper's values\n",
        "        # cast dtypes\n",
        "        self.warmup_steps = tf.constant(warmup_steps, dtype=tf.float32)\n",
        "        dim = tf.constant(352, dtype=tf.float32)\n",
        "        scale_factor = tf.constant(scale_factor, dtype=tf.float32)\n",
        "        \n",
        "        self.scale = scale_factor * tf.math.pow(dim, -1.5)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        opt_1 = step * tf.math.pow(self.warmup_steps, -1.5)  # linear increase\n",
        "        opt_2 = tf.math.pow(step, -.5) # decay\n",
        "        return self.scale * tf.math.reduce_min([opt_1, opt_2])\n",
        "\n",
        "# visualize learning rate \n",
        "#temp_lr = LRScheduleAIAYN()\n",
        "#plt.plot([i for i in range(1, 8000)], [temp_lr(i) for i in range(1, 8000)])\n",
        "#print('Learning Rate Schedule')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUtswyx2ru4O"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZAEoiSYsZb"
      },
      "source": [
        "Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDez379bi8Xy"
      },
      "source": [
        "NAME_MODIFIER = ''\n",
        "\n",
        "# build model\n",
        "IMAGE_DENSE_OUTPUT_DIM = 256  # note: only used with USE_DENSE_ENCODER_TOP = True.\n",
        "DECODER_UNITS = 512  # # \"All You Need is Attention\" uses 512 units\n",
        "BEAM_RNN_UNITS = 128  # note: only used in beam_model.\n",
        "NUM_ENCODER_BLOCKS = 6  # \"All You Need is Attention\" uses 6 blocks\n",
        "NUM_DECODER_BLOCKS = 6 # \"All You Need is Attention\" uses 6 blocks \n",
        "MAX_LEN = 200\n",
        "USE_DENSE_ENCODER_TOP = False  # for fine tuning image features pre-self-attention\n",
        "USE_DUAL_DECODERS = False\n",
        "USE_CONVOLUTIONS = False\n",
        "if USE_CONVOLUTIONS:\n",
        "    checkpoint_save_name = 'ConvAtt_model_checkpoints' + NAME_MODIFIER\n",
        "else:\n",
        "    checkpoint_save_name = 'AISAYN_model_checkpoints' + NAME_MODIFIER\n",
        "\n",
        "LOAD_CHECKPOINT_FILE = os.path.join(PARAMETERS.load_checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n",
        "SAVE_CHECKPOINT_FILE = os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n",
        "\n",
        "# note: in Kaggle,\n",
        "# LOAD_CHECKPOINT_FILE points to saved outputs from prev session\n",
        "# SAVE_CHECKPOINT_FILE points to saved outputs from current session"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7s5jU6OYwax"
      },
      "source": [
        "Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0OTg89vf1ht",
        "outputId": "489ba659-4e37-4ea8-e55f-aee58a0a1027"
      },
      "source": [
        "# NOTE: If nothing happens on model build / call, check if Kaggle GCS directories have changed. (This happends periodically)\n",
        "\n",
        "# Update inputs: remove string keys, as they are not compatible with TPU\n",
        "train_ds_int_index = train_ds.map(lambda x: (x['image'], x['tokenized_InChI'][:, :MAX_LEN], \n",
        "                                             x['image_id'], x['InChI'])).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds_int_index = valid_ds.map(lambda x: (x['image'], x['tokenized_InChI'][:, :MAX_LEN], \n",
        "                                             x['image_id'], x['InChI'])).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# create model using distribution strategy\n",
        "with PARAMETERS.strategy().scope():\n",
        "    base_model = BaseTrainer(image_dense_output_dim=IMAGE_DENSE_OUTPUT_DIM,\n",
        "                                decoder_units=DECODER_UNITS,\n",
        "                                num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
        "                                num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
        "                                use_dense_encoder_top=USE_DENSE_ENCODER_TOP,\n",
        "                                use_convolutions=USE_CONVOLUTIONS,\n",
        "                                use_dual_decoders=USE_DUAL_DECODERS,\n",
        "                                max_len=MAX_LEN,  \n",
        "                                parameters=PARAMETERS, \n",
        "                                name='BaseTrainer')\n",
        "\n",
        "    # build (Note: make sure to do this before compiling!)\n",
        "    # NOTE: On distributed training, the batch size is distributed among replicas, \n",
        "    # so a smaller batch size must be used on build /first call\n",
        "    if PARAMETERS.tpu():  \n",
        "    \n",
        "        temp_ds = PARAMETERS.strategy().experimental_distribute_dataset(train_ds_int_index)\n",
        "        temp_ds = iter(temp_ds)\n",
        "        val = next(temp_ds)\n",
        "\n",
        "        # build with new val\n",
        "        temp_func = tf.function(func=base_model, experimental_relax_shapes=True,\n",
        "                                experimental_follow_type_hints=True)\n",
        "        PARAMETERS.strategy().run(temp_func, args=[(val[0], val[1])])  # use strategy.run() on TPU\n",
        "            \n",
        "\n",
        "    else:  \n",
        "        # build with original val\n",
        "        for val in train_ds_int_index.take(1): \n",
        "            base_model(val, training=False)\n",
        "            #base_model(val, training=True)\n",
        "    \n",
        "    # compiler components\n",
        "    learning_rate = LRScheduleAIAYN(scale_factor=PARAMETERS.strategy().num_replicas_in_sync, \n",
        "                                    warmup_steps=4000)  # from \"Attention is All You Need\"       \n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,  # params from \"Attention is All You Need\"\n",
        "                                         beta_1=0.9, beta_2=0.98, epsilon=10e-9)\n",
        "\n",
        "    # callbacks\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(SAVE_CHECKPOINT_FILE, monitor='loss', \n",
        "                            save_weights_only=True, save_best_only=True, save_freq='epoch',\n",
        "                            options=tf.train.CheckpointOptions(experimental_io_device='/job:localhost'))\n",
        "    nan_stop = tf.keras.callbacks.TerminateOnNaN()\n",
        "    backup_checkpoint = tf.keras.callbacks.experimental.BackupAndRestore(\n",
        "        os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, '/'))\n",
        "    \n",
        "    # metrics\n",
        "    edit_dist_metric = EditDistanceMetric()\n",
        "    \n",
        "    # loss with label smoothing\n",
        "    loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=.1)\n",
        "\n",
        "    # optimizations\n",
        "    if not PARAMETERS.tpu():\n",
        "        os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # better balances CPU / GPU interaction in tf.data\n",
        "        tf.config.optimizer.set_jit(\"autoclustering\")  # XLA compiler optimization\n",
        "        optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)  # required with mixed precision on GPU / CPU\n",
        "\n",
        "        # compile       \n",
        "        base_model.compile(optimizer=optimizer, \n",
        "                           loss=loss_fn,\n",
        "                           metrics=['categorical_accuracy', edit_dist_metric],\n",
        "                           steps_per_execution=8)\n",
        "    else:\n",
        "        # compile (without EditDistance metric - not functioning on TPU)\n",
        "        base_model.compile(optimizer=optimizer, \n",
        "                           loss=loss_fn,\n",
        "                           metrics=['categorical_accuracy'],\n",
        "                           steps_per_execution=4*PARAMETERS.strategy().num_replicas_in_sync)\n",
        "\n",
        "    # show summary\n",
        "    print(base_model.summary())\n",
        "    print('Models initialized.')\n",
        "\n",
        "#\"\"\"\n",
        "# verify model calls & methods work\n",
        "if not PARAMETERS.tpu():\n",
        "    base_model(val, training=False)\n",
        "\n",
        "    # sync weights\n",
        "    # WARNING!: in Kaggle this loads from prev session saved weights\n",
        "    try:\n",
        "        with PARAMETERS.strategy().scope(): \n",
        "            base_model.load_weights(LOAD_CHECKPOINT_FILE)  \n",
        "            \n",
        "            pass\n",
        "    except:\n",
        "        print('No weights loaded')  \n",
        "\n",
        "else:\n",
        "    # sync weights\n",
        "    # WARNING!: in Kaggle this loads from prev session saved weights\n",
        "    try:\n",
        "        with PARAMETERS.strategy().scope(): \n",
        "            base_model.load_weights(LOAD_CHECKPOINT_FILE, \n",
        "                                    options=tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\"))  \n",
        "\n",
        "    except:\n",
        "        print('No weights loaded')    \n",
        "#\"\"\"\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BaseTrainer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "string_lookup_3 (StringLooku multiple                  0 (unused)\n",
            "_________________________________________________________________\n",
            "DecoderHead (Functional)     (None, 200, 199)          102087    \n",
            "_________________________________________________________________\n",
            "InchiPrep (Functional)       [(None, 200, 512), (None, 101888    \n",
            "_________________________________________________________________\n",
            "TokenEmbeddingLayer (Embeddi multiple                  101888    \n",
            "_________________________________________________________________\n",
            "InchiIndividalEmbedding (Fun (None, 1, 512)            101888    \n",
            "_________________________________________________________________\n",
            "ImageEncoder (Functional)    (None, 100, 320)          3634851   \n",
            "_________________________________________________________________\n",
            "EncoderAttention (Functional (None, 100, 320)          8158080   \n",
            "_________________________________________________________________\n",
            "DecoderAttention (Functional (None, 200, 512)          27712512  \n",
            "_________________________________________________________________\n",
            "TransferModel (Functional)   (None, 10, 10, 320)       3634851   \n",
            "=================================================================\n",
            "Total params: 39,709,418\n",
            "Trainable params: 39,669,955\n",
            "Non-trainable params: 39,463\n",
            "_________________________________________________________________\n",
            "None\n",
            "Models initialized.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oL0ub_P2ovDL"
      },
      "source": [
        "Inference Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGAGTWVKouN8"
      },
      "source": [
        "def run_inference(model, dataset, return_lev_score=False, take_num=100, skip_set_num=0):\n",
        "    \"\"\" produces image_id, pred_string pairs \"\"\"\n",
        "    \n",
        "    # update model to tf.functions\n",
        "    # NOTE: these model methods are decorates with @tf.function within class definition\n",
        "    model_tf = model.call\n",
        "    tokens_to_string_tf = model.tokens_to_string\n",
        "\n",
        "    \"\"\"  # if @tf.function not used in class def, adapt functions with:\n",
        "    def tf_fn(orig_func):\n",
        "        return tf.function(func=orig_func, experimental_relax_shapes=True,\n",
        "                           experimental_follow_type_hints=True,\n",
        "                           jit_compile=False)  # JIT compile causes GPU system to crash\n",
        "    \"\"\"\n",
        "\n",
        "    # initialize containers\n",
        "    image_ids_list = []\n",
        "    generated_predictions_list = []\n",
        "    true_InChI_list = []\n",
        "    \n",
        "    # prepare dataset for parallel / distributed execution\n",
        "    if PARAMETERS.tpu():\n",
        "        if not take_num:  # use full dataset (~ 4 min on TPU)\n",
        "            dataset = PARAMETERS.strategy().experimental_distribute_dataset(dataset)\n",
        "        \n",
        "        else:  # use restricted dataset (useful for testing purposes)\n",
        "            dataset = PARAMETERS.strategy().experimental_distribute_dataset(\n",
        "                        dataset.skip(take_num * skip_set_num).take(take_num))\n",
        "            \n",
        "        # convert distributed ds to iterator\n",
        "        dataset = iter(dataset)\n",
        "\n",
        "    else:\n",
        "        if not take_num:  # use full dataset\n",
        "            pass\n",
        "\n",
        "        else:  # use restricted dataset (useful for testing purposes)\n",
        "            dataset = dataset.skip(take_num * skip_set_num).take(take_num).prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "    \n",
        "    # generate (image_id, token preds)\n",
        "    for val in dataset:\n",
        "\n",
        "        # get actual values and gather into single batch\n",
        "        true_InChI = val[3]\n",
        "        if PARAMETERS.tpu():\n",
        "            true_InChI = PARAMETERS.strategy().gather(true_InChI, axis=0)\n",
        "\n",
        "        # get corresponding image ids and gather into single batch\n",
        "        image_ids = val[2]\n",
        "        if PARAMETERS.tpu():\n",
        "            image_ids = PARAMETERS.strategy().gather(image_ids, axis=0)\n",
        "\n",
        "        # get predictions (as tokens)  and gather into single batch\n",
        "        if PARAMETERS.tpu():\n",
        "            generated_probs = PARAMETERS.strategy().run(model_tf, args=[val[:2]])\n",
        "        else:\n",
        "            generated_probs = model_tf(val[:2])\n",
        "        \n",
        "        if PARAMETERS.tpu():\n",
        "            generated_probs = PARAMETERS.strategy().gather(generated_probs, axis=0)\n",
        "        generated_probs = tf.squeeze(generated_probs)\n",
        "\n",
        "        # convert predictions to strings\n",
        "        generated_predictions = tf.argmax(generated_probs, axis=-1)\n",
        "        generated_predictions = tokens_to_string_tf(generated_predictions)\n",
        "\n",
        "        # decode bytestrings and update containers\n",
        "        image_ids_list.extend([x.decode() for x in image_ids.numpy().tolist()])\n",
        "        generated_predictions_list.extend([x.decode() for x in generated_predictions.numpy().tolist()])\n",
        "        true_InChI_list.extend([x.decode() for x in true_InChI.numpy().tolist()])\n",
        "\n",
        "    output = [image_ids_list, generated_predictions_list, true_InChI_list]\n",
        "\n",
        "    if return_lev_score:\n",
        "        \n",
        "        # compute scores\n",
        "        lev_score = [levenshtein(pred, orig) for (pred, orig)\n",
        "                      in zip(generated_predictions_list, true_InChI_list)]\n",
        "\n",
        "        # add to outputs\n",
        "        output.append(lev_score)\n",
        "\n",
        "    return output"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsmcfhvEvdUP"
      },
      "source": [
        "Test inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m6BmY63-QSwJ",
        "outputId": "692812ee-488a-4a4f-8191-9fd46364b9a0"
      },
      "source": [
        "\"\"\"\n",
        "# test inference speed - time for 'take_num' batches\n",
        "%timeit run_inference(base_model, dataset=train_ds_int_index, return_lev_score=True, take_num=2, skip_set_num=0)\n",
        "\"\"\""
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 loop, best of 5: 47 s per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanU6T8tf1ht"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/')\n",
        "    %tensorboard --logdir './logs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBKRQowIWqq"
      },
      "source": [
        "\"\"\"\n",
        "# Full model: inference speed (with beam)\n",
        "%%timeit\n",
        "num_batches = 3\n",
        "\n",
        "for val in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(num_batches): \n",
        "    im_id, preds = (base_model.predict(val))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otxdN02mf1ht"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnw2vkOhCFXM"
      },
      "source": [
        "Prepare dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa48CRQtCFkN"
      },
      "source": [
        "# Dataset: Random Perturbations and One-Hot-Encoding\n",
        "# note: one-hot needed so we can use label smoothing in CrossEntropy Loss\n",
        "tf.keras.mixed_precision.set_global_policy('float32')  # temporarily removed mixed precision\n",
        "\n",
        "# define transformations\n",
        "rotate = keras.layers.experimental.preprocessing.RandomRotation(\n",
        "            factor=(-0.5, 0.5), fill_mode='constant')\n",
        "contrast = keras.layers.experimental.preprocessing.RandomContrast(factor=.1)\n",
        "depth = base_model.vocab_size\n",
        "one_hot = keras.layers.Lambda(lambda x: tf.one_hot(x, depth=depth)) \n",
        "\n",
        "# apply transformations\n",
        "train_ds_prepared = train_ds_int_index.map(lambda w, x, y, z: (rotate(w), x),\n",
        "                                           num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds_prepared = train_ds_prepared.map(lambda w, x: (contrast(w), x),\n",
        "                                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds_prepared = train_ds_prepared.map(lambda w, x: ((w, x), one_hot(x)),\n",
        "                                          num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                                          .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_ds_prepared = valid_ds_int_index.map(lambda w, x, y, z: ((w, x), one_hot(x)),\n",
        "                                          num_parallel_calls=tf.data.AUTOTUNE)\\\n",
        "                                          .prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# re-enable to mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy(PARAMETERS.mixed_precision())"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8MF9xt4uqG"
      },
      "source": [
        "Train base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdeuljeJf1h1",
        "outputId": "2cc95943-626e-498b-848f-5a70270f5ecd"
      },
      "source": [
        "# Train base model (teacher-fed training, prediction-fed validation, no beam update)\n",
        "if not PARAMETERS.tpu():\n",
        "    steps_per_epoch = 512\n",
        "    validation_steps = 128\n",
        "    callbacks=[checkpoint, nan_stop, backup_checkpoint],# tensorboard]\n",
        "    validation_freq = 6\n",
        "\n",
        "else:\n",
        "    steps_per_epoch = 12 * (4 * PARAMETERS.strategy().num_replicas_in_sync)\n",
        "    validation_steps = PARAMETERS.strategy().num_replicas_in_sync  \n",
        "    callbacks=[checkpoint, nan_stop]\n",
        "    validation_freq = 10  # note: validation step causes major slowdown on TPU\n",
        "\n",
        "\n",
        "epoch_multiple = 100\n",
        "epochs = epoch_multiple * int(1.8 * 1e6) // (steps_per_epoch * PARAMETERS.batch_size())\n",
        "    \n",
        "# Important! Lock transfer model during initial training. (Optional) unlock after initial convergence\n",
        "base_model.transfer_model.trainable = False  \n",
        "\n",
        "# train\n",
        "history = base_model.fit(train_ds_prepared.repeat(),\n",
        "                         validation_data=valid_ds_prepared.repeat(),\n",
        "                         epochs=epochs,\n",
        "                         steps_per_epoch=steps_per_epoch,\n",
        "                         validation_freq=validation_freq, \n",
        "                         validation_steps=validation_steps, \n",
        "                         callbacks=callbacks,\n",
        "                         verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/457\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/PartitionedCall:4\", shape=(25472,), dtype=int32), values=Tensor(\"Adam/gradients/PartitionedCall:3\", shape=(25472, 512), dtype=float32), dense_shape=Tensor(\"Adam/gradients/PartitionedCall:5\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/indexed_slices.py:449: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"while/Adam/gradients/PartitionedCall:4\", shape=(25472,), dtype=int32), values=Tensor(\"while/Adam/gradients/PartitionedCall:3\", shape=(25472, 512), dtype=float32), dense_shape=Tensor(\"while/Adam/gradients/PartitionedCall:5\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"shape. This may consume a large amount of memory.\" % value)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7y-PoC1ztE7"
      },
      "source": [
        "TPU-safe saving to local directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDaC2XbXnSS4"
      },
      "source": [
        "base_model.save_weights(os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, 'saved_model'), \n",
        "                        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oz66tXyM3LY6"
      },
      "source": [
        "base_model.load_weights(os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, 'saved_model'), \n",
        "                        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s1Lgn1e4wxz"
      },
      "source": [
        "Train beam update model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otz8JJdB3QZ6"
      },
      "source": [
        "\"\"\"\n",
        "Not yet implemented\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbvzr5rdmjgs"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Here we define function to conduct inference on the test set. Results are saved to \"submission.csv\".\n",
        "\n",
        "Intermediate results are saved at regular intervals to. This allows inference to be conducted in stages and is a safeguard in case of interruptions before the full set has been processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVm4YazBsFIB"
      },
      "source": [
        "def make_inference_progress(dataset, model, return_lev_score=True, save_freq=50, parameters=PARAMETERS):\n",
        "\n",
        "    batch_size = 1024\n",
        "    est_num_batches = 2*10e7 // batch_size\n",
        "    take_num = 100\n",
        "\n",
        "    #initialize new dataframe\n",
        "    predictions_df = pd.DataFrame(columns=['image_id', 'InChI', 'lev_score'])\n",
        "\n",
        "\n",
        "    for i in range(int(est_num_batches // take_num)):\n",
        "        try:\n",
        "\n",
        "             # get predictions\n",
        "            inference_outputs = run_inference(model, dataset, return_lev_score=True, \n",
        "                                              take_num=take_num, skip_set_num=i)\n",
        "            \n",
        "            im_id, pred, true_val, lev_score  = inference_outputs[:]\n",
        "\n",
        "            # add to dataframe\n",
        "            new_preds = pd.DataFrame({'image_id': im_id, 'InChI': pred, 'lev_score': lev_score})\n",
        "            predictions_df = predictions_df.append(new_preds)\n",
        "\n",
        "            # save to CSV\n",
        "            if i % save_freq == 0:\n",
        "                predictions_df = predictions_df.drop_duplicates(subset='image_id', keep='last')\n",
        "                predictions_df[['image_id', 'InChI']].to_csv(PARAMETERS.csv_save_dir() + 'submission.csv', index=False)\n",
        "                print(f'iteration {i}')\n",
        "\n",
        "        except:\n",
        "            print(f'completed at step {i}')\n",
        "            break\n",
        "\n",
        "    return predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN9S2TyMy__W"
      },
      "source": [
        "Load previosuly generated predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iSlSFmHy9mX"
      },
      "source": [
        "try:\n",
        "    predictions_df = pd.read_csv(PARAMETERS.csv_save_dir() + 'submission.csv')\n",
        "except:\n",
        "    predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlq0OYsqzHMA"
      },
      "source": [
        "Generate additional predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CplWdHecmiRQ"
      },
      "source": [
        "\"\"\" On first pass or to start from scratch, initialize the dataframe with:\n",
        "predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)\n",
        "\"\"\"\n",
        "\n",
        "predictions_df = make_inference_progress(predictions_df, save_freq=100, \n",
        "                                         num_batches=1, starting_batch=0, \n",
        "                                         parameters=PARAMETERS)\n",
        "predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-Wei5hLcef"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8OpV3B3LclS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLfbijH7LcoI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4nM-sh9Lg9e"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}