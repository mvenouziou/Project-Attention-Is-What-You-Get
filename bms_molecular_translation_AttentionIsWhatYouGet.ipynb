{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "bms_molecular_translation-AttentionIsWhatYouGet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Attention-Is-What-You-Get/blob/main/bms_molecular_translation_AttentionIsWhatYouGet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zujis6hSUc0o"
      },
      "source": [
        "# Attention is What You Get\n",
        "\n",
        "This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition.\n",
        "\n",
        "-----\n",
        "\n",
        "AUTHOR: \n",
        "\n",
        "Mo Venouziou\n",
        "\n",
        "- *Email: mvenouziou@gmail.com*\n",
        "- *LinkedIn: www.linkedin.com/in/movenouziou/*\n",
        "\n",
        "*June 2, 2021*\n",
        "\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvew4SZuM33i"
      },
      "source": [
        "### Our Goal: Predict the \"InChI\" value of any given chemical compound diagram. \n",
        "\n",
        "International Chemical Identifiers (\"InChI values\") are a standardized encoding to describe chemical compounds. They take the form of a string of letters, numbers and deliminators, often between 100 - 400 characters long. \n",
        "\n",
        "The chemical diagrams are provided as PNG files, often of such low quality that it may take a human several seconds to decipher. \n",
        "\n",
        "Label length and image quality become a serious challenge here, because we must predict labels for a very large quantity of images. There are 1.6 million images in the test set abd 2.4 million images available in the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "Obx8tqy_ICG0",
        "outputId": "e93f1506-1bfa-4ca8-f76b-ade812446481"
      },
      "source": [
        "\"\"\"\n",
        "# Example (image, target label) pair\\n\\n'\n",
        "for val in train_ds.unbatch().take(1):\n",
        "    print('Example Label:\\n', val['InChI'].numpy())\n",
        "    print('\\nCorresponding Image:', plt.imshow(val['image'][:,:,0], cmap='binary'))\n",
        "### note: load datasets before running this cell\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Example (image, target label) pair\\n\\n'\\nfor val in train_ds.unbatch().take(1):\\n    print('Example Label:\\n', val['InChI'].numpy())\\n    print('\\nCorresponding Image:', plt.imshow(val['image'][:,:,0], cmap='binary'))\\n### note: load datasets before running this cell\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTzwOF_khY65"
      },
      "source": [
        "## MODEL STRUCTURE: \n",
        "\n",
        "**Image CNN + Attention Features encoder --> text Attention + CNN feature layer decoder.**\n",
        "\n",
        "This is a hybrid approach with:\n",
        " \n",
        " - Image Encoder from [*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*](https://proceedings.mlr.press/v37/xuc15.pdf).  Generate image feature vectors using intermediate layer outputs from a pretrained CNN. (Here I use the more modern EfficientNet model with fixed weights and add a trainable Dense layer for customization.)\n",
        " \n",
        " - T2T encoder-decoder model from [*All You Need is Attention*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Self-attention feature extraction for both encoder and decoder, joint encoder-decoder attention feature interactions, and dense prediction output block. Model includes paramaters to control the number of attention blocks used in encoder and decoder steps.\n",
        "\n",
        " - ***PLUS*** *(optional):* Decoder Output Blocks placed in Series (not stacked). Increase the number of trainable paramaters without adding inference computational complexity, while also allowing decoders to specialize on different regions of the output. (Note: Training is a bit trickier. My experiments show it is best to first train with the decoders using shared weights, then allowing them to vary later on in training.)\n",
        " \n",
        " - ***PLUS*** *(optional):* Is attention really all you need? Add a convolutional layer to enhance text features before decoder self-attention to experiment with performance differences with and without extra convolutional layer(s). Use of CNN's in NLP comes from [*Convolutional Sequence to Sequence Learning*](http://proceedings.mlr.press/v70/gehring17a.html.)\n",
        "\n",
        " - ***PLUS*** *(optional):* Beam-Search Alternative, an extra decoding layer applied after the full logits prediction has been made. This takes the form of a bidirectional RNN applied to the full logits sequence. Because a full (initial) prediction has already been made, computations can be paralelized using stateful RNNs. (See more details below.)\n",
        "\n",
        "*Optional features can be enabled/disabled using parameters in my model definitions.*\n",
        "\n",
        "----\n",
        "\n",
        "## NEXT STEPS:\n",
        "\n",
        " - Implement **TPU training**. (Currently runs on GPU. Note that a CPU alone is not enough to achieve acceptable inference speed.)\n",
        "\n",
        " - experiment with **\"Tokens-to-Token ViT\"** in place of the image CNN. (Technique from [*Training Vision Transformers from Scratch on ImageNet*](https://arxiv.org/pdf/2101.11986.pdf)\n",
        "  \n",
        " - Train a **Beam-search Alternative**. \n",
        "\n",
        "    - Beam search is a technique to modify model predictions to reflect the (local) maximum likelihood estimate. However, it is *very* local in that computation expense increases quickly with the number of character steps taken into account. This is also a hard-coded algorithm, which is somewhat contrary to the philosophy of deep learning.\n",
        "\n",
        "    - A *Beam-search Alternative* would be an extra decoding layer applied *after* the full logits prediction has been made. This might be in the form of a stateful, bidirectional RNN that is computationally parallizable because it is applied to the full logits sequence.\n",
        "\n",
        "    - This is coded and ready to train, although I have not yet had the time to do so.\n",
        "\n",
        " - Treat the number of convolutional layers (decoder feature extraction) and number of decoders places in series (decoder prediction output) as **new hyperparamaters** to tune.\n",
        "\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htI5m-W2uJmx"
      },
      "source": [
        "### CITATIONS\n",
        "\n",
        "- \"Attention is All You Need.\" \n",
        " - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS (2017). *https://research.google/pubs/pub46201/*\n",
        "\n",
        "- \"Convolutional Sequence to Sequence Learning.\"\n",
        " \n",
        "  - Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y.N.. (2017). Convolutional Sequence to Sequence Learning. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:1243-1252 Available from *http://proceedings.mlr.press/v70/gehring17a.html.*\n",
        "\n",
        "\n",
        "-  \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\"\n",
        "  -  Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. & Bengio, Y.. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2048-2057 Available from *http://proceedings.mlr.press/v37/xuc15.html.* \n",
        "            \n",
        "\n",
        "- \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\"\n",
        "\n",
        "  - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Preprint (2021). Available at *https://arxiv.org/abs/2101.11986*.\n",
        "\n",
        "- Special thanks to [Darien Schettler](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook.) for leading readers to the \"Show\" and \"Attention\" papers cited above, and sharing his progress at various stages in the competition through public Kaggle notebooks. My work includes a few of his hyperparameter choices and selection of EfficientNet transfer model. I did not read or use his implementations of the papers above.\n",
        "\n",
        "- It is possible my idea of a Beam Search Alternative is based on a lecture video from DeepLearning.ai's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  on Coursera.\n",
        "\n",
        "- **Dataset / Kaggle Competition:** \"Bristol-Myers Squibb – Molecular Translation\" competition on Kaggle (2021). *https://www.kaggle.com/c/bms-molecular-translation*\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csshw-ehbeQY"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. [Imports](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=TjuUOVXao__C&line=4&uniqifier=1)\n",
        "2. [Data Pipeline](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=lrLHKs5Ni7Sz)\n",
        "3. [Model Layers](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=W0T-u0vZamI8)\n",
        "    - [InChI Encoding](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=DYApmA2lf1hp&line=1&uniqifier=1)\n",
        "    - [Image Encoding and Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=FESofcGdEaWF&line=1&uniqifier=1)\n",
        "    - [Decoder Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=6qFDs9RTjvod&line=1&uniqifier=1)\n",
        "    - [Joint Encoder-Decoder Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=jP-t1MkKnD5L)\n",
        "    - [Decoder Head (Prediction Output)](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=38GA7wtNEhqW&line=1&uniqifier=1)\n",
        "    - [Update Mechanism](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=_2UR1DLljD0S&line=1&uniqifier=1)\n",
        "4. [Full Model](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=D6GIs3f3rpu0&line=1&uniqifier=1)\n",
        "5. [Training](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=otxdN02mf1ht&line=1&uniqifier=1)\n",
        "6. [Inference](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=Sbvzr5rdmjgs&line=5&uniqifier=1)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjuUOVXao__C",
        "outputId": "8b24c1a3-7690-4ef1-e958-f673598c1914"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.data import TFRecordDataset\n",
        "from tensorflow.data.experimental import TFRecordWriter\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Kaggle (for TPU)\n",
        "#from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "!pip install -U tensorboard_plugin_profile\n",
        "%load_ext tensorboard\n",
        "\n",
        "\"\"\"\n",
        "# Debugger\n",
        "tf.debugging.experimental.enable_dump_debug_info('./logs/', tensor_debug_mode=\"FULL_HEALTH\", \n",
        "                                                 circular_buffer_size=-1)\n",
        "\"\"\"\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# file management\n",
        "import os"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboard_plugin_profile in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (57.0.0)\n",
            "Requirement already satisfied, skipping upgrade: gviz-api>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.9.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUS3jc1o1Nq8"
      },
      "source": [
        "## Model parameters\n",
        "\n",
        "The 'ModelParameters' class manages global hyperparamaters for portability between Colab and Kaggle notebook environments. Once set, all other cells will run on either platform.\n",
        "\n",
        "On Colab, connection to my personal Google Drive is required, as ModelParameters will extract the dataset from a zip file to the hosted environment. This process may take several minutes. (It would not be difficult for the reader to update the code to point to their own drive and download the zip dataset using the Kaggle API code below.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "q6FUA_lvrd4Y",
        "outputId": "9c868be9-b5b5-4b1d-ed7e-79db949db075"
      },
      "source": [
        "\"\"\" Kaggle api for download the compressed dataset from Kaggle's servers.\n",
        "\n",
        "# imports\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "\n",
        "# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\n",
        "# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\n",
        "\"\"\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Kaggle api for download the compressed dataset from Kaggle's servers.\\n\\n# imports\\n!pip uninstall -y kaggle\\n!pip install --upgrade pip\\n!pip install kaggle==1.5.6\\n\\n# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\\n# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\\nos.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7okJ4Do3bIZQ",
        "outputId": "4690339c-8a38-4622-88c6-b6108a7ff878"
      },
      "source": [
        "# check for TPU & initialize\n",
        "try:\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "    tf.config.experimental_connect_to_cluster(resolver)\n",
        "\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
        "\n",
        "    STRATEGY = tf.distribute.TPUStrategy(resolver)\n",
        "    TPU = True\n",
        "    os.environ[\"TFHUB_MODEL_LOAD_FORMAT\"] = \"UNCOMPRESSED\"  # for TF Hub models on TPU\n",
        "\n",
        "    # set mixed precision type\n",
        "    PRECISION_TYPE = 'mixed_bfloat16'\n",
        "\n",
        "    # extra imports for GCS\n",
        "    !pip install -q fsspec\n",
        "    !pip install -q gcsfs\n",
        "    import fsspec, gcsfs \n",
        "\n",
        "except ValueError:\n",
        "    TPU = False\n",
        "    STRATEGY = tf.distribute.get_strategy()\n",
        "    PRECISION_TYPE = 'mixed_float16'\n",
        "\n",
        "# set mixed precision policy\n",
        "tf.keras.mixed_precision.set_global_policy(PRECISION_TYPE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Mixed precision compatibility check (mixed_float16): WARNING\n",
            "Your GPU may run slowly with dtype policy mixed_float16 because it does not have compute capability of at least 7.0. Your GPU:\n",
            "  Tesla P100-PCIE-16GB, compute capability 6.0\n",
            "See https://developer.nvidia.com/cuda-gpus for a list of GPUs and their compute capabilities.\n",
            "If you will use compatible GPU(s) not attached to this host, e.g. by running a multi-worker model, you can ignore this warning. This message will only be logged once\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTkL38FRtUfQ"
      },
      "source": [
        "class ModelParameters:\n",
        "    def __init__(self, cloud_server='kaggle'):\n",
        "               \n",
        "        # universal parameters\n",
        "        self._batch_size = 16  # note: increased below on TPU\n",
        "        self._inference_batch_size = 256\n",
        "        self._padded_length = 200\n",
        "        self._image_size = (320, 320)  # shape to process images in data pipeline, matches HUB model\n",
        "        self.SOS_string = 'InChI=1S/'  # start of sentence value\n",
        "        self.EOS_string = '<EOS>'  # end of sentence value\n",
        "        self._strategy = STRATEGY\n",
        "        self._precision_type = PRECISION_TYPE\n",
        "        self._tpu = TPU\n",
        "\n",
        "        # File Paths       \n",
        "        if cloud_server == 'colab':  # Google Colab\n",
        "            \n",
        "            if self._tpu:\n",
        "                self._batch_size = 16 * self._strategy.num_replicas_in_sync\n",
        "\n",
        "                # TPU file structure (via Kaggle GCS folder)\n",
        "                self._dataset_dir = 'gs://kds-081663de130be3ab70a31d8099e476f26c1e39c7fad148fa904bb6e3' # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "                self._prepared_files_dir = 'gs://kds-1c4d8f964095d35319af0fa89901679f61fbb70015e6ae53d80b614d'  # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('periodic-table')\n",
        "                self._tfrec_dir = 'gs://kds-452fce33df15e88f971a264d53595c6ed86dc34bba28e9cdcaf4cb2a'  # from Kaggle. Get updated directory on Kaggle via KaggleDatasets().get_gcs_path('bmsshards')\n",
        "                self._checkpoint_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/'  # gdrive\n",
        "                self._load_checkpoint_dir = './'\n",
        "                self._csv_save_dir = './'\n",
        "\n",
        "            else:\n",
        "\n",
        "                # load drive\n",
        "                from google.colab import drive\n",
        "                drive.mount('/content/gdrive/') \n",
        "\n",
        "                # unzip data\n",
        "                if not os.path.isdir('/content/bms-molecular-translation'):\n",
        "                    !unzip -q /content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/bms-molecular-translation.zip -d '/content/bms-molecular-translation'\n",
        "                \n",
        "                # file paths\n",
        "                self._dataset_dir = 'bms-molecular-translation/'\n",
        "                self._prepared_files_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/'\n",
        "                self._checkpoint_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/checkpoints/'\n",
        "                self._load_checkpoint_dir = self._checkpoint_dir\n",
        "                self._csv_save_dir = self._prepared_files_dir \n",
        "                self._tfrec_dir = None\n",
        "                \n",
        "        elif cloud_server == 'kaggle': # Kaggle cloud notebook (CPU / GPU)\n",
        "            from kaggle_datasets import KaggleDatasets\n",
        "            \n",
        "            # check for TPU \n",
        "            if self._tpu: \n",
        "                \n",
        "                self._batch_size = 8 * self._batch_size\n",
        "                \n",
        "                # file paths\n",
        "                self._dataset_dir = KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "                self._prepared_files_dir = KaggleDatasets().get_gcs_path('periodic-table')\n",
        "                self._tfrec_dir = KaggleDatasets().get_gcs_path('bmsshards')\n",
        "                self._checkpoint_dir = './'\n",
        "                self._load_checkpoint_dir = './'\n",
        "                self._csv_save_dir = './'\n",
        "\n",
        "            # set GPU instance info\n",
        "            else:  \n",
        "\n",
        "                # file paths\n",
        "                self._dataset_dir = '../input/bms-molecular-translation/'\n",
        "                self._prepared_files_dir = '../input/periodic-table/'\n",
        "                self._tfrec_dir = '../input/bmsshards/'\n",
        "                self._checkpoint_dir = './'\n",
        "                self._load_checkpoint_dir = '../input/k/mvenou/bms-molecular-translation/checkpoints/'\n",
        "                self._csv_save_dir = './'\n",
        "                self._tfrec_dir = None\n",
        "\n",
        "        # common file paths\n",
        "        self._periodic_table_csv = os.path.join(self._prepared_files_dir, 'periodic_table_elements.csv')\n",
        "        self._vocab_csv = os.path.join(self._prepared_files_dir, 'vocab.csv')        \n",
        "        self._test_images_dir = os.path.join(self._dataset_dir, 'test/')\n",
        "        self._train_images_dir = os.path.join(self._dataset_dir, 'train/')\n",
        "        self._extra_labels_csv = os.path.join(self._dataset_dir, 'extra_approved_InChIs.csv')\n",
        "        self._train_labels_csv = os.path.join(self._dataset_dir, 'train_labels.csv')\n",
        "        self._sample_submission_csv = os.path.join(self._dataset_dir, 'sample_submission.csv')\n",
        "        \n",
        "    # functions to access params\n",
        "    def padded_length(self):\n",
        "        return self._padded_length\n",
        "    def mixed_precision(self):\n",
        "        return self._precision_type\n",
        "    def tpu(self):\n",
        "        return self._tpu\n",
        "    def tfrec_dir(self):\n",
        "        return self._tfrec_dir\n",
        "    def cloud_server(self):\n",
        "        return self._cloud_server\n",
        "    def strategy(self):\n",
        "        return self._strategy\n",
        "    def csv_save_dir(self):\n",
        "        return self._csv_save_dir\n",
        "    def train_labels_csv(self):\n",
        "        return self._train_labels_csv\n",
        "    def vocab_csv(self):\n",
        "        return self._vocab_csv\n",
        "    def periodic_table_csv(self):\n",
        "        return self._periodic_table_csv\n",
        "    def batch_size(self):\n",
        "        return self._batch_size  \n",
        "    def inference_batch_size(self):\n",
        "        return self._inference_batch_size\n",
        "    def image_size(self):\n",
        "        return self._image_size    \n",
        "    def SOS(self):\n",
        "        return self.SOS_string\n",
        "    def EOS(self):\n",
        "        return self.EOS_string\n",
        "    def train_images_dir(self):\n",
        "        return self._train_images_dir\n",
        "    def test_images_dir(self):\n",
        "        return self._test_images_dir   \n",
        "    def checkpoint_dir(self):\n",
        "        return self._checkpoint_dir\n",
        "    def load_checkpoint_dir(self):\n",
        "        return self._load_checkpoint_dir\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sFuCtyTUQwh"
      },
      "source": [
        "Initialize Parameter Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a301N_Gf1hl",
        "outputId": "ed76b63a-3fc5-4b30-8fdd-975e41f883e4"
      },
      "source": [
        "PARAMETERS = ModelParameters(cloud_server='colab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLHKs5Ni7Sz"
      },
      "source": [
        "# **Input Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgq0W_hZUX-r"
      },
      "source": [
        "Load train labels as DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "21sUOSyff1hm",
        "outputId": "76a9464c-6f10-41ba-d6a9-3227cb2562f9"
      },
      "source": [
        "# Load CSV as dataframe\n",
        "train_labels_df = pd.read_csv(PARAMETERS.train_labels_csv())\n",
        "train_labels_df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>InChI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000011a64c74</td>\n",
              "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000019cc0cd2</td>\n",
              "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000252b6d2b</td>\n",
              "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000026b49b7e</td>\n",
              "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000026fc6c36</td>\n",
              "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_id                                              InChI\n",
              "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...\n",
              "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...\n",
              "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...\n",
              "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...\n",
              "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1PolKXhNOy"
      },
      "source": [
        "### InChI Text Parsing\n",
        "\n",
        "We split each InChI label into its \"vocabulary\" of logical subunits, consisting of element abbreviations numbers, common symbols and the required string 'InChI=1S/', which is at the start of every InChI label. We want to narrow down this vocabulary to the smallest set represented in our training data. The functions below provide a system for finding this minimal set, as well as preparing a new CSV file with parsed labels ready to be fed into a tokenizer layer.\n",
        "\n",
        "(For clarity and to reduce reliance on loading external files, the true code has been commented out and replaced with corresponding hard-coded values.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQiYkMHIrXmo"
      },
      "source": [
        "def inchi_parsing_regex(parameters=PARAMETERS):\n",
        "    # regex for spliting on InChi, but preserving chemical element abbreviations and three-digit numbers\n",
        "    \n",
        "    # shortcut: hard coded values\n",
        "    vocab = [parameters.EOS(), parameters.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'Br', 'B', 'Cl', 'C', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'Si', 'S', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "    vocab += [str(num) for num in reversed(range(168))]\n",
        "    vocab = [re.escape(val) for val in vocab]\n",
        "       \n",
        "    \"\"\" # to create vocab from scratch, use:\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # load list of elements we should search for within InChI strings: \n",
        "    periodic_elements = pd.read_csv(PARAMETERS.periodic_table_csv(), header=None)[1].to_list()\n",
        "    periodic_elements = periodic_elements + [val.lower() for val in periodic_elements] + [val.upper() for val in periodic_elements]\n",
        "    \n",
        "    punctuation = list(string.punctuation)\n",
        "    punctuation = [re.escape(val) for val in punctuation]   # update values with regex escape chars added as needed\n",
        "\n",
        "    three_dig_nums_list = [str(i) for i in range(1000, -1, -1)]\n",
        "\n",
        "    vocab = [SOS, EOS] + periodic_elements + three_dig_nums_list + punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    split_elements_regex = rf\"({'|'.join(vocab)})\"\n",
        "    \n",
        "    return split_elements_regex"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_57mCRgimRj"
      },
      "source": [
        "INCHI_PARSING_REGEX = inchi_parsing_regex()\n",
        "\n",
        "def parse_InChI(texts, parsing_regex=INCHI_PARSING_REGEX):  \n",
        "    return ' '.join(re.findall(parsing_regex, texts))\n",
        "\n",
        "\n",
        "# TF dataset map-compatible version\n",
        "def parse_InChI_py_fn(texts, parsing_regex=INCHI_PARSING_REGEX):\n",
        "    def tf_parse_InChI(texts):  \n",
        "        texts = np.char.array(texts.numpy())\n",
        "        texts = np.char.decode(texts).tolist()\n",
        "        texts = tf.constant([parse_InChI(val) for val in texts])\n",
        "        return tf.squeeze(texts)\n",
        "    return tf.py_function(func=tf_parse_InChI, inp=[texts], Tout=tf.string)\n",
        "\n",
        "\n",
        "# extracts filepath from image name\n",
        "def path_from_image_id(x, root_folder):\n",
        "    folder_a = tf.strings.substr(x, pos=0, len=1)\n",
        "    folder_b = tf.strings.substr(x, pos=1, len=1)\n",
        "    folder_c = tf.strings.substr(x, pos=2, len=1)\n",
        "    filename =  tf.strings.join([x, '.png'])\n",
        "    return tf.strings.join([root_folder, folder_a, folder_b, folder_c, filename], separator='/')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcP1tFRrvyR"
      },
      "source": [
        "Tokenizer\n",
        "\n",
        "Note: This must be kept outside the model (and used in dataset prep) for TPU compatability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDY6kY8Mf1hq"
      },
      "source": [
        "def Tokenizer(parameters):\n",
        "    \"\"\" note: crops /pads to max len\n",
        "    \"\"\"\n",
        "\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    padded_length = PARAMETERS.padded_length()\n",
        "    \n",
        "    # Create vocabulary for tokenizer\n",
        "    def create_vocab():       \n",
        "        hard_coded_vocab = [PARAMETERS.EOS(), PARAMETERS.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'B', 'Br',  'C', 'Cl', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'S', 'Si', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "        numbers = [str(num) for num in range(168)]\n",
        "        \n",
        "        vocab = hard_coded_vocab + numbers\n",
        "        \n",
        "        \"\"\"\n",
        "        # get from saved file\n",
        "        vocab = pd.read_csv(PARAMETERS.vocab_csv())['vocab_value'].to_list()   \n",
        "        vocab = list(vocab)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" \n",
        "        # To create from scratch, extract all vocab elements appearing in train set:\n",
        "        df = pd.read_csv(PARAMETERS.train_labels_csv())  \n",
        "        seg_len = 250000\n",
        "        num_breaks = len(df) // seg_len\n",
        "\n",
        "        vocab = set()\n",
        "        for i in range(num_breaks):\n",
        "\n",
        "            df_i =  df['InChI'].iloc[seg_len * i: seg_len * (i+1)]\n",
        "            texts =  df_i.apply(lambda x: set(parse_InChI(x).split()))\n",
        "            texts = texts.tolist()\n",
        "\n",
        "            vocab = vocab.union(*texts)\n",
        "\n",
        "            print(f'completed {i} / {num_breaks}')\n",
        "\n",
        "        vocab = list(vocab)\n",
        "        vocab_df = pd.DataFrame({'vocab_value': vocab})\n",
        "\n",
        "        # save results\n",
        "        filename = os.path.join(PARAMETERS.csv_save_dir(), 'vocab.csv')\n",
        "        vocab_df.to_csv(filename, index=False)\n",
        "        \"\"\"\n",
        "               \n",
        "        return vocab\n",
        "\n",
        "    vocab = create_vocab()\n",
        "    \n",
        "    # create tokenizer\n",
        "    tokenizer_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        standardize=None, split=lambda x: tf.strings.split(x, sep=' ', maxsplit=-1), \n",
        "        output_mode='int', output_sequence_length=padded_length, vocabulary=vocab)\n",
        "\n",
        "    # record EOS token\n",
        "    tokenized_EOS = tokenizer_layer(tf.constant([EOS]))\n",
        "    \n",
        "    # create inverse (de-tokenizer)\n",
        "    inverse_tokenizer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=tokenizer_layer.get_vocabulary(), invert=True)\n",
        "\n",
        "    return tokenizer_layer, inverse_tokenizer, tokenized_EOS"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6ghpYe6l7W"
      },
      "source": [
        "TOKENIZER_LAYER, INVERSE_TOKENIZER, TOKENIZED_EOS = \\\n",
        "    Tokenizer(parameters=PARAMETERS)\n",
        "\n",
        "def tokenize_text(w, x, y, z):\n",
        "    # note: requires batch dim\n",
        "    y = TOKENIZER_LAYER(y)\n",
        "    return w, x, y, z"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rk6HKexVmY-"
      },
      "source": [
        "Image Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4VcUWGHVB7z"
      },
      "source": [
        "# Image loaders\n",
        "def load_image(image_path):\n",
        "    image_path = tf.squeeze(image_path)\n",
        "    image = keras.layers.Lambda(lambda x: tf.io.read_file(x))(image_path)\n",
        "    return image   \n",
        "\n",
        "def decode_image(image, target_size):\n",
        "    image = keras.layers.Lambda(lambda x: tf.io.decode_image(x, channels=1, expand_animations=False))(image)\n",
        "    image = keras.layers.experimental.preprocessing.Resizing(*target_size)(image)\n",
        "    return image    "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2ecfS_hR9k"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Here we create efficient tf.data.Dataset train / validation / test sets.\n",
        "\n",
        "Out data pipeline will read our prepared CSV of (image filename, parsed InChI and standard InChI) tuples. (If this file is not found, it will be created from scratch. This may take several minutes)  Iterating through the list, it will load batches of corresponding images and labels.\n",
        "\n",
        "Our datasets contain the following information, accessible by dict keys: images, image_id, InChI, parsed_InChI. (The test set uses InChI = parsed_InChI = 'InChI=1S/', the known required stating value for any InChI code.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDRJElyOf1hn"
      },
      "source": [
        "def data_generator(image_set, parameters=PARAMETERS, labels_df=None, decode_images=True):\n",
        "       \n",
        "    # get global params\n",
        "    batch_size = parameters.batch_size()\n",
        "    inference_batch_size = parameters.inference_batch_size()\n",
        "    target_size = parameters.image_size()\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # dataset options\n",
        "    prefetch_size = tf.data.AUTOTUNE \n",
        "    inference_prefetch_size = tf.data.AUTOTUNE    \n",
        "    options = tf.data.Options()\n",
        "    options.experimental_optimization.autotune_buffers = True\n",
        "    options.experimental_optimization.map_vectorization.enabled = True\n",
        "    options.experimental_deterministic = False\n",
        "    options.experimental_slack = True\n",
        "        \n",
        "    # Train & Validation Datasets\n",
        "    if image_set in ['train', 'valid']:\n",
        "        root_folder = parameters.train_images_dir()  # train / valid images\n",
        "        valid_split = 0.10\n",
        "        \n",
        "        # load labels into memory as dataframe\n",
        "        if labels_df is None:\n",
        "            labels_df = pd.read_csv(parameters.train_labels_csv())\n",
        "\n",
        "        # test / train split\n",
        "        num_valid_samples = int(valid_split * len(labels_df))\n",
        "        train_df = labels_df.iloc[num_valid_samples: ]  # get train split\n",
        "        valid_df = labels_df.iloc[: num_valid_samples]  # get validation split\n",
        "\n",
        "        # shuffle\n",
        "        train_df = train_df.sample(frac=1)\n",
        "        valid_df = valid_df.sample(frac=1)\n",
        "\n",
        "        # load into datasets  # (image_id, InChI)\n",
        "        train_ds = tf.data.Dataset.from_tensor_slices(train_df.values)\n",
        "        valid_ds = tf.data.Dataset.from_tensor_slices(valid_df.values)\n",
        "\n",
        "        train_ds = train_ds.with_options(options)\n",
        "        valid_ds = valid_ds.with_options(options)\n",
        "\n",
        "        # update image paths  \n",
        "        def map_path(x):  # (image_path, image_id, InChI)\n",
        "            image_id = x[0]\n",
        "            image_path = path_from_image_id(image_id, root_folder)\n",
        "            return image_path, x[0], x[1]\n",
        "\n",
        "        train_ds = train_ds.map(map_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_path, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        def map_parse(x, y, z):  # (image_path, image_id, InChI)\n",
        "            parsed_InChI = parse_InChI_py_fn(z)\n",
        "            return x, y, parsed_InChI, z\n",
        "   \n",
        "        train_ds = train_ds.map(map_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_parse, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                \n",
        "        # load images into dataset       \n",
        "        def open_images(w, x, y, z):\n",
        "            w = load_image(w)\n",
        "            return w, x, y, z\n",
        "        \n",
        "        train_ds = train_ds.map(open_images, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(open_images, num_parallel_calls=tf.data.AUTOTUNE)    \n",
        "\n",
        "        # PREFETCH dataset BEFORE decoding images\n",
        "        train_ds = train_ds.prefetch(prefetch_size)\n",
        "        valid_ds = valid_ds.prefetch(prefetch_size)\n",
        "\n",
        "        def decode(w, x, y, z):\n",
        "            w = decode_image(w, target_size)\n",
        "            return w, x, y, z\n",
        "\n",
        "        if decode_images:\n",
        "            train_ds = train_ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "            valid_ds = valid_ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)    \n",
        "\n",
        "        # BATCH dataset AFTER decoding images (required by tf.io)\n",
        "        # should batch before other pure TF Lambda layer ops\n",
        "        train_ds = train_ds.batch(batch_size, drop_remainder=True,\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "        valid_ds = valid_ds.batch(batch_size, drop_remainder=True,\n",
        "                        num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
        "        \n",
        "        # add extra \"EOS\" values to end of parsed inchi\n",
        "        def extend_EOS(w, x, y, z):\n",
        "            y = tf.strings.join([y, EOS, EOS, EOS, EOS, EOS], separator=' ')\n",
        "            y = tf.reshape(y, [-1])\n",
        "            return w, x, y, z\n",
        "\n",
        "        train_ds = train_ds.map(extend_EOS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(extend_EOS, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # Tokenize parsed_inchi.  Note: ds must be batched before this step (size=1 is ok) \n",
        "        train_ds = train_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        # name the elements\n",
        "        def map_names(w, x, y, z):\n",
        "            return  {'image': w, 'image_id': x, 'tokenized_InChI': y, 'InChI': z}\n",
        "        \n",
        "        train_ds = train_ds.map(map_names, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        valid_ds = valid_ds.map(map_names, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        return train_ds, valid_ds\n",
        "    \n",
        "    # Test Dataset\n",
        "    elif image_set == 'test':\n",
        "\n",
        "        # note: image resizing and batching done during this loading step\n",
        "        # other elements must be batched before combining\n",
        "        image_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            directory=parameters.test_images_dir(), labels='inferred', label_mode=None,\n",
        "            class_names=None, color_mode='grayscale', batch_size=inference_batch_size, \n",
        "            image_size=target_size, shuffle=False, seed=None, validation_split=None, \n",
        "            subset=None, follow_links=False)\n",
        "\n",
        "        if not decode_images:  \n",
        "            # convert image to raw byte string\n",
        "            image_ds = image_ds.unbatch()\n",
        "            image_ds = image_ds.map(lambda image: tf.io.encode_png(image))\n",
        "            image_ds = image_ds.batch(inference_batch_size)\n",
        "\n",
        "        # set filenames as label and batch\n",
        "        image_id_ds = tf.data.Dataset.from_tensor_slices(image_ds.file_paths)\n",
        "        image_id_ds = image_id_ds.map(lambda x: tf.strings.split(x, os.path.sep)[-1],\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        image_id_ds = image_id_ds.batch(inference_batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "                \n",
        "        # set InChI label as start value 'InChI=1S/' and batch\n",
        "        inchi_ds = image_id_ds.map(lambda x: tf.constant(SOS, dtype=tf.string),\n",
        "                                   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        inchi_ds = inchi_ds.batch(inference_batch_size, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # merge datasets\n",
        "        test_ds = tf.data.Dataset.zip((image_ds, image_id_ds, inchi_ds, inchi_ds))\n",
        "\n",
        "        # prefetch images AFTER data 'test_ds.file_paths'\n",
        "        test_ds = test_ds.prefetch(inference_prefetch_size)\n",
        "\n",
        "        # Tokenize parsed_inchi.  Note: ds must be batched before this step (size=1 is ok) \n",
        "        test_ds = test_ds.map(tokenize_text, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # set key names\n",
        "        test_ds = test_ds.map(lambda w, x, y, z: {'image': w, \n",
        "                                                  'image_id': x, \n",
        "                                                  'tokenized_InChI': y,\n",
        "                                                  'InChI': z},\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        return test_ds"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCaO3A_5f1ho"
      },
      "source": [
        "Create Test, Train and Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvHSKTapob8V"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "    train_ds, valid_ds = data_generator('train', parameters=PARAMETERS, labels_df=train_labels_df, decode_images=True)\n",
        "    #test_ds = data_generator('test', parameters=PARAMETERS, labels_df=None, decode_images=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juAkO-oQaUml"
      },
      "source": [
        "Examine data shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKbpsKKrf1hp",
        "outputId": "fb0fead3-0a84-4da9-f538-7d591d263105"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "\n",
        "    print('Train DS')\n",
        "    for val in train_ds.take(1):    \n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    print('\\nValidation DS')\n",
        "    for val in valid_ds.take(1):\n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    try:\n",
        "        print('\\nTest DS')\n",
        "        for val in test_ds.take(1):\n",
        "            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train DS\n",
            "image: (16, 320, 320, 1) image_id: (16,) InChI: (16,) tokenized_InChI: (16, 200)\n",
            "\n",
            "Validation DS\n",
            "image: (16, 320, 320, 1) image_id: (16,) InChI: (16,) tokenized_InChI: (16, 200)\n",
            "\n",
            "Test DS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCYSm7N74i7"
      },
      "source": [
        "### TF Records Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgYgaPoO4gp"
      },
      "source": [
        "# Create TF Examples\n",
        "def make_example(image, image_id, tokenized_InChI, InChI):\n",
        "    image_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[image.numpy()])  # image provided as raw bytestring\n",
        "    )\n",
        "    image_id_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[image_id.numpy()])\n",
        "    )\n",
        "    tokenized_InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[tf.io.serialize_tensor(tokenized_InChI).numpy()])\n",
        "    )\n",
        "    InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[InChI.numpy()])\n",
        "    )\n",
        "\n",
        "    features = tf.train.Features(feature={\n",
        "        'image': image_feature,\n",
        "        'image_id': image_id_feature,\n",
        "        'tokenized_InChI': tokenized_InChI_feature,\n",
        "        'InChI': InChI_feature\n",
        "    })\n",
        "    \n",
        "    example = tf.train.Example(features=features)\n",
        "\n",
        "    return example.SerializeToString()\n",
        "\n",
        "\n",
        "def make_example_py_fn(image, image_id, InChI, tokenized_InChI):\n",
        "    return tf.py_function(func=make_example, \n",
        "                   inp=[image, image_id, InChI, tokenized_InChI], \n",
        "                   Tout=tf.string)\n",
        "\n",
        "\n",
        "# Decode TF Examples\n",
        "def decode_example(example, parameters=PARAMETERS):        \n",
        "    feature_description = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'image_id': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'tokenized_InChI': tf.io.FixedLenFeature([], tf.string),\n",
        "                           'InChI': tf.io.FixedLenFeature([], tf.string)}\n",
        "    \n",
        "    values = tf.io.parse_single_example(example, feature_description)\n",
        "    \n",
        "    \n",
        "    values['image'] = decode_image(values['image'], parameters.image_size())\n",
        "    values['tokenized_InChI'] = tf.io.parse_tensor(values['tokenized_InChI'],\n",
        "                                                  out_type=tf.int64)\n",
        "    values['tokenized_InChI'] = tf.cast(values['tokenized_InChI'], tf.int32)\n",
        "    \n",
        "    return values"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hURRE1cnGOTD"
      },
      "source": [
        "def serialized_dataset_gen(parameters=PARAMETERS, labels_df=train_labels_df):\n",
        "    \n",
        "    train_ds, valid_ds = data_generator(image_set='train', \n",
        "                                        parameters=parameters, \n",
        "                                        labels_df=train_labels_df, \n",
        "                                        decode_images=False)  # make sure not to decode images    \n",
        "    \n",
        "    train_ds = train_ds.unbatch()\n",
        "    valid_ds = valid_ds.unbatch()\n",
        "    \n",
        "    # Create TF Examples\n",
        "    train_ds = train_ds.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['tokenized_InChI'], x['InChI']), \n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    valid_ds = valid_ds.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['tokenized_InChI'], x['InChI']), \n",
        "                            num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    \n",
        "    return train_ds, valid_ds\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPDDDbqDj6BV"
      },
      "source": [
        "# Create TF Record Shards\n",
        "def create_records(dataset, subset, num_shards):\n",
        "    \n",
        "    folder = subset + '_tfrec'\n",
        "    \n",
        "    if subset =='train':\n",
        "        num_samples = int(.9 * len(train_labels_df))    # test / valid split\n",
        "    elif subset == 'valid':\n",
        "        num_samples = int(.1 * len(train_labels_df))\n",
        "    else:\n",
        "        num_samples = 2000000\n",
        "\n",
        "    if not os.path.isdir(folder):\n",
        "        os.mkdir(folder)\n",
        "        \n",
        "    for shard_num in range(num_shards):\n",
        "        \n",
        "        filename = os.path.join(folder, f'{subset}_shard_{shard_num+1}')\n",
        "        try:\n",
        "            this_shard = dataset.skip(shard_num * num_samples//num_shards).take(num_samples//num_shards)\n",
        "        \n",
        "            print(f'Writing shard {shard_num+1}/{num_shards} to {filename}')\n",
        "            writer = tf.data.experimental.TFRecordWriter(filename)\n",
        "            writer.write(this_shard)\n",
        "        except:\n",
        "            break\n",
        "    \n",
        "    return None \n",
        "    \n",
        "# Load dataset from saved TF Record Shards\n",
        "def dataset_from_records(subset, parameters=PARAMETERS):\n",
        "\n",
        "    # options\n",
        "    options = tf.data.Options()\n",
        "    options.experimental_deterministic = False\n",
        "    options.experimental_slack = True\n",
        "\n",
        "    # get TF Records\n",
        "    filepath = os.path.join(parameters.tfrec_dir(), \n",
        "                            subset + '_tfrec/*')\n",
        "    filenames = tf.io.gfile.glob(filepath)\n",
        "\n",
        "    # create dataset from records\n",
        "    dataset = tf.data.TFRecordDataset(filenames,\n",
        "                            num_parallel_reads=tf.data.experimental.AUTOTUNE)  \n",
        "    dataset = dataset.with_options(options)\n",
        " \n",
        "    \"\"\"\n",
        "    # merge the files\n",
        "    num_readers = parameters.strategy().num_replicas_in_sync\n",
        "    dataset = dataset.interleave(tf.data.TFRecordDataset,  \n",
        "                                 cycle_length=num_readers, block_length=1)\n",
        "    \"\"\"\n",
        "    # decode examples\n",
        "    dataset = dataset.map(decode_example, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # coerce unknown shape\n",
        "    dataset = dataset.map(lambda x: {'image':x['image'],\n",
        "                                     'image_id': x['image_id'],\n",
        "                                     'tokenized_InChI': tf.reshape(x['tokenized_InChI'], [-1]),\n",
        "                                     'InChI': x['InChI']},\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)  \n",
        "\n",
        "    dataset = dataset.batch(parameters.batch_size())\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "        \n",
        "    return dataset"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VxuZInXxj8Fn",
        "outputId": "7f107084-a458-4fd5-f946-35ef85e7daee"
      },
      "source": [
        "# To create TF_Records files\n",
        "# note: can take 8+ hours for train set alone!\n",
        "\"\"\"\n",
        "create_records(serial_valid_ds, subset='valid', num_shards=8)\n",
        "create_records(serial_train_ds, subset='train', num_shards=80)\n",
        "#create_records(test_ds, subset='test', num_shards=80)\n",
        "\"\"\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncreate_records(serial_valid_ds, subset='valid', num_shards=8)\\ncreate_records(serial_train_ds, subset='train', num_shards=80)\\n#create_records(test_ds, subset='test', num_shards=80)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SeQrwPyh1Ok"
      },
      "source": [
        "# IF USING TF_RECORDS:\n",
        "if PARAMETERS.tpu():\n",
        "    with PARAMETERS.strategy().scope(): \n",
        "        train_ds = dataset_from_records('train', parameters=PARAMETERS)\n",
        "        valid_ds = dataset_from_records('valid', parameters=PARAMETERS)\n",
        "\n",
        "    print('Train DS')\n",
        "    for val in train_ds.take(1):    \n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    print('\\nValidation DS')\n",
        "    for val in valid_ds.take(1):\n",
        "        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "\n",
        "    try:\n",
        "        print('\\nTest DS')\n",
        "        for val in test_ds.take(1):\n",
        "            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n",
        "                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0T-u0vZamI8"
      },
      "source": [
        "# **Model Layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYApmA2lf1hp"
      },
      "source": [
        "## InChI Encoding\n",
        "\n",
        "Tokenizer and Embedding to convert parsed InChI strings to tensors of numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMv38YQzf1hq"
      },
      "source": [
        "InChI Input Prep Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwhCP06Wf1hq"
      },
      "source": [
        "def InchiPrep(padded_length, max_len, name='InchiPrep'):\n",
        "    \"\"\" separates tokenized InChI into input / target pair \"\"\"\n",
        "        \n",
        "    # inputs\n",
        "    tokenized_inchi = keras.layers.Input([padded_length], dtype=tf.int32, name='tokenized_inchi')\n",
        "    inputs = [tokenized_inchi]\n",
        "\n",
        "    # crop to max length\n",
        "    tokenized_inchi = tokenized_inchi[:, :max_len]\n",
        "    \n",
        "    # split into input / target pairs\n",
        "    inchi_target = tokenized_inchi\n",
        "    \n",
        "    # inputs\n",
        "    inchi_input = keras.layers.Lambda(lambda x: x[:, :-1])(tokenized_inchi)\n",
        "    \n",
        "    # outpus\n",
        "    outputs = [inchi_input, inchi_target]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs6gv3Zbf1hq",
        "outputId": "ed1cdf02-008b-4862-e4d1-2c7921b633ba"
      },
      "source": [
        "temp_prep = InchiPrep(padded_length=TOKENIZER_LAYER.get_config()['output_sequence_length'],\n",
        "                      max_len=177)\n",
        "temp_prep.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"InchiPrep\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "tokenized_inchi (InputLayer) [(None, 200)]             0         \n",
            "_________________________________________________________________\n",
            "tf.__operators__.getitem (Sl (None, 177)               0         \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 176)               0         \n",
            "=================================================================\n",
            "Total params: 0\n",
            "Trainable params: 0\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FESofcGdEaWF"
      },
      "source": [
        "# Image Encoder\n",
        "\n",
        "Feature Extraction Step 1: Run the images through a pre-trained image network, extracting features as the output of an intermediate convolutional layer. [Technique from \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,\" cited at the top of this notebook.]  A dense layer is added for transfer learning and to control the dimension of the attention mechanism used later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMyUVvA9jBXp"
      },
      "source": [
        "Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMfF3H6iuEi_"
      },
      "source": [
        "# NOTE: Hub model doesn't work with mixed precision. Temporarily disable while loading.\n",
        "tf.keras.mixed_precision.set_global_policy('float32')  # removed mixed precision\n",
        "\n",
        "# NOTE: local loading required (for TPU)\n",
        "load_locally = tf.saved_model.LoadOptions(experimental_io_device='/job:localhost')\n",
        "\n",
        "# NOTE: must load model within distribution strategy with load options (for TPU)\n",
        "with PARAMETERS.strategy().scope():  # (\"distribution strategy\" defined at top of notebook)\n",
        "    PRETRAINED_MODEL = hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientdet/lite0/feature-vector/1\",\n",
        "                                      load_options=load_locally)\n",
        "\n",
        "# re-enable mixed precision\n",
        "tf.keras.mixed_precision.set_global_policy(PARAMETERS.mixed_precision())  "
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdTKFUFKfHeU",
        "outputId": "358e0a4d-b6fe-463c-d650-1331acbedac5"
      },
      "source": [
        "def TransferModel(name='TransferModel'):\n",
        "\n",
        "    # input\n",
        "    image_shape = (*PARAMETERS.image_size(), 1)\n",
        "    image = keras.layers.Input(shape=image_shape, name='image_input')\n",
        "    inputs = [image]\n",
        "\n",
        "    # standardize to required transfer model format\n",
        "    image = keras.layers.Lambda(lambda x: tf.image.grayscale_to_rgb(x))(image)\n",
        "    image = keras.layers.experimental.preprocessing.Resizing(height=320, width=320)(image)\n",
        "    image = keras.layers.experimental.preprocessing.Rescaling(scale=1./127.5, offset=-1)(image)\n",
        "    \n",
        "    # apply transfer model\n",
        "    image = PRETRAINED_MODEL(image)\n",
        "    \n",
        "    # feature vector options\n",
        "    image_vec_0 = image[0][0]\n",
        "    image_vec_0 = tf.keras.layers.Reshape(target_shape=[-1, image_vec_0.shape[-1]])(image_vec_0)\n",
        "    \n",
        "    #image_vec_1 = image[0][1]\n",
        "    #image_vec_1 = tf.keras.layers.Reshape(target_shape=[-1, image_vec_1.shape[-1]])(image_vec_1)\n",
        "    \n",
        "    #image_vec_2 = image[0][2]\n",
        "    #image_vec_2 = tf.keras.layers.Reshape(target_shape=[-1, image_vec_2.shape[-1]])(image_vec_2)\n",
        "    \n",
        "    #image_vec_3 = image[0][3]\n",
        "    #image_vec_3 = tf.keras.layers.Reshape(target_shape=[-1, image_vec_3.shape[-1]])(image_vec_3)\n",
        "    \n",
        "    #image_vec_4 = image[0][4]\n",
        "    #image_vec_4 = tf.keras.layers.Reshape(target_shape=[-1, image_vec_4.shape[-1]])(image_vec_4)\n",
        "\n",
        "    # combine\n",
        "    image_vectors = [image_vec_0]\n",
        "    if len(image_vectors) > 1:\n",
        "        image_vectors = keras.layers.Concatenate(-2)(image_vectors)\n",
        "    \n",
        "        outputs = [image_vectors]\n",
        "    else:\n",
        "        outputs = image_vectors\n",
        "\n",
        "    model = keras.Model(inputs, outputs, name=name)\n",
        "\n",
        "    return model\n",
        "\n",
        "TRANSFER_MODEL = TransferModel()\n",
        "TRANSFER_MODEL.summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"TransferModel\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image_input (InputLayer)     [(None, 320, 320, 1)]     0         \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 320, 320, 3)       0         \n",
            "_________________________________________________________________\n",
            "resizing (Resizing)          (None, 320, 320, 3)       0         \n",
            "_________________________________________________________________\n",
            "rescaling (Rescaling)        (None, 320, 320, 3)       0         \n",
            "_________________________________________________________________\n",
            "keras_layer (KerasLayer)     ([(None, 40, 40, 64), (No 3234464   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 1600, 64)          0         \n",
            "=================================================================\n",
            "Total params: 3,234,464\n",
            "Trainable params: 0\n",
            "Non-trainable params: 3,234,464\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3OKI_t3f1hr"
      },
      "source": [
        "def ImageEncoder(image_shape, output_dim, use_dense_top, name='ImageEncoder'):\n",
        "\n",
        "    # transfer model\n",
        "    features_model = TRANSFER_MODEL\n",
        "    features_model.trainable=False  # optional. My system doesn't have enough RAM to train this portion of the model, at a reasonable batch size\n",
        "    \n",
        "    # Inputs\n",
        "    image = keras.layers.Input(image_shape, name='image')\n",
        "    inputs = [image]\n",
        "    \n",
        "    # Model Path\n",
        "    image_features = image\n",
        "    image_features = features_model(image_features)\n",
        "    \n",
        "    features_dim = image_features.shape[-1]\n",
        "    image_features = keras.layers.Reshape([-1, features_dim])(image_features)\n",
        "    if use_dense_top:\n",
        "        image_features = tf.keras.layers.Permute([2,1])(image_features)\n",
        "        image_features = keras.layers.Dense(output_dim, activation='relu',\n",
        "                                            name='dense')(image_features)\n",
        "\n",
        "    outputs = [image_features]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyEeXgr5f1hr",
        "outputId": "47aa375c-bd3d-4a75-fd39-bdfc910d260d"
      },
      "source": [
        "ImageEncoder(image_shape=(224, 224,1), output_dim=208, use_dense_top=True).summary()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 320, 320, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 320, 320, 1), dtype=tf.float32, name='image_input'), name='image_input', description=\"created by layer 'image_input'\"), but it was called on an input with incompatible shape (None, 224, 224, 1).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 320, 320, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 320, 320, 1), dtype=tf.float32, name='image_input'), name='image_input', description=\"created by layer 'image_input'\"), but it was called on an input with incompatible shape (None, 224, 224, 1).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"ImageEncoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (InputLayer)           [(None, 224, 224, 1)]     0         \n",
            "_________________________________________________________________\n",
            "TransferModel (Functional)   (None, 1600, 64)          3234464   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 1600, 64)          0         \n",
            "_________________________________________________________________\n",
            "permute (Permute)            (None, 64, 1600)          0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64, 208)           333008    \n",
            "=================================================================\n",
            "Total params: 3,567,472\n",
            "Trainable params: 333,008\n",
            "Non-trainable params: 3,234,464\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGYH_nP3Efsk"
      },
      "source": [
        "## Encoder Attention\n",
        "\n",
        "Feature Extraction Step 2: Now that we have basic feature vectors, we use self-attention to generate more complex features. This is the encoding step used in \"Attention is All You Need,\" cited above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmq0PjxxjayJ"
      },
      "source": [
        "def EncoderAttention(num_blocks, encoder_feature_dim, num_att_elems, name='EncoderAttention'):\n",
        "\n",
        "    \"\"\" note: use num_blocks=6 to match \"Attention is All You Need\" \"\"\"\n",
        "\n",
        "    # inputs\n",
        "    encoder_vectors = keras.layers.Input([num_att_elems, encoder_feature_dim], name='encoder_vectors')   # from image encoder\n",
        "    inputs = [encoder_vectors]\n",
        "\n",
        "    # attention (uses \"Attention is All You Need\" structure, \n",
        "    # except without positional encoding because image feature vectors are unordered)\n",
        "    for i in range(num_blocks):\n",
        "\n",
        "        # encoder self-attention blocks\n",
        "        attention = tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=8, key_dim=encoder_feature_dim//8, name=f'encoder_attention_{i}')(  # uses 'num_heads * key_dim = rnn_units' from paper\n",
        "                    query=encoder_vectors, value=encoder_vectors)\n",
        "        \n",
        "        attention = keras.layers.Dropout(rate=.1)(attention)\n",
        "        attention = keras.layers.Add()([encoder_vectors, attention])\n",
        "        attention = keras.layers.BatchNormalization()(attention)    \n",
        "\n",
        "        # Feed Forward Block\n",
        "        encoder_vectors = keras.layers.Dense(encoder_feature_dim, 'relu',\n",
        "                                             name=f'dense_{i}')(attention)    \n",
        "        encoder_vectors = keras.layers.Dropout(rate=.1)(encoder_vectors)\n",
        "        encoder_vectors = keras.layers.Add()([attention, encoder_vectors])\n",
        "        encoder_vectors = keras.layers.BatchNormalization()(encoder_vectors)     \n",
        "\n",
        "    # output\n",
        "    outputs = [encoder_vectors]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g8x4_cpkJ9A",
        "outputId": "a7408e89-35dc-42eb-f90e-b1052f82d7ce"
      },
      "source": [
        "EncoderAttention(num_blocks=6, encoder_feature_dim=208, num_att_elems=64).summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"EncoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_vectors (InputLayer)    [(None, 64, 208)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_0 (MultiHeadA (None, 64, 208)      173888      encoder_vectors[0][0]            \n",
            "                                                                 encoder_vectors[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 64, 208)      0           encoder_attention_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add (Add)                       (None, 64, 208)      0           encoder_vectors[0][0]            \n",
            "                                                                 dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 64, 208)      832         add[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "dense_0 (Dense)                 (None, 64, 208)      43472       batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 64, 208)      0           dense_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_1 (Add)                     (None, 64, 208)      0           batch_normalization[0][0]        \n",
            "                                                                 dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 64, 208)      832         add_1[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_1 (MultiHeadA (None, 64, 208)      173888      batch_normalization_1[0][0]      \n",
            "                                                                 batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 64, 208)      0           encoder_attention_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_2 (Add)                     (None, 64, 208)      0           batch_normalization_1[0][0]      \n",
            "                                                                 dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 64, 208)      832         add_2[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 64, 208)      43472       batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_3 (Dropout)             (None, 64, 208)      0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_3 (Add)                     (None, 64, 208)      0           batch_normalization_2[0][0]      \n",
            "                                                                 dropout_3[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 64, 208)      832         add_3[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_2 (MultiHeadA (None, 64, 208)      173888      batch_normalization_3[0][0]      \n",
            "                                                                 batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_4 (Dropout)             (None, 64, 208)      0           encoder_attention_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_4 (Add)                     (None, 64, 208)      0           batch_normalization_3[0][0]      \n",
            "                                                                 dropout_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 64, 208)      832         add_4[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 64, 208)      43472       batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_5 (Dropout)             (None, 64, 208)      0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_5 (Add)                     (None, 64, 208)      0           batch_normalization_4[0][0]      \n",
            "                                                                 dropout_5[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 64, 208)      832         add_5[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_3 (MultiHeadA (None, 64, 208)      173888      batch_normalization_5[0][0]      \n",
            "                                                                 batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_6 (Dropout)             (None, 64, 208)      0           encoder_attention_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_6 (Add)                     (None, 64, 208)      0           batch_normalization_5[0][0]      \n",
            "                                                                 dropout_6[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 64, 208)      832         add_6[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 64, 208)      43472       batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_7 (Dropout)             (None, 64, 208)      0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_7 (Add)                     (None, 64, 208)      0           batch_normalization_6[0][0]      \n",
            "                                                                 dropout_7[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 64, 208)      832         add_7[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_4 (MultiHeadA (None, 64, 208)      173888      batch_normalization_7[0][0]      \n",
            "                                                                 batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_8 (Dropout)             (None, 64, 208)      0           encoder_attention_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_8 (Add)                     (None, 64, 208)      0           batch_normalization_7[0][0]      \n",
            "                                                                 dropout_8[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 64, 208)      832         add_8[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 64, 208)      43472       batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_9 (Dropout)             (None, 64, 208)      0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_9 (Add)                     (None, 64, 208)      0           batch_normalization_8[0][0]      \n",
            "                                                                 dropout_9[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 64, 208)      832         add_9[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention_5 (MultiHeadA (None, 64, 208)      173888      batch_normalization_9[0][0]      \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "dropout_10 (Dropout)            (None, 64, 208)      0           encoder_attention_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_10 (Add)                    (None, 64, 208)      0           batch_normalization_9[0][0]      \n",
            "                                                                 dropout_10[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 64, 208)      832         add_10[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 64, 208)      43472       batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_11 (Dropout)            (None, 64, 208)      0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_11 (Add)                    (None, 64, 208)      0           batch_normalization_10[0][0]     \n",
            "                                                                 dropout_11[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 64, 208)      832         add_11[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,314,144\n",
            "Trainable params: 1,309,152\n",
            "Non-trainable params: 4,992\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFDs9RTjvod"
      },
      "source": [
        "## Decoder Attention\n",
        "\n",
        "Text Feature extraction + Encoder/Decoder Joint Attention interaction.\n",
        "\n",
        "With use_covolutions set to False, this is the decoder self-attention feature-extraction step from \"Attention is All You Need,\" cited above (with learned positional encoding). \n",
        "\n",
        "Includes an (optional) parameter to add a small convolutional layer for feature enhancement before the attention layer. This is included for experimentation / verification that attention really is all you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqw6CPAIavh5"
      },
      "source": [
        "def DecoderEmbedding(embedding_dim, max_len, vocab_size,\n",
        "                     name='DecoderAttention'):\n",
        "    \n",
        "    \"\"\" Creates decoder features \"\"\"\n",
        "\n",
        "    # positional encoding variable\n",
        "    initializer = tf.random_normal_initializer()\n",
        "    position_enc = tf.Variable(initializer(shape=[max_len, embedding_dim]))\n",
        "    position_enc = tf.expand_dims(position_enc, 0)  # for broadcasting against batch\n",
        "\n",
        "    # inputs\n",
        "    tokenized_InChI = keras.layers.Input([max_len - 1], name='tokenized_InChI')  # zero=padded, missing start value\n",
        "    start_var = keras.layers.Input([1, embedding_dim])\n",
        "    inputs = [tokenized_InChI, start_var]\n",
        "\n",
        "    # Embedding\n",
        "    inchi_features = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
        "                    mask_zero=True, name='Embedding')(tokenized_InChI)\n",
        "\n",
        "    # add start variable\n",
        "    inchi_features = keras.layers.Reshape([-1, embedding_dim])(inchi_features)  # NOTE: Required layer! It doesn't actually change the shape. For some reason model won't build without it\n",
        "    inchi_features = keras.layers.Concatenate(1)([start_var, inchi_features])\n",
        "    \n",
        "    # Add positional encoding\n",
        "    inchi_features = keras.layers.Add()([position_enc, inchi_features])\n",
        "\n",
        "    outputs = [inchi_features]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)\n",
        "    "
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3XHlnkEb4gF",
        "outputId": "9e537d96-ee15-4aa0-a73c-02e61225b5d0"
      },
      "source": [
        "DecoderEmbedding(embedding_dim=208, max_len=200, vocab_size=109).summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "tokenized_InChI (InputLayer)    [(None, 199)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding (Embedding)           (None, 199, 208)     22672       tokenized_InChI[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 1, 208)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_2 (Reshape)             (None, 199, 208)     0           Embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 200, 208)     0           input_1[0][0]                    \n",
            "                                                                 reshape_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "add_12 (Add)                    (None, 200, 208)     0           concatenate[0][0]                \n",
            "==================================================================================================\n",
            "Total params: 22,672\n",
            "Trainable params: 22,672\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-t1MkKnD5L"
      },
      "source": [
        "##  Decoder + Joint Encoder-Decoder Attention\n",
        "\n",
        "This is the 'translation' component where our image features interacts with our (masked) text features. Masking is used to prevent information leak so only known text values are used at a given time step. This is the encoder-decoder attention step from \"Attention is All You Need.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m927YThukFs-"
      },
      "source": [
        "def DecoderAttention(num_blocks, encoder_units, decoder_units, num_encoder_vectors, max_len, \n",
        "                   use_convolutions, name='DecoderAttention'):\n",
        "    \n",
        "    \"\"\" note: use num_blocks = 6 to match \"Attention is All You Need\" \"\"\"\n",
        "\n",
        "    encoder_features = keras.layers.Input([num_encoder_vectors, encoder_units], name='encoder_features')   # from image\n",
        "    decoder_features = keras.layers.Input([max_len, decoder_units], name='decoder_features')   # from known text\n",
        "    mask = keras.layers.Input([max_len, max_len], name='mask')   # used to avoid leaking future info\n",
        "    inputs = [encoder_features, decoder_features, mask]\n",
        "\n",
        "    \n",
        "    # (Optional convolution feature extraction, for experimentation) \n",
        "    if use_convolutions:\n",
        "        \n",
        "        # crop to masked input\n",
        "        step = tf.math.argmin(mask[0, :, 0])\n",
        "        decoder_features = decoder_features[:, :step, :]\n",
        "\n",
        "        # apply convolutions\n",
        "        decoder_features = tf.keras.layers.Conv1D(filters=decoder_units, kernel_size=3, \n",
        "                    strides=1, padding='same', groups=1)(decoder_features)\n",
        "\n",
        "        # pad back to full size for (masked) Attention input\n",
        "        decoder_features = tf.pad(decoder_features, [[0,0], [0, max_len - step], [0,0]])\n",
        "    \n",
        "    # Attention (uses \"Attention is All You Need\" structure)\n",
        "    for i in range(num_blocks):\n",
        "\n",
        "        # Decoder Self-Attention Block (with mask)\n",
        "        decoder_attention = tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=8, key_dim=decoder_units//8, name=f'decoder_attention_{i}')(\n",
        "                    query=decoder_features, value=decoder_features, attention_mask=mask)\n",
        "\n",
        "        decoder_attention = keras.layers.Dropout(rate=.1)(decoder_attention)            \n",
        "        decoder_attention = keras.layers.Add()([decoder_features, decoder_attention])\n",
        "        decoder_attention = keras.layers.BatchNormalization()(decoder_attention)     \n",
        "        \n",
        "        # Encoder-Decoder Attention Block\n",
        "        joint_attention = tf.keras.layers.MultiHeadAttention(\n",
        "                num_heads=8, key_dim=decoder_units//8, name=f'joint_attention_{i}')(\n",
        "                    query=decoder_attention, value=encoder_features)\n",
        "                \n",
        "        joint_attention = keras.layers.Dropout(rate=.1)(joint_attention)          \n",
        "        joint_attention = keras.layers.Add()([decoder_attention, joint_attention])\n",
        "        joint_attention = keras.layers.BatchNormalization()(joint_attention)  \n",
        "\n",
        "        # Feed Forward Block\n",
        "        decoder_features = keras.layers.Dense(decoder_units, activation='relu',\n",
        "            kernel_initializer= tf.keras.initializers.HeNormal())(joint_attention)\n",
        "\n",
        "        decoder_features = keras.layers.Dropout(rate=.1)(decoder_features)\n",
        "        decoder_features = keras.layers.Add()([joint_attention, decoder_features])\n",
        "        decoder_features = keras.layers.BatchNormalization()(decoder_features)\n",
        "\n",
        "    outputs = [decoder_features]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcaTvpPWkqgO",
        "outputId": "9eb8b128-e806-4101-cdc0-dcda19093946"
      },
      "source": [
        "DecoderAttention(num_blocks=6, encoder_units = 208, decoder_units=208, num_encoder_vectors=50, \n",
        "               max_len=200, use_convolutions=False, name='DecoderAttention').summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_features (InputLayer)   [(None, 200, 208)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "mask (InputLayer)               [(None, 200, 200)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_0 (MultiHeadA (None, 200, 208)     173888      decoder_features[0][0]           \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 decoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_12 (Dropout)            (None, 200, 208)     0           decoder_attention_0[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_13 (Add)                    (None, 200, 208)     0           decoder_features[0][0]           \n",
            "                                                                 dropout_12[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 200, 208)     832         add_13[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "encoder_features (InputLayer)   [(None, 50, 208)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_0 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_12[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_13 (Dropout)            (None, 200, 208)     0           joint_attention_0[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_14 (Add)                    (None, 200, 208)     0           batch_normalization_12[0][0]     \n",
            "                                                                 dropout_13[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 200, 208)     832         add_14[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 200, 208)     43472       batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_14 (Dropout)            (None, 200, 208)     0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "add_15 (Add)                    (None, 200, 208)     0           batch_normalization_13[0][0]     \n",
            "                                                                 dropout_14[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 200, 208)     832         add_15[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_1 (MultiHeadA (None, 200, 208)     173888      batch_normalization_14[0][0]     \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_15 (Dropout)            (None, 200, 208)     0           decoder_attention_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_16 (Add)                    (None, 200, 208)     0           batch_normalization_14[0][0]     \n",
            "                                                                 dropout_15[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 200, 208)     832         add_16[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_1 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_15[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_16 (Dropout)            (None, 200, 208)     0           joint_attention_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_17 (Add)                    (None, 200, 208)     0           batch_normalization_15[0][0]     \n",
            "                                                                 dropout_16[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 200, 208)     832         add_17[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 200, 208)     43472       batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_17 (Dropout)            (None, 200, 208)     0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_18 (Add)                    (None, 200, 208)     0           batch_normalization_16[0][0]     \n",
            "                                                                 dropout_17[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 200, 208)     832         add_18[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_2 (MultiHeadA (None, 200, 208)     173888      batch_normalization_17[0][0]     \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_18 (Dropout)            (None, 200, 208)     0           decoder_attention_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_19 (Add)                    (None, 200, 208)     0           batch_normalization_17[0][0]     \n",
            "                                                                 dropout_18[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 200, 208)     832         add_19[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_2 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_18[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_19 (Dropout)            (None, 200, 208)     0           joint_attention_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_20 (Add)                    (None, 200, 208)     0           batch_normalization_18[0][0]     \n",
            "                                                                 dropout_19[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 200, 208)     832         add_20[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 200, 208)     43472       batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_20 (Dropout)            (None, 200, 208)     0           dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_21 (Add)                    (None, 200, 208)     0           batch_normalization_19[0][0]     \n",
            "                                                                 dropout_20[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 200, 208)     832         add_21[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_3 (MultiHeadA (None, 200, 208)     173888      batch_normalization_20[0][0]     \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_21 (Dropout)            (None, 200, 208)     0           decoder_attention_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_22 (Add)                    (None, 200, 208)     0           batch_normalization_20[0][0]     \n",
            "                                                                 dropout_21[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 200, 208)     832         add_22[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_3 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_21[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_22 (Dropout)            (None, 200, 208)     0           joint_attention_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_23 (Add)                    (None, 200, 208)     0           batch_normalization_21[0][0]     \n",
            "                                                                 dropout_22[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 200, 208)     832         add_23[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 200, 208)     43472       batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_23 (Dropout)            (None, 200, 208)     0           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 200, 208)     0           batch_normalization_22[0][0]     \n",
            "                                                                 dropout_23[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 200, 208)     832         add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_4 (MultiHeadA (None, 200, 208)     173888      batch_normalization_23[0][0]     \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_24 (Dropout)            (None, 200, 208)     0           decoder_attention_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 200, 208)     0           batch_normalization_23[0][0]     \n",
            "                                                                 dropout_24[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 200, 208)     832         add_25[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_4 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_24[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_25 (Dropout)            (None, 200, 208)     0           joint_attention_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 200, 208)     0           batch_normalization_24[0][0]     \n",
            "                                                                 dropout_25[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 200, 208)     832         add_26[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 200, 208)     43472       batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_26 (Dropout)            (None, 200, 208)     0           dense_4[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 200, 208)     0           batch_normalization_25[0][0]     \n",
            "                                                                 dropout_26[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 200, 208)     832         add_27[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention_5 (MultiHeadA (None, 200, 208)     173888      batch_normalization_26[0][0]     \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_27 (Dropout)            (None, 200, 208)     0           decoder_attention_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "add_28 (Add)                    (None, 200, 208)     0           batch_normalization_26[0][0]     \n",
            "                                                                 dropout_27[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 200, 208)     832         add_28[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention_5 (MultiHeadAtt (None, 200, 208)     173888      batch_normalization_27[0][0]     \n",
            "                                                                 encoder_features[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dropout_28 (Dropout)            (None, 200, 208)     0           joint_attention_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_29 (Add)                    (None, 200, 208)     0           batch_normalization_27[0][0]     \n",
            "                                                                 dropout_28[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 200, 208)     832         add_29[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_5 (Dense)                 (None, 200, 208)     43472       batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_29 (Dropout)            (None, 200, 208)     0           dense_5[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "add_30 (Add)                    (None, 200, 208)     0           batch_normalization_28[0][0]     \n",
            "                                                                 dropout_29[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 200, 208)     832         add_30[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 2,362,464\n",
            "Trainable params: 2,354,976\n",
            "Non-trainable params: 7,488\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GA7wtNEhqW"
      },
      "source": [
        "## Decoder Head (Prediction Output)\n",
        "\n",
        "This is where we use what was learned in the encoder-decoder attention to output predicted labels. It is the prediction step from \"Attention is All You Need.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2LMbmclf1hs"
      },
      "source": [
        "def DecoderHead(decoder_units, vocab_size, max_len, name='DecoderHead'):\n",
        "    \n",
        "    decoder_output = keras.layers.Input([max_len, decoder_units])  # from Decoder Attention layer\n",
        "\n",
        "    inputs = [decoder_output]\n",
        "\n",
        "    # Prediction Block               \n",
        "    # include activation dtype on final output layer for overriding mixed precision policies\n",
        "    decoder_output = keras.layers.Dense(vocab_size, activation=None, \n",
        "            kernel_initializer= tf.keras.initializers.HeNormal())(decoder_output)  \n",
        "\n",
        "    probs = keras.layers.Activation('softmax', dtype=tf.keras.mixed_precision.Policy('float32'), \n",
        "                                    name='decoder_prediction')(decoder_output)  \n",
        "\n",
        "    outputs = [probs]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js6yvxIGf1hs",
        "outputId": "4936c07b-95db-4b9d-c59d-dbda9ed9690a"
      },
      "source": [
        "DecoderHead(decoder_units=320, vocab_size=199, max_len=200).summary()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderHead\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 200, 320)]        0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 200, 199)          63879     \n",
            "_________________________________________________________________\n",
            "decoder_prediction (Activati (None, 200, 199)          0         \n",
            "=================================================================\n",
            "Total params: 63,879\n",
            "Trainable params: 63,879\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2UR1DLljD0S"
      },
      "source": [
        "## Update Mechanism (Optional)\n",
        "\n",
        "*Note: this is fully coded but I have not had time to train parameters with it. I leave that as a future opportunity for exploration.*\n",
        "\n",
        "NLP technicques typically output logits to find the highest likelhood token prediction. This can be improved to a (local) maximum likelihood selection using a \"beam step\" that ay override the initial prediction choice. \n",
        "\n",
        "This layer is an alternative system for updating predictions. Unlike \"beam,\" it is trainable and includes longer-range dependencies (instead of the very \"local\" beam step.) The entire original prediction is passed through a bidirectional RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpM3FS5zZ-3Y"
      },
      "source": [
        "def BeamUpdate(decoder_units, max_len, beam_rnn_units, input_dim, vocab_size, name='BeamUpdate'):\n",
        "    \n",
        "    # update to required GRU model dtypes\n",
        "    tf.keras.mixed_precision.set_global_policy('float32')\n",
        "    \n",
        "    # rnn layers\n",
        "    BeamUnit = keras.layers.GRU(beam_rnn_units, return_sequences=True, \n",
        "                                return_state=True, go_backwards=True)  # GRU doesn't appear compatible with bfloat\n",
        "    \n",
        "    # Inputs\n",
        "    beam_input = keras.layers.Input([max_len, input_dim], name='beam_input') \n",
        "    hidden_state = keras.layers.Input([decoder_units], name='hidden_state')\n",
        "    \n",
        "    inputs = [beam_input, hidden_state]\n",
        "\n",
        "    # downscale hidden state to beam dims\n",
        "    beam_hidden_state = keras.layers.Dense(beam_rnn_units, activation='relu',\n",
        "                              kernel_initializer= tf.keras.initializers.HeNormal())(hidden_state)   \n",
        "    # RNN\n",
        "    beam_out, beam_hidden_state = \\\n",
        "        BeamUnit(beam_input, initial_state=[beam_hidden_state])  # beam 1\n",
        "\n",
        "    \n",
        "    # logits\n",
        "    beam_out = keras.layers.Dense(vocab_size, activation=None, name='dense_beam_probs',\n",
        "                   kernel_initializer= tf.keras.initializers.HeNormal())(beam_out)\n",
        "    probs = keras.layers.Activation('softmax', dtype=tf.keras.mixed_precision.Policy('float32'), \n",
        "                                    name='beam_predictions')(beam_out)\n",
        "\n",
        "    outputs = [probs]\n",
        "\n",
        "    # revert to original mied precision policy\n",
        "    tf.keras.mixed_precision.set_global_policy(PARAMETERS.mixed_precision())\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnHn_q5xcsB7",
        "outputId": "9200a7be-dca6-4c79-a807-b40f2ba7302a"
      },
      "source": [
        "temp_beam = BeamUpdate(decoder_units=320, max_len=200, beam_rnn_units=128, input_dim=130, vocab_size=199, name='BeamUpdate')\n",
        "temp_beam.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BeamUpdate\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "hidden_state (InputLayer)       [(None, 320)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "beam_input (InputLayer)         [(None, 200, 130)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_7 (Dense)                 (None, 128)          41088       hidden_state[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "gru (GRU)                       [(None, 200, 128), ( 99840       beam_input[0][0]                 \n",
            "                                                                 dense_7[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_beam_probs (Dense)        (None, 200, 199)     25671       gru[0][0]                        \n",
            "__________________________________________________________________________________________________\n",
            "beam_predictions (Activation)   (None, 200, 199)     0           dense_beam_probs[0][0]           \n",
            "==================================================================================================\n",
            "Total params: 166,599\n",
            "Trainable params: 166,599\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6GIs3f3rpu0"
      },
      "source": [
        "# **Full Model**\n",
        "\n",
        "All the components are combined into a full encoder/decoder model. This is implemented using the subclassing API with custom call, train,  evaluation and prediction steps. Once initialized, the models have full access to high-level model.fit(), model.compile() and model.save_weights() methods.\n",
        "\n",
        "An extra features implemented is having Decoder() elements in *series* (not stacked). This adds more trainable parameters without affecting inference speed, and allows decoders to specialize more on different regions of the text.\n",
        "\n",
        "BaseTrainer() model has the BeamUpdate mechanism disabled. InchiGenerator() models include the BeamUpdate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcEV3ifLf1hs"
      },
      "source": [
        "class BaseTrainer(keras.Model):\n",
        "    \n",
        "    def __init__(self, batch_size, image_features_dim, decoder_units, \n",
        "                 num_encoder_blocks, num_decoder_blocks, \n",
        "                 use_dense_encoder_top, use_convolutions, use_dual_decoder, \n",
        "                 max_len, parameters, enable_beam=False, \n",
        "                 name='BaseTrainer', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        \"\"\" Beam updates turned off. Training conducted with teach-fed inputs.\n",
        "        note: dataset provided as (image, image_id, tokenized_InChI, InChI) \"\"\"\n",
        "        \n",
        "        # other params (required values)\n",
        "        self.max_len = max_len  # max number of token prediction length\n",
        "        self.image_features_dim = image_features_dim\n",
        "        self.decoder_units = decoder_units\n",
        "        self.num_encoder_blocks = num_encoder_blocks\n",
        "        self.num_decoder_blocks = num_decoder_blocks\n",
        "        self.use_dense_encoder_top = tf.constant(use_dense_encoder_top, tf.bool)\n",
        "        self.use_convolutions = tf.constant(use_convolutions, tf.bool)\n",
        "        self.use_dual_decoder = tf.constant(use_dual_decoder, tf.bool)\n",
        "        self.parameters = parameters\n",
        "        self.SOS = parameters.SOS()\n",
        "        self.EOS = parameters.EOS()\n",
        "        self.batch_size = batch_size  # NOTE: needed for TPU compat\n",
        "        \n",
        "        # tokenizer / inverse tokenizer\n",
        "        #self.tokenizer_layer = TOKENIZER_LAYER\n",
        "        #self.inverse_tokenizer = INVERSE_TOKENIZER\n",
        "        self.tokenized_EOS = TOKENIZED_EOS\n",
        "        self.padded_length = TOKENIZER_LAYER.get_config()['output_sequence_length']\n",
        "        self.vocab_size = TOKENIZER_LAYER.get_config()['vocabulary_size']\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'image_features_dim': self.image_features_dim,\n",
        "                  'decoder_units': self.decoder_units,\n",
        "                  'num_decoder_blocks': self.num_decoder_blocks,\n",
        "                  'num_encoder_blocks': self.num_encoder_blocks,\n",
        "                  'use_dense_encoder_top': self.use_dense_encoder_top,\n",
        "                  'use_convolutions': use_convolutions,\n",
        "                  'max_len': self.max_len,\n",
        "                  'use_dual_decoder': self.use_dual_decoder,\n",
        "                  'parameters': self.parameters}\n",
        "        return config \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # note: dataset prepared with dict keys (image, image_id, tokenized_InChI, InChI)\n",
        "        \n",
        "        #self.batch_size = input_shape['image'][0]  # NOTE: batch size is hard coded above for TPU compatability\n",
        "\n",
        "        # encoder\n",
        "        self.image_shape = input_shape['image'][1:]  # drops batch dims       \n",
        "        self.image_encoder = ImageEncoder(image_shape=self.image_shape, \n",
        "                                          output_dim=self.image_features_dim,\n",
        "                                          use_dense_top=self.use_dense_encoder_top,\n",
        "                                          name='ImageEncoder')   \n",
        "\n",
        "        # collect params\n",
        "        self.num_encoder_vectors = self.image_encoder.output_shape[-2]\n",
        "        self.embedding_dim = self.decoder_units  # required for consistency\n",
        "\n",
        "        # trainable start value\n",
        "        mean = 1.0 / self.embedding_dim\n",
        "        intializer = tf.random_normal_initializer(mean=mean, stddev=.2*mean)\n",
        "        self.start_var = tf.Variable(intializer(shape=[1, 1, self.embedding_dim]), \n",
        "                                     name='start_var')\n",
        "        \n",
        "        # InChI prep\n",
        "        self.inchi_prep = InchiPrep(padded_length=self.padded_length, \n",
        "                                    max_len=self.max_len, name='InchiPrep')\n",
        "\n",
        "        # attentions\n",
        "        self.encoder_attention = EncoderAttention(num_blocks=self.num_encoder_blocks, \n",
        "                                                  encoder_feature_dim=self.image_features_dim, \n",
        "                                                  num_att_elems=self.num_encoder_vectors,\n",
        "                                                  name='EncoderAttention')\n",
        "        \n",
        "        self.decoder_embedding = DecoderEmbedding(embedding_dim=self.embedding_dim, \n",
        "                                                  max_len=self.max_len,\n",
        "                                                  vocab_size=self.vocab_size,\n",
        "                                                  name='DecoderEmbedding')   \n",
        "        self.embedding_layer = self.decoder_embedding.get_layer('Embedding')\n",
        "        \n",
        "        self.decoder_attention = DecoderAttention(num_blocks=self.num_decoder_blocks,\n",
        "                                              encoder_units=self.image_features_dim,\n",
        "                                              decoder_units=self.decoder_units, \n",
        "                                              num_encoder_vectors=self.num_encoder_vectors,\n",
        "                                              max_len=self.max_len,\n",
        "                                              use_convolutions=self.use_convolutions,\n",
        "                                              name='DecoderAttention') \n",
        "        \n",
        "        # collect params\n",
        "        self.rnn_input_dim = self.encoder_attention.output_shape[-1]\n",
        "        beam_input_dim = self.vocab_size\n",
        "        \n",
        "        # decoders\n",
        "        self.start1 = tf.cond(tf.math.equal(self.use_dual_decoder, True),\n",
        "                              lambda: 50,\n",
        "                              lambda: self.max_len)\n",
        "        \"\"\"\n",
        "        self.start1 = 50  # step to switch to next decoder\n",
        "        if tf.math.equal(self.use_dual_decoder, False):  # override prev value\n",
        "            self.start1 = self.max_len  # never switches\n",
        "        \"\"\"\n",
        "\n",
        "        self.decoder_head_0 = DecoderHead(decoder_units=self.decoder_units, \n",
        "                                          vocab_size=self.vocab_size, \n",
        "                                          max_len=self.start1, \n",
        "                                          name='DecoderHead_0')\n",
        "\n",
        "        if tf.math.equal(self.use_dual_decoder, True):  # override prev value\n",
        "            self.decoder_head_1 = DecoderHead(decoder_units=self.decoder_units, \n",
        "                                              vocab_size=self.vocab_size, \n",
        "                                              max_len= self.max_len - self.start1, \n",
        "                                              name='DecoderHead_1')\n",
        "        else:\n",
        "            self.decoder_head_1 = self.decoder_head_0\n",
        "\n",
        "    def encoding_step(self, image, tokenized_InChI):\n",
        "        \n",
        "        # text encoding\n",
        "        start_var_batch = tf.tile(self.start_var, [self.batch_size, 1, 1])\n",
        "\n",
        "        # tokenize and encode InChI\n",
        "        inchi, targets = self.inchi_prep(tokenized_InChI)  # tokenize & separate out targets\n",
        "        inchi_vectors = self.decoder_embedding([inchi, start_var_batch])  \n",
        "        \n",
        "        # image encoding\n",
        "        image = self.image_encoder(image)\n",
        "        encoder_vectors = self.encoder_attention(image)\n",
        "\n",
        "        return encoder_vectors, inchi_vectors, targets\n",
        "    \n",
        "    def call_no_tf_func(self, inputs, training):\n",
        "        # note: dataset provided as (image, image_id, tokenized_InChI, InChI)\n",
        "\n",
        "        # generation step options\n",
        "        use_beam = tf.constant(False, tf.bool)\n",
        "        use_preds = not training\n",
        "\n",
        "        # inputs\n",
        "        image = inputs['image']\n",
        "        tokenized_InChI = inputs['tokenized_InChI']\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_vectors, inchi_vectors, targets = self.encoding_step(image, tokenized_InChI)\n",
        "\n",
        "        # Decoder\n",
        "        predicted_tokens, predicted_probs = self.generation_loop(use_preds, encoder_vectors, \n",
        "                                                                inchi_vectors, targets)\n",
        "        \n",
        "        return targets, predicted_tokens, predicted_probs\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs, training=tf.constant(False, tf.bool)):\n",
        "        return self.call_no_tf_func(inputs, training)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        \n",
        "        # get loss and grads\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            targets, predictions, probs = self.call_no_tf_func(data, training=True)       \n",
        "            loss = self.compiled_loss(targets, probs)\n",
        "\n",
        "            # add any regularization losses\n",
        "            loss += tf.math.reduce_sum(self.losses)\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables) \n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update metrics\n",
        "        self.compiled_metrics.update_state(targets, probs)\n",
        "        \n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    # validation step\n",
        "    def test_step(self, data):\n",
        "        \n",
        "        # Compute predictions\n",
        "        targets, predictions, probs = self(data, training=False)       \n",
        "        \n",
        "        # record loss\n",
        "        self.compiled_loss(targets, probs)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(targets, probs)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "    # Full Generation Loop\n",
        "    def generation_loop(self, use_preds, encoder_vectors, inchi_vectors, target):\n",
        "        # NOTE: all tensor arrays should be created within the @tf.function\n",
        "        \n",
        "        # containers\n",
        "        # note: use fixed size arrays for XLA and/or parallel ops\n",
        "        tokens_array = tf.TensorArray(size=self.max_len, dtype=tf.int32, \n",
        "                            dynamic_size=False, element_shape=[self.batch_size],\n",
        "                            tensor_array_name='tokens_array')\n",
        "        \n",
        "        probs_array = tf.TensorArray(size=self.max_len, dtype=tf.float32, \n",
        "                            dynamic_size=False, element_shape=[self.batch_size, self.vocab_size],\n",
        "                            tensor_array_name='probs_array')\n",
        "        \n",
        "        # create initial (embedded) predictions\n",
        "        embedded_preds_array = tf.TensorArray(size=self.max_len, dtype=tf.float32, \n",
        "                            clear_after_read = False, dynamic_size=False, \n",
        "                            element_shape=[self.batch_size, self.embedding_dim],\n",
        "                            tensor_array_name='embedded_preds_array')\n",
        "        \n",
        "        for step in range(self.max_len):\n",
        "            embedded_preds_array = embedded_preds_array.write(step, tf.cast(inchi_vectors[:, step, :], tf.float32))\n",
        "        \n",
        "        # create masks array\n",
        "        def create_mask_array(size):\n",
        "            masks_array = tf.TensorArray(size=size, dtype=tf.int32, clear_after_read=False, \n",
        "                                         dynamic_size=False, \n",
        "                                         element_shape=[self.batch_size, size, size],\n",
        "                                         tensor_array_name='masks_array')\n",
        "            for step in range(size):\n",
        "                mask = tf.ones((step + 1, step + 1), dtype=tf.int32)\n",
        "                mask = tf.pad(mask, [[0, size - step - 1], [0, size - step - 1]])\n",
        "                mask = tf.expand_dims(mask, 0)\n",
        "                mask = tf.tile(mask, [self.batch_size, 1, 1])\n",
        "                masks_array = masks_array.write(step, tf.cast(mask, tf.int32))\n",
        "            return masks_array\n",
        "\n",
        "        masks_array = create_mask_array(self.max_len)\n",
        "        \n",
        "        # \"while loop\" character generation\n",
        "        def loop_fn(step, continue_cond, inchi_vectors, tokens_array, probs_array, embedded_preds_array):\n",
        "\n",
        "            # caution: make sure input is correctly masked at this step!\n",
        "            # create mask\n",
        "            mask = masks_array.read(index=step)\n",
        "\n",
        "            # attention update\n",
        "            decoder_attention = self.decoder_attention([encoder_vectors, inchi_vectors, mask])\n",
        "            \n",
        "            # get char probs (using correct decoder for position in sequence)\n",
        "            probs = tf.cond(tf.math.less(step, self.start1), \n",
        "                            lambda: self.decoder_head_0([decoder_attention]),\n",
        "                            lambda: self.decoder_head_1([decoder_attention]))\n",
        "\n",
        "            # select current step's probabilities and predictions\n",
        "            probs = probs[:, step, :]\n",
        "            predictions = tf.argmax(probs, axis=-1)\n",
        "            predictions = tf.cast(predictions, tf.int32)\n",
        "\n",
        "            # save results\n",
        "            tokens_array = tokens_array.write(step, tf.cast(predictions, tf.int32))\n",
        "            probs_array = probs_array.write(step, tf.cast(probs, tf.float32))  # match array dtype\n",
        "            \n",
        "            \n",
        "            def true_fn(embedded_preds_array, predictions):\n",
        "                preds = tf.squeeze(predictions)\n",
        "                embedded_preds = self.embedding_layer(preds)\n",
        "                embedded_preds_array = embedded_preds_array.write(step, tf.cast(embedded_preds, tf.float32))  # match array dtype\n",
        "                return embedded_preds_array\n",
        "\n",
        "            embedded_preds_array = tf.cond(tf.math.equal(use_preds, True), \n",
        "                                           lambda: true_fn(embedded_preds_array, predictions),\n",
        "                                           lambda: embedded_preds_array)\n",
        "\n",
        "            # increment counter\n",
        "            step = tf.math.add(step, 1)\n",
        "\n",
        "            # check early stopping criteria\n",
        "            def true_fn(predictions):\n",
        "                predictions = tf.expand_dims(predictions, axis=1)\n",
        "                continue_cond = tf.math.reduce_any(predictions != tf.cast(self.tokenized_EOS, tf.int32))\n",
        "                return continue_cond\n",
        "\n",
        "            continue_cond = tf.cond(tf.math.equal(use_preds, True), \n",
        "                                    lambda: true_fn(predictions),\n",
        "                                    lambda: continue_cond)\n",
        "\n",
        "            # continue condition (check if max step number reached)\n",
        "            continue_cond = tf.cond(tf.math.equal(continue_cond, True), \n",
        "                                    lambda: tf.math.less(step, self.max_len), \n",
        "                                    lambda: continue_cond)\n",
        "            \n",
        "            # prepare next input if continuing\n",
        "            def true_fn(embedded_preds_array, orig_dtype):\n",
        "                inchi_vectors = embedded_preds_array.stack()\n",
        "                inchi_vectors = tf.transpose(inchi_vectors, perm=[1, 0, 2])  \n",
        "                \n",
        "                # cast back to original dtype\n",
        "                inchi_vectors = tf.cast(inchi_vectors, dtype=orig_dtype)  \n",
        "                return inchi_vectors\n",
        "\n",
        "            orig_dtype = inchi_vectors.dtype  # careful casting needed for mixed precision\n",
        "            inchi_vectors = tf.cond(tf.math.logical_and(\n",
        "                tf.math.equal(continue_cond, True), tf.math.equal(use_preds, True)),\n",
        "                                    lambda: true_fn(embedded_preds_array, orig_dtype),\n",
        "                                    lambda: tf.cast(inchi_vectors, dtype=orig_dtype))\n",
        "\n",
        "            return [step, continue_cond, inchi_vectors, tokens_array, probs_array, embedded_preds_array]\n",
        "\n",
        "        # stopping condition function\n",
        "        def cond_fn(step, continue_cond, inchi_vectors, tokens_array, probs_array, embedded_preds_array):\n",
        "            return tf.equal(continue_cond, True)\n",
        "\n",
        "        # generation loop\n",
        "        # parallelized via the 'parallel_iterations' parameters\n",
        "        step = tf.constant(0, dtype=tf.int32)\n",
        "        continue_cond = tf.constant(True, dtype=tf.bool)\n",
        "\n",
        "        step, continue_cond, inchi_vectors, tokens_array, probs_array, embedded_preds_array \\\n",
        "            = tf.while_loop(\n",
        "                    maximum_iterations=self.max_len,  \n",
        "                    cond=cond_fn, \n",
        "                    body=loop_fn, \n",
        "                    loop_vars=[step, continue_cond, inchi_vectors, tokens_array, probs_array, embedded_preds_array],               \n",
        "                    shape_invariants=[tf.TensorShape([]), # step\n",
        "                                      tf.TensorShape([]), # continue_cond\n",
        "                                      tf.TensorShape([self.batch_size, self.max_len, self.embedding_dim]), # inchi_vectors\n",
        "                                      None,  # tokens_array\n",
        "                                      None,  # probs_array\n",
        "                                      None],  # embedded_preds_array\n",
        "                    parallel_iterations=64,\n",
        "                    swap_memory=True)\n",
        "        \n",
        "        # unpack token arrays\n",
        "        predicted_tokens = tokens_array.stack()  # predicted characters\n",
        "        predicted_tokens = tf.squeeze(predicted_tokens)\n",
        "        predicted_tokens = tf.transpose(predicted_tokens, perm=[1, 0])   \n",
        "\n",
        "        # unpack probs_array (no beam update)\n",
        "        predicted_probs = probs_array.stack()  # predicted logits\n",
        "        predicted_probs = tf.squeeze(predicted_probs)\n",
        "        predicted_probs = tf.transpose(predicted_probs, perm=[1, 0, 2])  \n",
        "\n",
        "        return predicted_tokens, predicted_probs    \n",
        "    \n",
        "    def tokens_to_string(self, tokens):\n",
        "        #parsed_string_vals = self.inverse_tokenizer(tf.constant(tokens))\n",
        "        parsed_string_vals = INVERSE_TOKENIZER(tf.constant(tokens))\n",
        "        \n",
        "        string_vals = keras.layers.Lambda(lambda x: tf.strings.reduce_join(x, axis=-1))(parsed_string_vals)\n",
        "\n",
        "        # remove first EOS generated and everything after\n",
        "        pattern = ''.join([self.EOS, '.*$'])\n",
        "        string_vals = tf.strings.regex_replace(string_vals, pattern, rewrite='', \n",
        "                                               replace_global=True, name='remove_EOS')   \n",
        "\n",
        "        return string_vals, parsed_string_vals\n",
        "    \n",
        "    \"\"\"     \n",
        "    # Our Tensorflow metric calculates Levenshtein scores of the tokens.\n",
        "    # To calculate the true character-level score use this:\n",
        "    \n",
        "    !pip install levenshtein\n",
        "    from leven import levenshtein\n",
        "\n",
        "    def compute_levenshtein_scores(self, inchi_true, inchi_predicted):\n",
        "        scores = [levenshtein(pred, orig) for (pred, orig)\n",
        "                  in zip(inchi_predicted.numpy().tolist(), inchi_true.numpy().tolist())]\n",
        "        return tf.reduce_mean(scores)\n",
        "    \"\"\""
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5TuwjWo0RRB"
      },
      "source": [
        "class InchiGenerator(BaseTrainer):\n",
        "    \"\"\"\n",
        "    Beam updates turned on, training conducted using generated preds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, base_model, beam_rnn_units, name='BeamInchiTrainer', **kwargs):\n",
        "        \n",
        "        super().__init__(batch_size=batch_size,\n",
        "        image_features_dim=base_model.image_features_dim,\n",
        "                         use_dense_encoder_top=base_model.use_dense_encoder_top,\n",
        "                         decoder_units=base_model.decoder_units,\n",
        "                         num_decoder_blocks=base_model.num_decoder_blocks,\n",
        "                         num_encoder_blocks=base_model.num_encoder_blocks,\n",
        "                         use_convolutions=base_model.use_convolutions,\n",
        "                         use_dual_decoder= base_model.use_dual_decoder,\n",
        "                         max_len = base_model.max_len,\n",
        "                         parameters=base_model.parameters, \n",
        "                         enable_beam=True,\n",
        "                         name=name, **kwargs)\n",
        "        \n",
        "        self.beam_rnn_units = beam_rnn_units\n",
        "        self.beam = BeamUpdate(self.decoder_units, self.max_len, self.beam_rnn_units, \n",
        "                        beam_input_dim, self.vocab_size, name='beam')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        # generation step options\n",
        "        use_beam = True\n",
        "        use_preds = True\n",
        "\n",
        "        # inputs\n",
        "        # note: dataset provided as (image, image_id, tokenized_InChI, InChI)\n",
        "        image = inputs['image']      \n",
        "        tokenized_InChI = inputs['tokenized_InChI']\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_attention, inchi, targets = self.encoding_step(image, tokenized_InChI)\n",
        "\n",
        "        # Decoder\n",
        "        predictions, predicted_probs = self.generation_loop(use_preds, use_beam, \n",
        "                                                            encoder_attention,\n",
        "                                                            inchi, targets)\n",
        "        \n",
        "        # beam update\n",
        "        # pad and mask\n",
        "        mask_value = -1.0\n",
        "        beam_inputs = tf.pad(predicted_probs, constant_values=mask_value,\n",
        "                             paddings=([[0, 0], [0, 200], [0, 0]]))\n",
        "        \n",
        "        beam_inputs = tf.keras.layers.Masking(mask_value=mask_value)(beam_inputs)\n",
        "\n",
        "        # create initial RNN state\n",
        "        initial_state = tf.math.reduce_mean(encoder_attention, axis=1)\n",
        "\n",
        "        # get probs and predictions\n",
        "        predicted_probs = self.beam([beam_inputs, initial_state])\n",
        "        updated_predicted_tokens = tf.argmax(predicted_probs, axis=-1)\n",
        "        predictions = tf.cast(updated_predicted_tokens, tf.int32)\n",
        "        \n",
        "        return targets, predictions, predicted_probs"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtWg2jRlI1_W"
      },
      "source": [
        "class EditDistanceMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='edit_distance', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.edit_distance = self.add_weight(name='edit_distance', initializer='zeros')\n",
        "        self.batch_counter = self.add_weight(name='batch_counter', initializer='zeros')\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.sparse.from_dense(y_true)\n",
        "        y_pred = tf.sparse.from_dense(tf.argmax(y_pred, axis=-1))  # convert probs to preds\n",
        "\n",
        "        y_true = tf.cast(y_true, tf.int32)\n",
        "        y_pred = tf.cast(y_pred, tf.int32)\n",
        "\n",
        "        # compute edit distance (of parsed tokens)\n",
        "        edit_distance = tf.edit_distance(y_pred, y_true, normalize=False)\n",
        "        self.edit_distance.assign_add(tf.reduce_mean(edit_distance))\n",
        "\n",
        "        # update counter\n",
        "        self.batch_counter.assign_add(tf.reduce_sum(1.))\n",
        "    \n",
        "    def result(self):\n",
        "        return self.edit_distance / self.batch_counter\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.edit_distance.assign(0.0)\n",
        "        self.batch_counter.assign(0.0)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt3klX0SYnVw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "f4ecb591-9ae0-4563-a6d8-209e79449d1b"
      },
      "source": [
        "# Learning rate schedule used in \"Attention is All You Need\"\n",
        "class LRScheduleAIAYN(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, scale_factor=1, warmup_steps=4000):  # defaults reflect paper's values\n",
        "        self.warmup_steps = tf.constant(warmup_steps, dtype=tf.float32)\n",
        "        dim = tf.constant(352, dtype=tf.float32)\n",
        "        self.scale = scale_factor * tf.math.pow(dim, -1.5)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        step = tf.cast(step, tf.float32)\n",
        "        opt_1 = step * tf.math.pow(self.warmup_steps, -1.5)  # linear increase\n",
        "        opt_2 = tf.math.pow(step, -.5) # decay\n",
        "        return self.scale * tf.math.reduce_min([opt_1, opt_2])\n",
        "\n",
        "# visualize learning rate \n",
        "temp_lr = LRScheduleAIAYN()\n",
        "plt.plot([i for i in range(1, 8000)], [temp_lr(i) for i in range(1, 8000)])\n",
        "print('Learning Rate Schedule')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning Rate Schedule\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUtswyx2ru4O"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZAEoiSYsZb"
      },
      "source": [
        "Model compile options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDez379bi8Xy"
      },
      "source": [
        "NAME_MODIFIER = ''\n",
        "\n",
        "# build model\n",
        "IMAGE_FEATURES_DIM = 256  # note: only used if USE_DENSE_ENCODER_TOP = True.\n",
        "                     # use multiple of 8 or 128. Darien Schettler public Kaggle notebook recommends 208\n",
        "DECODER_UNITS = 256  # use multiple of 8 or 128.  Darien Schettler public Kaggle notebook recommends 208\n",
        "BEAM_RNN_UNITS = 128  # note: only used in beam_model. use multiple of 8 or 128. \n",
        "NUM_ENCODER_BLOCKS = 2  # \"All You Need is Attention\" uses 6 blocks\n",
        "NUM_DECODER_BLOCKS = 2  # \"All You Need is Attention\" uses 6 blocks\n",
        "USE_DENSE_ENCODER_TOP = True  # for fine tuning the image transfer model \n",
        "USE_DUAL_DECODERS = False\n",
        "USE_CONVOLUTIONS = False\n",
        "if USE_CONVOLUTIONS:\n",
        "    checkpoint_save_name = 'ConvAtt_model_weights' + NAME_MODIFIER\n",
        "else:\n",
        "    checkpoint_save_name = 'AISAYN_model_weights' + NAME_MODIFIER\n",
        "\n",
        "LOAD_CHECKPOINT_FILE = os.path.join(PARAMETERS.load_checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n",
        "SAVE_CHECKPOINT_FILE = os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n",
        "\n",
        "# note: in Kaggle,\n",
        "# LOAD_CHECKPOINT_FILE points to saved outputs from prev session\n",
        "# SAVE_CHECKPOINT_FILE points to saved outputs from current session"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7s5jU6OYwax"
      },
      "source": [
        "Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0OTg89vf1ht",
        "outputId": "aec55502-581a-469f-cb00-c67b87f83575"
      },
      "source": [
        "# initialize models\n",
        "for val in train_ds.take(1):\n",
        "    batch_size=val['image'].shape[0]\n",
        "    if not PARAMETERS.tpu():\n",
        "        steps_per_execution = 1\n",
        "    else:\n",
        "        steps_per_execution = 32\n",
        "\n",
        "    with PARAMETERS.strategy().scope():  # (\"distribution strategy\" defined at top of notebook)\n",
        "        model_base = BaseTrainer(batch_size=batch_size,\n",
        "                                 image_features_dim=IMAGE_FEATURES_DIM,\n",
        "                                 decoder_units=DECODER_UNITS,\n",
        "                                 num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
        "                                 num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
        "                                 use_dense_encoder_top=USE_DENSE_ENCODER_TOP,\n",
        "                                 use_convolutions=USE_CONVOLUTIONS,\n",
        "                                 use_dual_decoder=USE_DUAL_DECODERS,\n",
        "                                 max_len=176,  # 133 is recommended value from Darien Schettler Kaggle notebook\n",
        "                                 parameters=PARAMETERS, \n",
        "                                 name='BaseTrainer')\n",
        "\n",
        "        # compiler components\n",
        "        learning_rate = LRScheduleAIAYN(scale_factor=1, warmup_steps=4000)  # from \"Attention is All You Need\"\n",
        "        \n",
        "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,  # params from \"Attention is All You Need\"\n",
        "                                             beta_1=0.9, beta_2=0.98, epsilon=10e-9)\n",
        "        checkpoint = tf.keras.callbacks.ModelCheckpoint(SAVE_CHECKPOINT_FILE, monitor='loss', \n",
        "                    save_weights_only=True, save_best_only=True, save_freq='epoch')\n",
        "        \n",
        "        # optimizations\n",
        "        if not PARAMETERS.tpu():\n",
        "            os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # better balances CPU / GPU interaction in tf.data\n",
        "            tf.config.optimizer.set_jit(\"autoclustering\")  # XLA compiler optimization\n",
        "            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)  # required with mixed precision on GPU / CPU\n",
        "\n",
        "        # custom metrics\n",
        "        edit_dist_metric = EditDistanceMetric()\n",
        "\n",
        "        # compile\n",
        "        model_base.compile(optimizer=optimizer, \n",
        "                           loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                           metrics=['sparse_categorical_accuracy', edit_dist_metric],\n",
        "                           steps_per_execution=steps_per_execution)\n",
        "        \n",
        "\n",
        "        \n",
        "        \"\"\" \n",
        "        # Beam-update model\n",
        "        model_beam = InchiGenerator(model_base, beam_rnn_units=BEAM_RNN_UNITS, \n",
        "                                    name='InchiGenerator_inference')  \n",
        "        \"\"\"\n",
        "\n",
        "    print('Models initialized.')\n",
        "\n",
        "\n",
        "# build / verify function call work\n",
        "# make sure this is NOT within distribution strategy on TPU\n",
        "if not PARAMETERS.tpu():\n",
        "    for val in train_ds.take(1):\n",
        "        model_base(val, training=False)\n",
        "        model_base(val, training=True)\n",
        "        model_base.predict(val)\n",
        "\n",
        "        \"\"\"\n",
        "        model_beam(val, training=False)\n",
        "        model_beam(val, training=True)\n",
        "        model_beam.predict(val)\n",
        "        \"\"\"\n",
        "\n",
        "        print('\\n\\n')    \n",
        "        model_base.summary()\n",
        "\n",
        "    # sync weights\n",
        "    # WARNING!: in Kaggle this loads from prev session saved weights\n",
        "    try:\n",
        "        with PARAMETERS.strategy().scope(): \n",
        "            model_base.load_weights(LOAD_CHECKPOINT_FILE)  \n",
        "    except:\n",
        "        print('No weights loaded')  \n",
        "\n",
        "else:\n",
        "    model_base(val, training=True)\n",
        "    print('\\n\\n')    \n",
        "    model_base.summary()\n",
        "    \"\"\"\n",
        "    # sync weights\n",
        "    # WARNING!: in Kaggle this loads from prev session saved weights\n",
        "    try:\n",
        "        with PARAMETERS.strategy().scope(): \n",
        "            model_base.load_weights(LOAD_CHECKPOINT_FILE)  \n",
        "    except:\n",
        "        print('No weights loaded')\n",
        "    \"\"\""
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Models initialized.\n",
            "\n",
            "\n",
            "\n",
            "Model: \"BaseTrainer\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "ImageEncoder (Functional)    (None, 64, 256)           3644320   \n",
            "_________________________________________________________________\n",
            "InchiPrep (Functional)       [(None, 175), (None, 176) 0         \n",
            "_________________________________________________________________\n",
            "EncoderAttention (Functional (None, 64, 256)           662016    \n",
            "_________________________________________________________________\n",
            "DecoderEmbedding (Functional (None, 176, 256)          50944     \n",
            "_________________________________________________________________\n",
            "Embedding (Embedding)        (None, 175, 256)          50944     \n",
            "_________________________________________________________________\n",
            "DecoderAttention (Functional (None, 176, 256)          1190400   \n",
            "_________________________________________________________________\n",
            "DecoderHead_0 (Functional)   (None, 176, 199)          51143     \n",
            "=================================================================\n",
            "Total params: 5,599,079\n",
            "Trainable params: 2,359,495\n",
            "Non-trainable params: 3,239,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanU6T8tf1ht"
      },
      "source": [
        "if not PARAMETERS.tpu():\n",
        "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/')\n",
        "    %tensorboard --logdir './logs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsmcfhvEvdUP"
      },
      "source": [
        "Test inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EMrmPRzvTEr"
      },
      "source": [
        "# Load inference model (adjusted batch size)\n",
        "if not PARAMETERS.tpu():\n",
        "    with PARAMETERS.strategy().scope(): \n",
        "        model_inference = BaseTrainer(batch_size=PARAMETERS.inference_batch_size(),\n",
        "                                  image_features_dim=IMAGE_FEATURES_DIM,\n",
        "                                  decoder_units=DECODER_UNITS,\n",
        "                                  num_encoder_blocks=NUM_ENCODER_BLOCKS,\n",
        "                                  num_decoder_blocks=NUM_DECODER_BLOCKS,\n",
        "                                  use_dense_encoder_top=USE_DENSE_ENCODER_TOP,\n",
        "                                  use_convolutions=USE_CONVOLUTIONS,\n",
        "                                  use_dual_decoder=USE_DUAL_DECODERS,\n",
        "                                  max_len=176, \n",
        "                                  parameters=PARAMETERS, \n",
        "                                  name='BaseInference')\n",
        "    \n",
        "    # sync weights\n",
        "    # WARNING!: in Kaggle this loads from prev session saved weights\n",
        "    try:\n",
        "        model_inference.load_weights(LOAD_CHECKPOINT_FILE)\n",
        "        pass\n",
        "    except:\n",
        "        print('No weights loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6BmY63-QSwJ"
      },
      "source": [
        "# test inference speed\n",
        "\n",
        "num_batches = 3  # processing time for 3 * 256 = 768 images           \n",
        "%timeit for val2 in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(1): im_id, preds = (model_inference.predict(val2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBKRQowIWqq"
      },
      "source": [
        "\"\"\"\n",
        "# Full model: inference speed (with beam)\n",
        "%%timeit\n",
        "num_batches = 3\n",
        "\n",
        "for val in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(num_batches): \n",
        "    im_id, preds = (model_base.predict(val))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otxdN02mf1ht"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8MF9xt4uqG"
      },
      "source": [
        "Train base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdeuljeJf1h1"
      },
      "source": [
        "# Train base model (teacher-fed training, prediction-fed validation, no beam update)\n",
        "\n",
        "# Note: if training decoders segments, initialize second decoder weights using:\n",
        "# \"model_base.decoder_1.set_weights(model_base.decoder_0.get_weights())\"\n",
        "\n",
        "if not PARAMETERS.tpu():\n",
        "    steps_per_epoch = 40\n",
        "    epochs = int(1.6 * 1e6) // (steps_per_epoch * PARAMETERS.batch_size())  # one full pass through train set\n",
        "else:\n",
        "    steps_per_epoch = None\n",
        "    epochs = 4\n",
        "\n",
        "\n",
        "# train\n",
        "model_base.fit(train_ds, epochs=epochs, steps_per_epoch=steps_per_epoch, \n",
        "               validation_data=valid_ds, validation_steps=10, validation_freq=6,\n",
        "               callbacks=[checkpoint, tensorboard], \n",
        "               verbose=2, \n",
        "               use_multiprocessing=True\n",
        "               )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s1Lgn1e4wxz"
      },
      "source": [
        "Train beam update model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otz8JJdB3QZ6"
      },
      "source": [
        "\"\"\"\n",
        "# train beam model (prediction-fed training and inference, includes beam update mech)\n",
        "\n",
        "steps_per_epoch = 150\n",
        "epochs = len(train_labels_df) // steps_per_epoch  # one full pass through the dataset\n",
        "\n",
        "# sync weights\n",
        "model_beam.load_weights(PARAMETERS.load_checkpoint_dir() + 'checkpoints')\n",
        "print('Loaded saved weights')\n",
        "\n",
        "# choose variables to train\n",
        "model_base.decoder_0.trainable = True\n",
        "model_base.decoder_1.trainable = True  # if multiple decoders enabled\n",
        "\n",
        "# compile\n",
        "model_beam.compile(optimizer=optimizer, \n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['sparse_categorical_accuracy', EditDistanceMetric()])\n",
        "\n",
        "# train\n",
        "model_beam.fit(train_ds, epochs=30, steps_per_epoch=20, \n",
        "               validation_data=valid_ds, validation_steps=3, validation_freq=5,\n",
        "               callbacks=[checkpoint],#, tensorboard], \n",
        "               verbose=2, use_multiprocessing=True)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbvzr5rdmjgs"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Here we define function to conduct inference on the test set. Results are saved to \"submission.csv\".\n",
        "\n",
        "Intermediate results are saved at regular intervals to. This allows inference to be conducted in stages and is a safeguard in case of interruptions before the full set has been processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVm4YazBsFIB"
      },
      "source": [
        "def make_inference_progress(predictions_df, skip_recorded=True, save_freq=50, parameters=PARAMETERS):\n",
        "\n",
        "    # initialize model and build with inference batch size (test_ds)\n",
        "    model_base = BaseTrainer(embedding_dim=EMBEDDING_DIM, rnn_units=RNN_DIM, \n",
        "                             beam_rnn_units=BEAM_RNN_UNITS, \n",
        "                             parameters=PARAMETERS, name='BaseTrainer')\n",
        "    for val in test_ds.take(1):\n",
        "        model_base(val)\n",
        "    model_full = InchiGenerator(model_base, name='InchiGenerator')\n",
        "    for val in test_ds.take(1):\n",
        "        model_full(val)\n",
        "\n",
        "    # load saved weights\n",
        "    model_full.load_weights(PARAMETERS.load_checkpoint_dir() + 'checkpoints')\n",
        "    print('Loaded model')\n",
        "\n",
        "    batch_size = 1024\n",
        "    if skip_recorded:\n",
        "        existing_batches = len(predictions_df) // batch_size\n",
        "    else:\n",
        "        existing_batches = 0\n",
        "    i = 0\n",
        "\n",
        "    for val in test_ds.skip(existing_batches):\n",
        "        test_im_id, test_pred = model_full.predict(val, return_lev_score=False)\n",
        "\n",
        "        # decode bytestrings\n",
        "        test_im_id = [x.decode()[:-4] for x in test_im_id.numpy().tolist()]  # drops '.png'\n",
        "        test_pred = [x.decode() for x in test_pred.numpy().tolist()]\n",
        "\n",
        "        new_preds = pd.DataFrame({'image_id': test_im_id, \n",
        "                                  'InChI': test_pred})\n",
        "\n",
        "        \n",
        "        predictions_df = predictions_df.append(new_preds)\n",
        "\n",
        "        # save to CSV\n",
        "        if i % save_freq == 0:\n",
        "            predictions_df = predictions_df.drop_duplicates(subset='image_id', keep='last')\n",
        "            predictions_df.to_csv(PARAMETERS.csv_save_dir() + 'submission.csv', index=False)\n",
        "            print(f'iteration {i}')\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN9S2TyMy__W"
      },
      "source": [
        "Load previosuly generated predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iSlSFmHy9mX"
      },
      "source": [
        "try:\n",
        "    predictions_df = pd.read_csv(PARAMETERS.csv_save_dir() + 'submission.csv')\n",
        "except:\n",
        "    predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlq0OYsqzHMA"
      },
      "source": [
        "Generate additional predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CplWdHecmiRQ"
      },
      "source": [
        "\"\"\" On first pass or to start from scratch, initialize the dataframe with:\n",
        "predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)\n",
        "\"\"\"\n",
        "\n",
        "predictions_df = make_inference_progress(predictions_df, save_freq=100, num_batches=1, starting_batch=0, parameters=PARAMETERS)\n",
        "predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}