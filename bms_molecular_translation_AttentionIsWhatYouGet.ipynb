{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bms_molecular_translation-AttentionIsWhatYouGet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Attention-Is-What-You-Get/blob/main/bms_molecular_translation_AttentionIsWhatYouGet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zujis6hSUc0o"
      },
      "source": [
        "# Attention is What You Get\n",
        "\n",
        "This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition.\n",
        "\n",
        "-----\n",
        "\n",
        "AUTHOR: \n",
        "\n",
        "Mo Venouziou\n",
        "\n",
        "- *Email: mvenouziou@gmail.com*\n",
        "- *LinkedIn: www.linkedin.com/in/movenouziou/*\n",
        "\n",
        "*June 2, 2021*\n",
        "\n",
        "----\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvew4SZuM33i"
      },
      "source": [
        "### Our Goal: Predict the \"InChI\" value of any given chemical compound diagram. \n",
        "\n",
        "International Chemical Identifiers (\"InChI values\") are a standardized encoding to describe chemical compounds. They take the form of a string of letters, numbers and deliminators, often between 100 - 400 characters long. \n",
        "\n",
        "The chemical diagrams are provided as PNG files, often of such low quality that it may take a human several seconds to decipher. \n",
        "\n",
        "Label length and image quality become a serious challenge here, because we must predict labels for a very large quantity of images. There are 1.6 million images in the test set abd 2.4 million images available in the training set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Obx8tqy_ICG0"
      },
      "source": [
        "\"\"\"\n",
        "# Example (image, target label) pair\\n\\n'\n",
        "for val in train_ds.unbatch().take(1):\n",
        "    print('Example Label:\\n', val['InChI'].numpy())\n",
        "    print('\\nCorresponding Image:', plt.imshow(val['image'][:,:,0], cmap='binary'))\n",
        "### note: load datasets before running this cell\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTzwOF_khY65"
      },
      "source": [
        "## MODEL STRUCTURE: \n",
        "\n",
        "**Image CNN + Attention Features encoder --> text Attention + CNN feature layer decoder.**\n",
        "\n",
        "This is a hybrid approach with:\n",
        " \n",
        " - Image Encoder from [*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*](https://proceedings.mlr.press/v37/xuc15.pdf).  Generate image feature vectors using intermediate layer outputs from a pretrained CNN. (Here I use the more modern EfficientNet model with fixed weights and add a trainable Dense layer for customization.)\n",
        " \n",
        " - T2T encoder-decoder model from [*All You Need is Attention*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Self-attention feature extraction for both encoder and decoder, joint encoder-decoder attention feature interactions, and an (optional) dense prediction output block. \n",
        "\n",
        " - ***PLUS*** *(optional):* Decoder Output Blocks placed in Series (not stacked). Increase the number of trainable paramaters without adding inference computational complexity, while also allowing decoders to specialize on different regions of the output. (Note: Training is a bit trickier. My experiments show it is best to first train with the decoders using shared weights, then allowing them to vary later on in training.)\n",
        " \n",
        " - ***PLUS*** *(optional):* Is attention really all you need? Add a convolutional layer to enhance text features before decoder self-attention to experiment with performance differences with and without extra convolutional layer(s). Use of CNN's in NLP comes from [*Convolutional Sequence to Sequence Learning*](http://proceedings.mlr.press/v70/gehring17a.html.)\n",
        "\n",
        " - ***PLUS*** *(optional):* Beam-Search Alternative, an extra decoding layer applied after the full logits prediction has been made. This takes the form of a bidirectional RNN applied to the full logits sequence. Because a full (initial) prediction has already been made, computations can be paralelized using stateful RNNs. (See more details below.)\n",
        "\n",
        "*Optional features can be enabled/disabled using parameters in my model definitions.*\n",
        "\n",
        "----\n",
        "\n",
        "## NEXT STEPS:\n",
        "\n",
        " - Implement **TPU training**. (Currently runs on GPU. Note that a CPU along is not enough to achieve acceptable inference speed.)\n",
        "\n",
        " - experiment with **\"Tokens-to-Token ViT\"** in place of the image CNN. (Technique from [*Training Vision Transformers from Scratch on ImageNet*](https://arxiv.org/pdf/2101.11986.pdf)\n",
        "  \n",
        " - Train my **Beam-search Alternative**. \n",
        "\n",
        "    - Beam search is a technique to modify model predictions to reflect the (local) maximum likelihood estimate. However, it is *very* local in that computation expense increases quickly with the number of character steps taken into account. This is also a hard-coded algorithm, which is somewhat contrary to the philosophy of deep learning.\n",
        "\n",
        "    - A *Beam-search Alternative* would be an extra decoding layer applied *after* the full logits prediction has been made. This might be in the form of a stateful, bidirectional RNN that is computationally parallizable because it is applied to the full logits sequence.\n",
        "\n",
        "    - This is coded and ready to train, although I have not yet had the time to do so.\n",
        "\n",
        " - Treat the number of convolutional layers (decoder feature extraction) and number of decoders places in series (decoder prediction output) as **new hyperparamaters** to tune.\n",
        "\n",
        "-------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htI5m-W2uJmx"
      },
      "source": [
        "### CITATIONS\n",
        "\n",
        "- \"Attention is All You Need.\" \n",
        " - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS (2017). *https://research.google/pubs/pub46201/*\n",
        "\n",
        "- \"Convolutional Sequence to Sequence Learning.\"\n",
        " \n",
        "  - Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y.N.. (2017). Convolutional Sequence to Sequence Learning. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:1243-1252 Available from *http://proceedings.mlr.press/v70/gehring17a.html.*\n",
        "\n",
        "\n",
        "-  \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\"\n",
        "  -  Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. & Bengio, Y.. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2048-2057 Available from *http://proceedings.mlr.press/v37/xuc15.html.* \n",
        "            \n",
        "\n",
        "- \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\"\n",
        "\n",
        "  - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Preprint (2021). Available at *https://arxiv.org/abs/2101.11986*.\n",
        "\n",
        "- Special thanks to [Darien Schettler](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook.) for leading readers to the \"Show\" and \"Attention\" papers cited above, and sharing his progress at various stages in the competition through public Kaggle notebooks. My work includes a few of his hyperparameter choices and selection of EfficientNet transfer model. I did not read or use his implementations of the papers above.\n",
        "\n",
        "- It is possible my idea of a Beam Search Alternative is based on a lecture video from DeepLearning.ai's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  on Coursera.\n",
        "\n",
        "- **Dataset / Kaggle Competition:** \"Bristol-Myers Squibb â€“ Molecular Translation\" competition on Kaggle (2021). *https://www.kaggle.com/c/bms-molecular-translation*\n",
        "\n",
        "----\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csshw-ehbeQY"
      },
      "source": [
        "## Contents\n",
        "\n",
        "1. [Imports](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=TjuUOVXao__C&line=4&uniqifier=1)\n",
        "2. [Data Pipeline](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=lrLHKs5Ni7Sz)\n",
        "3. [Model Layers](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=W0T-u0vZamI8)\n",
        "    - [InChI Encoding](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=DYApmA2lf1hp&line=1&uniqifier=1)\n",
        "    - [Image Encoding and Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=FESofcGdEaWF&line=1&uniqifier=1)\n",
        "    - [Decoder Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=6qFDs9RTjvod&line=1&uniqifier=1)\n",
        "    - [Joint Encoder-Decoder Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=jP-t1MkKnD5L)\n",
        "    - [Decoder Head (Prediction Output)](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=38GA7wtNEhqW&line=1&uniqifier=1)\n",
        "    - [Update Mechanism](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=_2UR1DLljD0S&line=1&uniqifier=1)\n",
        "4. [Full Model](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=D6GIs3f3rpu0&line=1&uniqifier=1)\n",
        "5. [Training](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=otxdN02mf1ht&line=1&uniqifier=1)\n",
        "6. [Inference](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=Sbvzr5rdmjgs&line=5&uniqifier=1)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjuUOVXao__C",
        "outputId": "866c0167-cd81-466f-9ba4-31ebabb93648"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# TF Model design\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.data import TFRecordDataset\n",
        "from tensorflow.data.experimental import TFRecordWriter\n",
        "#!pip install -q tensorflow_addons\n",
        "#import tensorflow_addons as tfa\n",
        "\n",
        "# Text processing\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Kaggle (for TPU)\n",
        "#from kaggle_datasets import KaggleDatasets\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from PIL import Image\n",
        "!pip install -U tensorboard_plugin_profile\n",
        "%load_ext tensorboard\n",
        "\n",
        "# data management\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools\n",
        "\n",
        "# file management\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboard_plugin_profile in /usr/local/lib/python3.7/dist-packages (2.4.0)\n",
            "Requirement already satisfied, skipping upgrade: gviz-api>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.9.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (56.1.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard_plugin_profile) (1.15.0)\n",
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUS3jc1o1Nq8"
      },
      "source": [
        "## Model parameters\n",
        "\n",
        "The 'ModelParameters' class manages global hyperparamaters for portability between Colab and Kaggle notebook environments. Once set, all other cells will run on either platform.\n",
        "\n",
        "On Colab, connection to my personal Google Drive is required, as ModelParameters will extract the dataset from a zip file to the hosted environment. This process may take several minutes. (It would not be difficult for the reader to update the code to point to their own drive and download the zip dataset using the Kaggle API code below.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "q6FUA_lvrd4Y",
        "outputId": "f8e5c4d7-8381-47aa-bfe8-044ca1a5e37c"
      },
      "source": [
        "\"\"\" Kaggle api for download the compressed dataset from Kaggle's servers.\n",
        "\n",
        "# imports\n",
        "!pip uninstall -y kaggle\n",
        "!pip install --upgrade pip\n",
        "!pip install kaggle==1.5.6\n",
        "\n",
        "# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\n",
        "# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" Kaggle api for download the compressed dataset from Kaggle's servers.\\n\\n# imports\\n!pip uninstall -y kaggle\\n!pip install --upgrade pip\\n!pip install kaggle==1.5.6\\n\\n# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\\n# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\\nos.environ['KAGGLE_CONFIG_DIR'] = '/content/gdrive/MyDrive/Kaggle'  # api token location\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTkL38FRtUfQ"
      },
      "source": [
        "class ModelParameters:\n",
        "    def __init__(self, cloud_server='kaggle'):\n",
        "               \n",
        "        # universal parameters\n",
        "        self._batch_size = 8  # adjust based on system RAM\n",
        "        self._inference_batch_size = 256\n",
        "        self._image_size = (224, 224)  # shape to process images in data pipeline\n",
        "        self.SOS_string = 'InChI=1S/'  # start of sentence value\n",
        "        self.EOS_string = '<EOS>'  # end of sentence value\n",
        "        self._strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. May be overwritten below\n",
        "\n",
        "        # File Paths\n",
        "        \n",
        "        if cloud_server == 'colab':  # Google Colab with GDrive (CPU / GPU)\n",
        "            \n",
        "            from google.colab import drive\n",
        "            drive.mount('/content/gdrive/') \n",
        "\n",
        "            # unzip data\n",
        "            if not os.path.isdir('/content/bms-molecular-translation'):\n",
        "                !unzip -q /content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/bms-molecular-translation.zip -d '/content/bms-molecular-translation'\n",
        "            \n",
        "            self._dataset_dir = 'bms-molecular-translation/'\n",
        "            self._labels_dir = self._dataset_dir\n",
        "            self._prepared_files_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/'\n",
        "            self._checkpoint_dir = '/content/gdrive/MyDrive/Colab_Notebooks/models/MolecularTranslation/checkpoints/'\n",
        "            self._load_checkpoint_dir = self._checkpoint_dir\n",
        "            self._csv_save_dir = self._prepared_files_dir\n",
        "\n",
        "        elif cloud_server == 'colab_TPU':\n",
        "            \"\"\" NOTE: not yet implemented \"\"\"\n",
        "            resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
        "            tf.config.experimental_connect_to_cluster(resolver)\n",
        "            tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "            self._strategy = tf.distribute.TPUStrategy(resolver)    \n",
        "\n",
        "            # update batch size\n",
        "            self._batch_size = 16 * self._strategy .num_replicas_in_sync\n",
        "            self._inference_batch_size = 16 * self._strategy .num_replicas_in_sync            \n",
        "\n",
        "            \"\"\"\n",
        "            # add file system structure here\n",
        "            \"\"\"\n",
        "                \n",
        "        elif cloud_server == 'kaggle': # Kaggle cloud notebook (CPU / GPU)\n",
        "            \n",
        "            self._dataset_dir = '../input/bms-molecular-translation/'\n",
        "            self._labels_dir = self._dataset_dir\n",
        "            self._prepared_files_dir = '../input/periodic-table/'\n",
        "            self._checkpoint_dir = './'\n",
        "            self._load_checkpoint_dir = '../input/k/mvenou/bms-molecular-translation/checkpoints/'\n",
        "            self._csv_save_dir = './'\n",
        "        \n",
        "        elif cloud_server == 'kaggle_TPU': # Enables Kaggle TPU \n",
        "            # Detect hardware, return appropriate distribution strategy\n",
        "            \n",
        "            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
        "            print('Running on TPU ', tpu.master())\n",
        "\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            self._strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "\n",
        "            print(\"REPLICAS: \", self._strategy.num_replicas_in_sync)\n",
        "            \n",
        "            # update batch size\n",
        "            self._batch_size = 16 * self._strategy .num_replicas_in_sync\n",
        "            self._inference_batch_size = 16 * self._strategy .num_replicas_in_sync\n",
        "\n",
        "            # Data Path\n",
        "            self._dataset_dir = KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "            self._labels_dir = KaggleDatasets().get_gcs_path('bms-molecular-translation')\n",
        "            self._prepared_files_dir = KaggleDatasets().get_gcs_path('periodic-table')\n",
        "            self._checkpoint_dir = './'\n",
        "            self._load_checkpoint_dir = './'\n",
        "            self._csv_save_dir = './'\n",
        "            \n",
        "        # common file paths\n",
        "        self._periodic_table_csv = os.path.join(self._prepared_files_dir, 'periodic_table_elements.csv')\n",
        "        self._vocab_csv = os.path.join(self._prepared_files_dir, 'vocab.csv')\n",
        "        self._processed_labels_valid_csv = os.path.join(self._prepared_files_dir, 'processed_labels_valid.csv')\n",
        "        self._processed_labels_train_csv = os.path.join(self._prepared_files_dir, 'processed_labels_train.csv')\n",
        "            \n",
        "        self._test_images_dir = os.path.join(self._dataset_dir, 'test/')\n",
        "        self._train_images_dir = os.path.join(self._dataset_dir, 'train/')\n",
        "        self._extra_labels_csv = os.path.join(self._labels_dir, 'extra_approved_InChIs.csv')\n",
        "        self._train_labels_csv = os.path.join(self._labels_dir, 'train_labels.csv')\n",
        "        self._sample_submission_csv = os.path.join(self._labels_dir, 'sample_submission.csv')\n",
        "        \n",
        "    # functions to access params\n",
        "    def cloud_server(self):\n",
        "        return self._cloud_server\n",
        "    def strategy(self):\n",
        "        return self._strategy\n",
        "    def csv_save_dir(self):\n",
        "        return self._csv_save_dir\n",
        "    def processed_labels_train_csv(self):\n",
        "        return self._processed_labels_train_csv\n",
        "    def processed_labels_valid_csv(self):\n",
        "        return self._processed_labels_valid_csv\n",
        "    def train_labels_csv(self):\n",
        "        return self._train_labels_csv\n",
        "    def vocab_csv(self):\n",
        "        return self._vocab_csv\n",
        "    def periodic_table_csv(self):\n",
        "        return self._periodic_table_csv\n",
        "    def batch_size(self):\n",
        "        return self._batch_size  \n",
        "    def inference_batch_size(self):\n",
        "        return self._inference_batch_size\n",
        "    def image_size(self):\n",
        "        return self._image_size    \n",
        "    def SOS(self):\n",
        "        return self.SOS_string\n",
        "    def EOS(self):\n",
        "        return self.EOS_string\n",
        "    def dataset_dir(self):\n",
        "        return self._dataset_dir\n",
        "    def labels_dir(self):\n",
        "        return self._labels_dir\n",
        "    def temp_dataset_dir(self):\n",
        "        return self._temp_dataset_dir\n",
        "    def images_dir(self):\n",
        "        return self._images_dir\n",
        "    def train_images_dir(self):\n",
        "        return self._train_images_dir\n",
        "    def test_images_dir(self):\n",
        "        return self._test_images_dir   \n",
        "    def checkpoint_dir(self):\n",
        "        return self._checkpoint_dir\n",
        "    def load_checkpoint_dir(self):\n",
        "        return self._load_checkpoint_dir\n",
        "    def image_size(self):\n",
        "        return self._image_size   \n",
        "    def batch_size(self):\n",
        "        return self._batch_size\n",
        "    def elements_csv_path(self):\n",
        "        return self._periodic_table_csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sFuCtyTUQwh"
      },
      "source": [
        "Initialize Parameter Options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_a301N_Gf1hl",
        "outputId": "b0cf90ab-8511-49b9-fa04-77b30cb45e99"
      },
      "source": [
        "PARAMETERS = ModelParameters(cloud_server='colab')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrLHKs5Ni7Sz"
      },
      "source": [
        "# **Input Pipeline**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgq0W_hZUX-r"
      },
      "source": [
        "Load train labels as DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21sUOSyff1hm"
      },
      "source": [
        "# Load CSV as dataframe\n",
        "train_labels_df = pd.read_csv(PARAMETERS.train_labels_csv())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "ry6qU0GbXanM",
        "outputId": "99d63e11-7646-4256-bc12-6b27d34b6419"
      },
      "source": [
        "train_labels_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>InChI</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000011a64c74</td>\n",
              "      <td>InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000019cc0cd2</td>\n",
              "      <td>InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000252b6d2b</td>\n",
              "      <td>InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000026b49b7e</td>\n",
              "      <td>InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000026fc6c36</td>\n",
              "      <td>InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       image_id                                              InChI\n",
              "0  000011a64c74  InChI=1S/C13H20OS/c1-9(2)8-15-13-6-5-10(3)7-12...\n",
              "1  000019cc0cd2  InChI=1S/C21H30O4/c1-12(22)25-14-6-8-20(2)13(1...\n",
              "2  0000252b6d2b  InChI=1S/C24H23N5O4/c1-14-13-15(7-8-17(14)28-1...\n",
              "3  000026b49b7e  InChI=1S/C17H24N2O4S/c1-12(20)18-13(14-7-6-10-...\n",
              "4  000026fc6c36  InChI=1S/C10H19N3O2S/c1-15-10(14)12-8-4-6-13(7..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1PolKXhNOy"
      },
      "source": [
        "### InChI Text Parsing\n",
        "\n",
        "We split each InChI label into its \"vocabulary\" of logical subunits, consisting of element abbreviations numbers, common symbols and the required string 'InChI=1S/', which is at the start of every InChI label. We want to narrow down this vocabulary to the smallest set represented in our training data. The functions below provide a system for finding this minimal set, as well as preparing a new CSV file with parsed labels ready to be fed into a tokenizer layer.\n",
        "\n",
        "(For clarity and to reduce reliance on loading external files, the true code has been commented out and replaced with corresponding hard-coded values.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQiYkMHIrXmo"
      },
      "source": [
        "def inchi_parsing_regex(parameters=PARAMETERS):\n",
        "    # regex for spliting on InChi, but preserving chemical element abbreviations and three-digit numbers\n",
        "    \n",
        "    # shortcut: hard coded values\n",
        "    vocab = [parameters.EOS(), parameters.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'Br', 'B', 'Cl', 'C', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'Si', 'S', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "    vocab += [str(num) for num in reversed(range(168))]\n",
        "    vocab = [re.escape(val) for val in vocab]\n",
        "       \n",
        "    \"\"\" # to create vocab from scratch, use:\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # load list of elements we should search for within InChI strings: \n",
        "    periodic_elements = pd.read_csv(PARAMETERS.periodic_table_csv(), header=None)[1].to_list()\n",
        "    periodic_elements = periodic_elements + [val.lower() for val in periodic_elements] + [val.upper() for val in periodic_elements]\n",
        "    \n",
        "    punctuation = list(string.punctuation)\n",
        "    punctuation = [re.escape(val) for val in punctuation]   # update values with regex escape chars added as needed\n",
        "\n",
        "    three_dig_nums_list = [str(i) for i in range(1000, -1, -1)]\n",
        "\n",
        "    vocab = [SOS, EOS] + periodic_elements + three_dig_nums_list + punctuation\n",
        "    \"\"\"\n",
        "\n",
        "    split_elements_regex = rf\"({'|'.join(vocab)})\"\n",
        "    \n",
        "    return split_elements_regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_57mCRgimRj"
      },
      "source": [
        "INCHI_PARSING_REGEX = inchi_parsing_regex()\n",
        "\n",
        "def parse_InChI(texts, parsing_regex=INCHI_PARSING_REGEX):  \n",
        "    return ' '.join(re.findall(parsing_regex, texts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "No2ecfS_hR9k"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Here we create efficient tf.data.Dataset train / validation / test sets.\n",
        "\n",
        "Out data pipeline will read our prepared CSV of (image filename, parsed InChI and standard InChI) tuples. (If this file is not found, it will be created from scratch. This may take several minutes)  Iterating through the list, it will load batches of corresponding images and labels.\n",
        "\n",
        "Our datasets contain the following information, accessible by dict keys: images, image_id, InChI, parsed_InChI. (The test set uses InChI = parsed_InChI = 'InChI=1S/', the known required stating value for any InChI code.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDRJElyOf1hn"
      },
      "source": [
        "def data_generator(image_set, labels_dataframe=None, parameters=PARAMETERS):\n",
        "    \n",
        "    # image parameters\n",
        "    load_image_size = (350, 350)  # use min of (224, 224), the EfficientNet model input size\n",
        "    \n",
        "    # get universal params\n",
        "    batch_size = parameters.batch_size()\n",
        "    inference_batch_size = parameters.inference_batch_size()\n",
        "    target_size = parameters.image_size()\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # dataset options\n",
        "    prefetch_size = tf.data.AUTOTUNE \n",
        "    inference_prefetch_size = tf.data.AUTOTUNE    \n",
        "    options = tf.data.Options()\n",
        "    options.experimental_optimization.autotune_buffers = True\n",
        "    options.experimental_optimization.map_vectorization.enabled = True\n",
        "    options.experimental_optimization.apply_default_optimizations = True\n",
        "            \n",
        "    def preprocess_image(image):\n",
        "        # layers\n",
        "        scale = keras.layers.experimental.preprocessing.Rescaling(1./255)\n",
        "        resize = keras.layers.experimental.preprocessing.Resizing(\n",
        "            height=target_size[0], width=target_size[1], interpolation='bicubic')\n",
        "        \n",
        "        # preprocessing steps\n",
        "        image = tf.cast(image, tf.float32)\n",
        "        image = resize(image)\n",
        "        \n",
        "        return image\n",
        "        \n",
        "    # Train & Validation Datasets\n",
        "    if image_set in ['train', 'valid']:\n",
        "        DIRECTORY = os.path.join(parameters.dataset_dir(), 'train/')\n",
        "        \n",
        "        # try loading processed CSV\n",
        "        try:\n",
        "            if image_set == 'train':\n",
        "                assert os.path.isfile(parameters.processed_labels_train_csv()) or parameters.cloud_server()=='kaggle_TPU'\n",
        "                processed_labels_csv = parameters.processed_labels_train_csv()\n",
        "                \n",
        "            elif image_set == 'valid':\n",
        "                assert os.path.isfile(parameters.processed_labels_valid_csv()) or parameters.cloud_server()=='kaggle_TPU'\n",
        "                processed_labels_csv = parameters.processed_labels_valid_csv()\n",
        "                \n",
        "     # otherwise process data from scratch \n",
        "        except:\n",
        "            valid_split = .05\n",
        "            num_valid_samples = int(valid_split * len(labels_dataframe))\n",
        "\n",
        "            if image_set == 'train':\n",
        "                dataframe = labels_dataframe.iloc[num_valid_samples: ]  # get train split\n",
        "            elif image_set == 'valid':\n",
        "                dataframe = labels_dataframe.iloc[: num_valid_samples]  # get validation split\n",
        "\n",
        "            # shuffle\n",
        "            dataframe = dataframe.sample(frac=1)\n",
        "\n",
        "            # add image path info\n",
        "            dataframe['image_path'] =  dataframe['image_id'].apply(lambda x: os.path.join(x[0], x[1], x[2], x + '.png'))  # update image path info\n",
        "\n",
        "            # prepare InChI labels for model       \n",
        "            dataframe['parsed_InChI'] =  dataframe['InChI'].apply(lambda x: parse_InChI(x))\n",
        "\n",
        "            # save as CSV\n",
        "            dataframe = dataframe[['image_path', 'image_id', 'InChI', 'parsed_InChI']]  # set column order\n",
        "            processed_labels_csv = ''.join([parameters.csv_save_dir(), 'processed_labels_', image_set, '.csv'])\n",
        "            dataframe.to_csv(processed_labels_csv, index=False)\n",
        "\n",
        "        # reload as dataset\n",
        "        dataset = tf.data.experimental.make_csv_dataset(\n",
        "                    file_pattern=processed_labels_csv, \n",
        "                    batch_size=1, \n",
        "                    column_names=['image_path', 'image_id', 'InChI', 'parsed_InChI'], \n",
        "                    column_defaults=[tf.string, tf.string, tf.string, tf.string],\n",
        "                    label_name=None, select_columns=None, field_delim=',',\n",
        "                    use_quote_delim=True, na_value='', header=True, num_epochs=None,\n",
        "                    shuffle=True, shuffle_buffer_size=250, shuffle_seed=None,\n",
        "                    prefetch_buffer_size=None, num_parallel_reads=5, sloppy=True,\n",
        "                    num_rows_for_inference=100, compression_type=None, ignore_errors=False)\n",
        "        \n",
        "        # load images into dataset\n",
        "        def load_image(image_path, target_size=target_size):\n",
        "            \n",
        "            # layer for initial image loading (to match test data pipeline)\n",
        "            resize = keras.layers.experimental.preprocessing.Resizing(\n",
        "                height=load_image_size[0], width=load_image_size[1], interpolation='bicubic')\n",
        "            \n",
        "            # update image path\n",
        "            image_path = tf.strings.join([DIRECTORY, image_path])\n",
        "            \n",
        "            # load file\n",
        "            image = keras.layers.Lambda(lambda x: tf.io.read_file(x))(image_path)\n",
        "            image = keras.layers.Lambda(lambda x: tf.io.decode_jpeg(x, channels=1))(image)\n",
        "            image = tf.cast(image, tf.float32)\n",
        "            image = resize(image)\n",
        "            \n",
        "            # preprocessing / standardization\n",
        "            image = preprocess_image(image)\n",
        "\n",
        "            return image      \n",
        "\n",
        "        # final dataset with column keys\n",
        "        dataset = dataset.map(lambda x: {'image': load_image(x['image_path'][0]), \n",
        "                                         'image_id': x['image_id'][0], \n",
        "                                         'parsed_InChI': tf.strings.join([x['parsed_InChI'][0], EOS, EOS, EOS, EOS, EOS], separator=' '),\n",
        "                                         'InChI': x['InChI'][0]}, \n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "        dataset = dataset.with_options(options)\n",
        "        dataset = dataset.batch(batch_size)\n",
        "        dataset = dataset.prefetch(prefetch_size)\n",
        "    \n",
        "    # Test Dataset\n",
        "    elif image_set == 'test':\n",
        "        dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "            directory=parameters.test_images_dir(), labels='inferred', label_mode=None,\n",
        "            class_names=None, color_mode='grayscale', batch_size=1, \n",
        "            image_size=load_image_size, shuffle=False, seed=None, validation_split=None, subset=None,\n",
        "            interpolation='bicubic', follow_links=False)\n",
        "        \n",
        "        # set filenames as label\n",
        "        image_id_ds = tf.data.Dataset.from_tensor_slices(dataset.file_paths)\n",
        "        image_id_ds = image_id_ds.map(lambda x: tf.strings.split(x, os.path.sep)[-1],\n",
        "                                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # image preprocessing / standardization\n",
        "        dataset = dataset.map(preprocess_image)\n",
        "        \n",
        "        # set InChI label as start value 'InChI=1S/'\n",
        "        inchi_ds = image_id_ds.map(lambda x: tf.constant(SOS, dtype=tf.string),\n",
        "                                   num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        # merge datasets\n",
        "        dataset = tf.data.Dataset.zip((dataset, image_id_ds, inchi_ds))\n",
        "        \n",
        "        # set key names\n",
        "        dataset = dataset.map(lambda x, y, z: {'image': tf.squeeze(x, axis=0), \n",
        "                                               'image_id': y, \n",
        "                                               'parsed_InChI': z,\n",
        "                                               'InChI': z},\n",
        "                              num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        \n",
        "        dataset = dataset.with_options(options)\n",
        "        dataset = dataset.batch(inference_batch_size)\n",
        "        dataset = dataset.prefetch(inference_prefetch_size)\n",
        "        \n",
        "        \n",
        "    else: # generate error for invalid set name\n",
        "        assert 0==1    \n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCaO3A_5f1ho"
      },
      "source": [
        "Create Test, Train and Validation Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36x7lSfVf1ho"
      },
      "source": [
        "train_ds = data_generator(image_set='train', labels_dataframe=train_labels_df, parameters=PARAMETERS)\n",
        "valid_ds = data_generator(image_set='valid', labels_dataframe=train_labels_df, parameters=PARAMETERS)\n",
        "#test_ds = data_generator(image_set='test', parameters=PARAMETERS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juAkO-oQaUml"
      },
      "source": [
        "Examine data shapes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKbpsKKrf1hp"
      },
      "source": [
        "for val in train_ds.take(1):\n",
        "    print('Train DS')\n",
        "    print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, 'InChI:', val['InChI'].shape, 'parsed_InChI:', val['parsed_InChI'].shape)\n",
        "\n",
        "for val in valid_ds.take(1):\n",
        "    print('\\nValidation DS')\n",
        "    print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, 'InChI:', val['InChI'].shape, 'parsed_InChI:', val['parsed_InChI'].shape)\n",
        "\n",
        "\"\"\"\n",
        "for val in test_ds.take(1):\n",
        "    print('\\nTest DS')\n",
        "    print(val[0].shape, val[1].shape, val[2].shape)\n",
        "    # print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, 'parsed_InChI:', val['parsed_InChI'].shape)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeCYSm7N74i7"
      },
      "source": [
        "### TF Records Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQgYgaPoO4gp"
      },
      "source": [
        "# helper functions to create TFRecords file for use on TPU\n",
        "def make_example(image, image_id, parsed_InChI, InChI):\n",
        "    image_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[            \n",
        "            tf.io.serialize_tensor(image).numpy()])\n",
        "    )\n",
        "    image_id_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[\n",
        "            tf.io.serialize_tensor(image_id).numpy()])\n",
        "    )\n",
        "    parsed_InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[\n",
        "            tf.io.serialize_tensor(parsed_InChI).numpy()])\n",
        "    )\n",
        "    InChI_feature = tf.train.Feature(\n",
        "        bytes_list=tf.train.BytesList(value=[\n",
        "            tf.io.serialize_tensor(InChI).numpy()])\n",
        "    )\n",
        "\n",
        "    features = tf.train.Features(feature={\n",
        "        'image': image_feature,\n",
        "        'image_id': image_id_feature,\n",
        "        'parsed_InChI': parsed_InChI_feature,\n",
        "        'InChI': InChI_feature\n",
        "    })\n",
        "    \n",
        "    example = tf.train.Example(features=features)\n",
        "\n",
        "    return example.SerializeToString()\n",
        "\n",
        "def make_example_py_fn(image, image_id, InChI, parsed_InChI):\n",
        "    return tf.py_function(func=make_example, \n",
        "                   inp=[image, image_id, InChI, parsed_InChI], \n",
        "                   Tout=tf.string)\n",
        "\n",
        "def decode_example(example):    \n",
        "    feature_description = {'image': tf.io.FixedLenFeature([], tf.string),\n",
        "                            'image_id': tf.io.FixedLenFeature([], tf.string),\n",
        "                            'parsed_InChI': tf.io.FixedLenFeature([], tf.string),\n",
        "                            'InChI': tf.io.FixedLenFeature([], tf.string)}\n",
        "    \n",
        "    values = tf.io.parse_single_example(example, feature_description)\n",
        "    values['image'] = tf.io.parse_tensor(values['image'], out_type=tf.float32)\n",
        "    \n",
        "    return values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hURRE1cnGOTD"
      },
      "source": [
        "def create_records(dataset, folder):\n",
        "    \n",
        "    num_shards = 200\n",
        "    \n",
        "    # map to Feature Examples\n",
        "    dataset = dataset.unbatch().batch(128)\n",
        "    dataset = dataset.map(lambda x: make_example_py_fn(x['image'], x['image_id'], x['InChI'], x['parsed_InChI']))\n",
        "    \n",
        "    for shard_num in range(num_shards):\n",
        "        path = os.path.join('./', folder, str(shard_num))\n",
        "        if not os.path.isdir(os.path.join('./', folder)):\n",
        "            os.mkdir(os.path.join('./', folder))\n",
        "        writer = TFRecordWriter(path)\n",
        "        \n",
        "        this_shard = dataset.shard(num_shards, index=shard_num)\n",
        "        writer.write(this_shard)\n",
        "    \n",
        "    return None\n",
        "        \n",
        "def read_records(filename):\n",
        "\n",
        "    dataset = TFRecordDataset(filename)\n",
        "    return dataset.map(decode_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luknoE5wTCS7"
      },
      "source": [
        "\"\"\"\n",
        "# This takes an incredible amount of time to run. \n",
        "# I think the code works, so I'm letting it run on Kaggle\n",
        "# and will update when I have results\n",
        "\n",
        "create_records(valid_ds, folder='valid_tfrec')\n",
        "create_records(train_ds, folder='train_tfrec')\n",
        "create_records(test_ds, folder='test_tfrec')\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0T-u0vZamI8"
      },
      "source": [
        "# **Model Layers**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYApmA2lf1hp"
      },
      "source": [
        "## InChI Encoding\n",
        "\n",
        "Tokenizer and Embedding to convert parsed InChI strings to tensors of numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDY6kY8Mf1hq"
      },
      "source": [
        "def Tokenizer(parameters, padded_length):\n",
        "    \"\"\" note: crops /pads to max len\n",
        "    \"\"\"\n",
        "\n",
        "    SOS = parameters.SOS()\n",
        "    EOS = parameters.EOS()\n",
        "    \n",
        "    # Create vocabulary for tokenizer\n",
        "    def create_vocab():       \n",
        "        hard_coded_vocab = [PARAMETERS.EOS(), PARAMETERS.SOS(), '(',\n",
        "            ')', '+', ',', '-', '/', 'B', 'Br',  'C', 'Cl', 'D', 'F',\n",
        "            'H', 'I', 'N', 'O', 'P', 'S', 'Si', 'T', 'b', 'c', 'h', 'i',\n",
        "            'm', 's', 't']\n",
        "        \n",
        "        numbers = [str(num) for num in range(168)]\n",
        "        \n",
        "        vocab = hard_coded_vocab + numbers\n",
        "        \n",
        "        \"\"\"\n",
        "        # get from saved file\n",
        "        vocab = pd.read_csv(PARAMETERS.vocab_csv())['vocab_value'].to_list()   \n",
        "        vocab = list(vocab)\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\" \n",
        "        # To create from scratch, extract all vocab elements appearing in train set:\n",
        "        df = pd.read_csv(PARAMETERS.train_labels_csv())  \n",
        "        seg_len = 250000\n",
        "        num_breaks = len(df) // seg_len\n",
        "\n",
        "        vocab = set()\n",
        "        for i in range(num_breaks):\n",
        "\n",
        "            df_i =  df['InChI'].iloc[seg_len * i: seg_len * (i+1)]\n",
        "            texts =  df_i.apply(lambda x: set(parse_InChI(x).split()))\n",
        "            texts = texts.tolist()\n",
        "\n",
        "            vocab = vocab.union(*texts)\n",
        "\n",
        "            print(f'completed {i} / {num_breaks}')\n",
        "\n",
        "        vocab = list(vocab)\n",
        "        vocab_df = pd.DataFrame({'vocab_value': vocab})\n",
        "\n",
        "        # save results\n",
        "        filename = os.path.join(PARAMETERS.csv_save_dir(), 'vocab.csv')\n",
        "        vocab_df.to_csv(filename, index=False)\n",
        "        \"\"\"\n",
        "               \n",
        "        return vocab\n",
        "\n",
        "    vocab = create_vocab()\n",
        "    \n",
        "    # create tokenizer\n",
        "    tokenizer_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        standardize=None, split=lambda x: tf.strings.split(x, sep=' ', maxsplit=-1), \n",
        "        output_mode='int', output_sequence_length=padded_length, vocabulary=vocab)\n",
        "\n",
        "    # record EOS token\n",
        "    tokenized_EOS = tokenizer_layer(tf.constant([EOS]))\n",
        "    \n",
        "    # create inverse (de-tokenizer)\n",
        "    inverse_tokenizer = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=tokenizer_layer.get_vocabulary(), invert=True)\n",
        "\n",
        "    return tokenizer_layer, inverse_tokenizer, tokenized_EOS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw6ghpYe6l7W"
      },
      "source": [
        "temp_tokenizer_layer, temp_inverse_tokenizer, temp_tokenized_EOS = \\\n",
        "    Tokenizer(parameters=PARAMETERS, padded_length=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMv38YQzf1hq"
      },
      "source": [
        "InChI Input Prep Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwhCP06Wf1hq"
      },
      "source": [
        "def InchiPrep(tokenizer_layer, vocab_size, embedding_dim, name='InchiPrep'):\n",
        "        \n",
        "    # inputs\n",
        "    parsed_inchi = keras.layers.Input([], dtype=tf.string, name='parsed_inchi')\n",
        "    start_var = keras.layers.Input([1, embedding_dim], dtype=tf.float32)\n",
        "    inputs = [parsed_inchi, start_var]\n",
        "    \n",
        "    # tokenize\n",
        "    tokenized_inchi = tokenizer_layer(parsed_inchi)\n",
        "\n",
        "    # split into input / target pairs\n",
        "    inchi_target = tokenized_inchi\n",
        "    \n",
        "    inchi_input = keras.layers.Lambda(lambda x: x[:, :-1])(tokenized_inchi)\n",
        "    inchi_input = keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
        "                    mask_zero=True, name='Embedding')(inchi_input)\n",
        "    inchi_input = keras.layers.Reshape([-1, embedding_dim])(inchi_input)\n",
        "    inchi_input = keras.layers.Concatenate(-2)([start_var, inchi_input])\n",
        "    \n",
        "    # outpus\n",
        "    outputs = [inchi_input, inchi_target]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xs6gv3Zbf1hq",
        "outputId": "b09fa6f9-5221-44f1-ee83-bfc75a25f4d0"
      },
      "source": [
        "temp_tokenizer_layer, _, _ = Tokenizer(PARAMETERS, padded_length=200)\n",
        "temp_prep = InchiPrep(temp_tokenizer_layer, vocab_size=130, embedding_dim=100)\n",
        "temp_prep.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"InchiPrep\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "parsed_inchi (InputLayer)       [(None,)]            0                                            \n",
            "__________________________________________________________________________________________________\n",
            "text_vectorization_9 (TextVecto (None, 200)          0           parsed_inchi[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "lambda_8 (Lambda)               (None, 199)          0           text_vectorization_9[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "Embedding (Embedding)           (None, 199, 100)     13000       lambda_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "input_16 (InputLayer)           [(None, 1, 100)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "reshape_12 (Reshape)            (None, 199, 100)     0           Embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_6 (Concatenate)     (None, 200, 100)     0           input_16[0][0]                   \n",
            "                                                                 reshape_12[0][0]                 \n",
            "==================================================================================================\n",
            "Total params: 13,000\n",
            "Trainable params: 13,000\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FESofcGdEaWF"
      },
      "source": [
        "# Image Encoder\n",
        "\n",
        "Feature Extraction Step 1: Run the images through a pre-trained image network, extracting features as the output of an intermediate convolutional layer. [Technique from \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,\" cited at the top of this notebook.]  A dense layer is added for transfer learning and to control the dimension of the attention mechanism used later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMyUVvA9jBXp"
      },
      "source": [
        "Transfer Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdTKFUFKfHeU"
      },
      "source": [
        "TRANSFER_MODEL = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False, pooling='max', weights='imagenet')\n",
        "\n",
        "FEATURES_LAYER = 'block7a_project_bn'  # layer to extract features from"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3OKI_t3f1hr"
      },
      "source": [
        "def ImageEncoder(image_shape, output_dim, use_dense_top, name='ImageEncoder'):\n",
        "\n",
        "    # transfer model\n",
        "    transfer_model = TRANSFER_MODEL\n",
        "    features_layers = FEATURES_LAYER\n",
        "    transfer_model.trainable=False  # optional. My system doesn't have enough RAM to train this portion of the model, at a reasonable batch size\n",
        "\n",
        "    # sub-models\n",
        "    features_model = keras.Model(inputs=transfer_model.inputs,\n",
        "                                 outputs=transfer_model.get_layer(features_layers).output,\n",
        "                                 name='transfer_features') \n",
        "    \n",
        "    # Inputs\n",
        "    image = keras.layers.Input(image_shape, dtype=tf.float32, name='image')\n",
        "    inputs = [image]\n",
        "    \n",
        "    # Model Path\n",
        "    image_features = image\n",
        "    image_features = tf.image.grayscale_to_rgb(image_features)  # enable if needed\n",
        "    image_features = tf.keras.applications.efficientnet.preprocess_input(image_features)  # preprocessing\n",
        "    image_features = features_model(image_features)\n",
        "    \n",
        "    features_dim = image_features.shape[-1]\n",
        "    image_features = keras.layers.Reshape([-1, features_dim])(image_features)\n",
        "    if use_dense_top:\n",
        "        image_features = keras.layers.Dense(output_dim, activation='relu',\n",
        "                                            name='dense')(image_features)\n",
        "\n",
        "    outputs = [image_features]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SyEeXgr5f1hr",
        "outputId": "e02ec6c9-6a27-43c3-a509-f4a8711d3860"
      },
      "source": [
        "ImageEncoder(image_shape=(224, 224,1), output_dim=208, use_dense_top=True).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"ImageEncoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "image (InputLayer)           [(None, 224, 224, 1)]     0         \n",
            "_________________________________________________________________\n",
            "tf.image.grayscale_to_rgb_18 (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "transfer_features (Functiona (None, None, None, 320)   3634851   \n",
            "_________________________________________________________________\n",
            "reshape_33 (Reshape)         (None, 49, 320)           0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 49, 208)           66768     \n",
            "=================================================================\n",
            "Total params: 3,701,619\n",
            "Trainable params: 66,768\n",
            "Non-trainable params: 3,634,851\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGYH_nP3Efsk"
      },
      "source": [
        "## Encoder Attention\n",
        "\n",
        "Feature Extraction Step 2: Now that we have basic feature vectors, we use self-attention to generate more complex features. This is the encoding step used in \"Attention is All You Need,\" cited above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmq0PjxxjayJ"
      },
      "source": [
        "def EncoderAttention(decoder_units, num_att_elems, name='EncoderAttention'):\n",
        "\n",
        "    # inputs\n",
        "    encoder_vectors = keras.layers.Input([num_att_elems, decoder_units], name='encoder_vectors')   # from image encoder\n",
        "    inputs = [encoder_vectors]\n",
        "\n",
        "    # attention (uses \"Attention is All You Need\" structure)\n",
        "\n",
        "    # encoder self-attentiondef EncoderAttention(decoder_units, num_att_elems, name='EncoderAttention'):\n",
        "\n",
        "    # inputs\n",
        "    encoder_vectors = keras.layers.Input([num_att_elems, decoder_units], name='encoder_vectors')   # from image encoder\n",
        "    inputs = [encoder_vectors]\n",
        "\n",
        "    # attention (uses \"Attention is All You Need\" structure)\n",
        "\n",
        "    # encoder self-attention\n",
        "    attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=8, key_dim=decoder_units//8, name='encoder_attention')(  # uses 'num_heads * key_dim = rnn_units' from paper\n",
        "                query=encoder_vectors, value=encoder_vectors)\n",
        "    \n",
        "    attention = keras.layers.Dropout(rate=.1)(attention)\n",
        "    attention = keras.layers.Add()([encoder_vectors, attention])\n",
        "    attention = keras.layers.BatchNormalization()(attention)    \n",
        "\n",
        "    # updated encoder vectors\n",
        "    encoder_vectors = keras.layers.Dense(decoder_units, 'relu')(attention)    \n",
        "    \n",
        "    encoder_vectors = keras.layers.Dropout(rate=.1)(encoder_vectors)\n",
        "    encoder_vectors = keras.layers.Add()([attention, encoder_vectors])\n",
        "    encoder_vectors = keras.layers.BatchNormalization()(encoder_vectors)     \n",
        "\n",
        "    # output\n",
        "    outputs = [encoder_vectors]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)\n",
        "    attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=8, key_dim=decoder_units//8, name='encoder_attention')(  # uses 'num_heads * key_dim = rnn_units' from paper\n",
        "                query=encoder_vectors, value=encoder_vectors)\n",
        "    \n",
        "    attention = keras.layers.Dropout(rate=.1)(attention)\n",
        "    attention = keras.layers.Add()([encoder_vectors, attention])\n",
        "    attention = keras.layers.BatchNormalization()(attention)    \n",
        "\n",
        "    # updated encoder vectors\n",
        "    encoder_vectors = keras.layers.Dense(decoder_units, 'relu')(attention)    \n",
        "    \n",
        "    encoder_vectors = keras.layers.Dropout(rate=.1)(encoder_vectors)\n",
        "    encoder_vectors = keras.layers.Add()([attention, encoder_vectors])\n",
        "    encoder_vectors = keras.layers.BatchNormalization()(encoder_vectors)     \n",
        "\n",
        "    # output\n",
        "    outputs = [encoder_vectors]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g8x4_cpkJ9A",
        "outputId": "38387819-8d8c-444f-efb8-6941fae5778f"
      },
      "source": [
        "EncoderAttention(decoder_units=320, num_att_elems=196).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"EncoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_vectors (InputLayer)    [(None, 196, 320)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention (MultiHeadAtt (None, 196, 320)     410880      encoder_vectors[0][0]            \n",
            "                                                                 encoder_vectors[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 196, 320)     0           encoder_attention[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_24 (Add)                    (None, 196, 320)     0           encoder_vectors[0][0]            \n",
            "                                                                 dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 196, 320)     1280        add_24[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_35 (Dense)                (None, 196, 320)     102720      batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 196, 320)     0           dense_35[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 196, 320)     0           batch_normalization_32[0][0]     \n",
            "                                                                 dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 196, 320)     1280        add_25[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 516,160\n",
            "Trainable params: 514,880\n",
            "Non-trainable params: 1,280\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qFDs9RTjvod"
      },
      "source": [
        "## Decoder Self-Attention\n",
        "\n",
        "Text Feature extraction + Encoder/Decoder Joint Attention interaction.\n",
        "\n",
        "With use_covolutions set to False, this is the decoder self-attention feature-extraction step from \"Attention is All You Need,\" cited above (with learned positional encoding). \n",
        "\n",
        "Includes an (optional) parameter to add a small convolutional layer for feature enhancement before the attention layer. This is included for experimentation / verification that attention really is all you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqw6CPAIavh5"
      },
      "source": [
        "def DecoderAttention(embedding_dim, decoder_units, max_len, \n",
        "                     use_convolutions=True, name='DecoderAttention'):\n",
        "\n",
        "    # inputs\n",
        "    embedded_text = keras.layers.Input([max_len, embedding_dim], name='embedded_text')  # zero=masked\n",
        "    mask = keras.layers.Input([max_len, max_len], name='mask')   # used to avoid leaking future info\n",
        "\n",
        "    inputs = [embedded_text, mask]\n",
        "\n",
        "    # positional encoding\n",
        "    initializer = tf.random_normal_initializer()\n",
        "    position_enc = tf.Variable(initializer(shape=[max_len, decoder_units], \n",
        "                                             dtype=tf.float32))\n",
        "    position_enc = tf.expand_dims(position_enc, 0)  # for broadcasting against batch\n",
        "\n",
        "    # mask the text input\n",
        "    decoder_features = embedded_text * tf.expand_dims(mask[:, :, 0], axis=2)\n",
        "    decoder_features = tf.keras.layers.Masking(mask_value=0.0)(decoder_features)  # not sure if Conv1D accepts masks?\n",
        "\n",
        "    # (Optional, for experimentation) update features using convolution kernel\n",
        "    if use_convolutions:\n",
        "        # crop to unmasked input\n",
        "        step = tf.math.argmin(mask[0, :, 0])\n",
        "        decoder_features = tf.keras.layers.Conv1D(filters=decoder_units, kernel_size=3, \n",
        "                    strides=1, padding='same', groups=1)(decoder_features[:, :step, :])\n",
        "\n",
        "        # pad back to pull length for uniform (masked) Attention input\n",
        "        decoder_features = tf.pad(decoder_features, [[0,0],[0, max_len - step], [0,0]])\n",
        "    \n",
        "    # Add positional encoding\n",
        "    decoder_features = position_enc + decoder_features\n",
        "\n",
        "    # Decoder Self-Attention Block (with mask)\n",
        "    decoder_attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=8, key_dim=decoder_units//8, name='decoder_attention')(\n",
        "                query=decoder_features, value=decoder_features, attention_mask=mask)\n",
        "\n",
        "    decoder_attention = keras.layers.Dropout(rate=.1)(decoder_attention)            \n",
        "    decoder_attention = keras.layers.Add()([decoder_features, decoder_attention])\n",
        "    decoder_attention = keras.layers.BatchNormalization()(decoder_attention)     \n",
        "\n",
        "    outputs = [decoder_attention]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3XHlnkEb4gF",
        "outputId": "c8af9273-7346-474f-c2df-e45b1bbb8cd0"
      },
      "source": [
        "DecoderAttention(embedding_dim=352, decoder_units=352, max_len=200, use_convolutions=True).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "mask (InputLayer)               [(None, 200, 200)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_15 (Sl (None, 200)          0           mask[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "embedded_text (InputLayer)      [(None, 200, 352)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf.expand_dims_6 (TFOpLambda)   (None, 200, 1)       0           tf.__operators__.getitem_15[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.math.multiply_6 (TFOpLambda) (None, 200, 352)     0           embedded_text[0][0]              \n",
            "                                                                 tf.expand_dims_6[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_16 (Sl (200,)               0           mask[0][0]                       \n",
            "__________________________________________________________________________________________________\n",
            "masking_6 (Masking)             (None, 200, 352)     0           tf.math.multiply_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.math.argmin_1 (TFOpLambda)   ()                   0           tf.__operators__.getitem_16[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.getitem_17 (Sl (None, None, 352)    0           masking_6[0][0]                  \n",
            "                                                                 tf.math.argmin_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv1d_1 (Conv1D)               (None, None, 352)    372064      tf.__operators__.getitem_17[0][0]\n",
            "__________________________________________________________________________________________________\n",
            "tf.math.subtract_1 (TFOpLambda) ()                   0           tf.math.argmin_1[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "tf.compat.v1.pad_1 (TFOpLambda) (None, None, 352)    0           conv1d_1[0][0]                   \n",
            "                                                                 tf.math.subtract_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_14 (TFOpLa (None, 200, 352)     0           tf.compat.v1.pad_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "decoder_attention (MultiHeadAtt (None, 200, 352)     497024      tf.__operators__.add_14[0][0]    \n",
            "                                                                 mask[0][0]                       \n",
            "                                                                 tf.__operators__.add_14[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 200, 352)     0           decoder_attention[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "add_26 (Add)                    (None, 200, 352)     0           tf.__operators__.add_14[0][0]    \n",
            "                                                                 dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 200, 352)     1408        add_26[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 870,496\n",
            "Trainable params: 869,792\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP-t1MkKnD5L"
      },
      "source": [
        "##  Joint Encoder-Decoder Attention\n",
        "\n",
        "This is the 'translation' component where our image features interacts with our (masked) text features. Masking is used to prevent information leak so only known text values are used at a given time step. This is the encoder-decoder attention step from \"Attention is All You Need.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m927YThukFs-"
      },
      "source": [
        "def JointAttention(decoder_units, num_att_elems, max_len, name='JointAttention'):\n",
        "\n",
        "    encoder_attention = keras.layers.Input([num_att_elems, decoder_units], name='encoder_attention')   # from image\n",
        "    decoder_attention = keras.layers.Input([max_len, decoder_units], name='decoder_attention')   # from known text\n",
        "\n",
        "    inputs = [encoder_attention, decoder_attention]\n",
        "    \n",
        "    # Encode-Decoder Attention Block\n",
        "    joint_attention = tf.keras.layers.MultiHeadAttention(\n",
        "            num_heads=8, key_dim=decoder_units//8, name='joint_attention')(\n",
        "                query=decoder_attention, value=encoder_attention)\n",
        "            \n",
        "    joint_attention = keras.layers.Dropout(rate=.1)(joint_attention)          \n",
        "    joint_attention = keras.layers.Add()([decoder_attention, joint_attention])\n",
        "    joint_attention = keras.layers.BatchNormalization()(joint_attention)    \n",
        "\n",
        "    outputs = [joint_attention]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcaTvpPWkqgO",
        "outputId": "ed76938b-4b8f-4488-9616-9e26ede7894d"
      },
      "source": [
        "JointAttention(decoder_units=32, num_att_elems=50, max_len=200, name='JointAttention').summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"JointAttention\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "decoder_attention (InputLayer)  [(None, 200, 32)]    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_attention (InputLayer)  [(None, 50, 32)]     0                                            \n",
            "__________________________________________________________________________________________________\n",
            "joint_attention (MultiHeadAtten (None, 200, 32)      4224        decoder_attention[0][0]          \n",
            "                                                                 encoder_attention[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 200, 32)      0           joint_attention[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_27 (Add)                    (None, 200, 32)      0           decoder_attention[0][0]          \n",
            "                                                                 dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 200, 32)      128         add_27[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 4,352\n",
            "Trainable params: 4,288\n",
            "Non-trainable params: 64\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38GA7wtNEhqW"
      },
      "source": [
        "## Decoder Head (Prediction Output)\n",
        "\n",
        "This is where we use what was learned in the encoder-decoder attention to output predicted labels. It is the prediction step from \"Attention is All You Need.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2LMbmclf1hs"
      },
      "source": [
        "def DecoderHead(decoder_units, vocab_size, max_len, name='DecoderHead'):\n",
        "    \n",
        "    decoder_input = keras.layers.Input([max_len, decoder_units])  # from Decoder Attention layer\n",
        "\n",
        "    inputs = [decoder_input]\n",
        "\n",
        "    # Prediction Block\n",
        "    decoder_out = keras.layers.Dense(decoder_units, activation='relu',\n",
        "                kernel_initializer= tf.keras.initializers.HeNormal())(decoder_input)\n",
        "\n",
        "    decoder_out = keras.layers.Dropout(rate=.1)(decoder_out)\n",
        "    decoder_out = decoder_input + decoder_out\n",
        "    decoder_out = keras.layers.BatchNormalization()(decoder_out)\n",
        "\n",
        "    probs = keras.layers.Dense(vocab_size, activation='softmax',\n",
        "                kernel_initializer= tf.keras.initializers.HeNormal())(decoder_out)\n",
        "\n",
        "    outputs = [probs]\n",
        "\n",
        "    return keras.Model(inputs, outputs, name=name)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js6yvxIGf1hs",
        "outputId": "a2f0939d-d84e-4195-9255-a2ea3d073f89"
      },
      "source": [
        "DecoderHead(decoder_units=320, vocab_size=199, max_len=200).summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"DecoderHead\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_18 (InputLayer)           [(None, 200, 320)]   0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_36 (Dense)                (None, 200, 320)     102720      input_18[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 200, 320)     0           dense_36[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf.__operators__.add_15 (TFOpLa (None, 200, 320)     0           input_18[0][0]                   \n",
            "                                                                 dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 200, 320)     1280        tf.__operators__.add_15[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "dense_37 (Dense)                (None, 200, 199)     63879       batch_normalization_36[0][0]     \n",
            "==================================================================================================\n",
            "Total params: 167,879\n",
            "Trainable params: 167,239\n",
            "Non-trainable params: 640\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2UR1DLljD0S"
      },
      "source": [
        "## Update Mechanism (Optional)\n",
        "\n",
        "*Note: this is fully coded but I have not had time to train parameters with it. I leave that as a future opportunity for exploration.*\n",
        "\n",
        "NLP technicques typically output logits to find the highest likelhood token prediction. This can be improved to a (local) maximum likelihood selection using a \"beam step\" that ay override the initial prediction choice. \n",
        "\n",
        "This layer is an alternative system for updating predictions. Unlike \"beam,\" it is trainable and includes longer-range dependencies (instead of the very \"local\" beam step.) The entire original prediction is passed through a bidirectional RNN. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpM3FS5zZ-3Y"
      },
      "source": [
        "def BeamUpdate(decoder_units, beam_rnn_units, input_dim, vocab_size, name='BeamUpdate'):\n",
        "    \n",
        "    # rnn layers\n",
        "    BeamUnit = keras.layers.GRU(beam_rnn_units, return_sequences=True, return_state=True, go_backwards=True)\n",
        "    \n",
        "    # Inputs\n",
        "    beam_input = keras.layers.Input([None, input_dim], dtype=tf.float32, name='beam_input') \n",
        "    hidden_state = keras.layers.Input([decoder_units], dtype=tf.float32, name='hidden_state')\n",
        "    \n",
        "    inputs = [beam_input, hidden_state]\n",
        "\n",
        "    # downscale hidden state to beam dims\n",
        "    beam_hidden_state = keras.layers.Dense(beam_rnn_units, activation='relu',\n",
        "                              kernel_initializer= tf.keras.initializers.HeNormal()\n",
        "                              )(hidden_state)   \n",
        "    # RNN\n",
        "    beam_out, beam_hidden_state = \\\n",
        "        BeamUnit(beam_input, initial_state=[beam_hidden_state])  # beam 1\n",
        "\n",
        "    \n",
        "    # logits\n",
        "    probs = keras.layers.Dense(vocab_size, activation='softmax', name='dense_beam_probs',\n",
        "                   kernel_initializer= tf.keras.initializers.HeNormal())(beam_out)\n",
        "\n",
        "    outputs = [probs]\n",
        "    \n",
        "    return keras.Model(inputs, outputs, name=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnHn_q5xcsB7",
        "outputId": "37b3cdb5-2274-498b-8c18-ef8622814cb2"
      },
      "source": [
        "temp_beam = BeamUpdate(decoder_units=320, beam_rnn_units=128, input_dim=130, vocab_size=199, name='BeamUpdate')\n",
        "temp_beam.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"BeamUpdate\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "hidden_state (InputLayer)       [(None, 320)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "beam_input (InputLayer)         [(None, None, 130)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_38 (Dense)                (None, 128)          41088       hidden_state[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "gru_6 (GRU)                     [(None, None, 128),  99840       beam_input[0][0]                 \n",
            "                                                                 dense_38[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_beam_probs (Dense)        (None, None, 199)    25671       gru_6[0][0]                      \n",
            "==================================================================================================\n",
            "Total params: 166,599\n",
            "Trainable params: 166,599\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6GIs3f3rpu0"
      },
      "source": [
        "# **Full Model**\n",
        "\n",
        "All the components are combined into a full encoder/decoder model. This is implemented using the subclassing API with custom call, train,  evaluation and prediction steps. Once initialized, the models have full access to high-level model.fit(), model.compile() and model.save_weights() methods.\n",
        "\n",
        "An extra features implemented is having Decoder() elements in *series* (not stacked). This adds more trainable parameters without affecting inference speed, and allows decoders to specialize more on different regions of the text.\n",
        "\n",
        "BaseTrainer() model has the BeamUpdate mechanism disabled. InchiGenerator() models include the BeamUpdate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcEV3ifLf1hs"
      },
      "source": [
        "class BaseTrainer(keras.Model):\n",
        "    \n",
        "    def __init__(self, beam_rnn_units, attention_dim, use_dense_encoder_top, \n",
        "                 use_convolutions, use_dual_decoder, \n",
        "                 max_len, parameters, name='BaseTrainer', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        \"\"\"\n",
        "        Beam updates turned off. \n",
        "        Training conducted with teach-fed inputs.\n",
        "\n",
        "        note: dataset provided as (image, image_id, parsed_InChI, InChI)\n",
        "        \"\"\"\n",
        "    \n",
        "        # hard-coded parameters\n",
        "        self.regularization_factor = 0.0\n",
        "        \n",
        "        # other params (required values)\n",
        "        self.max_len = max_len  # max number of token prediction length\n",
        "        self.beam_rnn_units = beam_rnn_units\n",
        "        self.attention_dim = attention_dim\n",
        "        self.use_dense_encoder_top = use_dense_encoder_top\n",
        "        self.use_convolutions = use_convolutions\n",
        "        self.use_dual_decoder = use_dual_decoder\n",
        "        self.parameters = parameters\n",
        "        self.SOS = parameters.SOS()\n",
        "        self.EOS = parameters.EOS()\n",
        "        \n",
        "        # tokenizer / inverse tokenizer\n",
        "        self.tokenizer_layer, self.inverse_tokenizer, self.tokenized_EOS = \\\n",
        "            Tokenizer(parameters=self.parameters, padded_length=self.max_len)\n",
        "        self.vocab_size = len(self.tokenizer_layer.get_vocabulary())\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'beam_rnn_units': self.beam_rnn_units, \n",
        "                  'attention_dim':self.attention_dim,\n",
        "                  'use_dense_encoder_top': self.use_dense_encoder_top,\n",
        "                  'use_convolutions':use_convolutions,\n",
        "                  'max_len':self.max_len,\n",
        "                  'use_dual_decoder': self.use_dual_decoder,\n",
        "                  'parameters': self.parameters}\n",
        "        return config \n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # note: dataset prepared with dict keys (image, image_id, parsed_InChI, InChI)\n",
        "        \n",
        "        self.batch_size = input_shape['image'][0]\n",
        "\n",
        "        # encoder\n",
        "        self.image_shape = input_shape['image'][1:]  # drops batch dims       \n",
        "        self.image_encoder = ImageEncoder(image_shape=self.image_shape, \n",
        "                                          output_dim=self.attention_dim,\n",
        "                                          use_dense_top=self.use_dense_encoder_top,\n",
        "                                          name='ImageEncoder')   \n",
        "\n",
        "        # collect params\n",
        "        self.decoder_units = self.image_encoder.output_shape[-1]\n",
        "        self.num_att_vectors = self.image_encoder.output_shape[-2]\n",
        "        self.embedding_dim = self.decoder_units  # required for consistency\n",
        "\n",
        "        # trainable start value\n",
        "        mean = 1.0 / self.embedding_dim\n",
        "        intializer = tf.random_normal_initializer(mean=mean, stddev=.2*mean)\n",
        "        self.start_var = tf.Variable(intializer(shape=[1, 1, self.embedding_dim]), \n",
        "                                     name='start_var')\n",
        "        \n",
        "        # InChI token embedding layers        \n",
        "        self.inchi_prep = InchiPrep(self.tokenizer_layer, self.vocab_size, \n",
        "                                    self.embedding_dim, name='InchiPrep')\n",
        "        self.embedding_layer = self.inchi_prep.get_layer('Embedding')\n",
        "\n",
        "        # attentions\n",
        "        self.encoder_attention = EncoderAttention(decoder_units=self.decoder_units, \n",
        "                                                  num_att_elems=self.num_att_vectors,\n",
        "                                                  name='EncoderAttention')\n",
        "        self.decoder_attention = DecoderAttention(embedding_dim=self.embedding_dim, \n",
        "                                                decoder_units=self.decoder_units, \n",
        "                                        #num_att_elems=self.num_att_vectors,\n",
        "                                                max_len=self.max_len,\n",
        "                                                use_convolutions=self.use_convolutions,\n",
        "                                                name='DecoderAttention')   \n",
        "        self.joint_attention = JointAttention(decoder_units=self.decoder_units, \n",
        "                                              num_att_elems=self.num_att_vectors,\n",
        "                                              max_len=self.max_len,\n",
        "                                              name='JointAttention') \n",
        "        \n",
        "        # collect params\n",
        "        self.rnn_input_dim = self.encoder_attention.output_shape[-1]\n",
        "        beam_input_dim = self.vocab_size\n",
        "        \n",
        "        # decoders\n",
        "        self.start1 = 50  # step to switch to next decoder\n",
        "        self.decoder_0 = DecoderHead(self.decoder_units, self.vocab_size, \n",
        "                                     max_len=self.max_len, name='DecoderHead_0')\n",
        "        \n",
        "        if self.use_dual_decoder:\n",
        "            self.decoder_1 = DecoderHead(self.decoder_units, self.vocab_size, \n",
        "                                         max_len=self.max_len, name='DecoderHead_1')\n",
        "        else:\n",
        "            self.decoder_1 = self.decoder_0\n",
        "\n",
        "        # prediction update\n",
        "        self.beam = BeamUpdate(self.decoder_units, self.beam_rnn_units, beam_input_dim, self.vocab_size, name='beam')\n",
        "\n",
        "\n",
        "    def encoding_step(self, image, parsed_inchi):\n",
        "        \n",
        "        # text encoding\n",
        "        start_var_batch = tf.tile(self.start_var, [self.batch_size, 1, 1])\n",
        "\n",
        "        # tokenize and encode InChI\n",
        "        inchi, targets = self.inchi_prep([parsed_inchi, start_var_batch])\n",
        "        \n",
        "        # image encoding\n",
        "        image = self.image_encoder(image)\n",
        "        encoder_attention = self.encoder_attention(image)\n",
        "\n",
        "        return encoder_attention, inchi, targets\n",
        "\n",
        "    def call(self, inputs, training=False):#, validation=False):        \n",
        "        # note: dataset provided as (image, image_id, parsed_InChI, InChI)\n",
        "\n",
        "        # generation step options\n",
        "        use_beam = False\n",
        "        use_preds = not training\n",
        "\n",
        "        # inputs\n",
        "        image = inputs['image']\n",
        "        parsed_inchi = inputs['parsed_InChI']\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_attention, inchi, targets = self.encoding_step(image, parsed_inchi)\n",
        "\n",
        "        # Decoder\n",
        "        predictions, probs = self.generation_loop(use_preds, use_beam, \n",
        "                                                   encoder_attention,\n",
        "                                                   inchi, targets)\n",
        "        \n",
        "        return targets, predictions, probs\n",
        "\n",
        "    def predict(self, data):\n",
        "        # note: dataset provided as (image, image_id, parsed_InChI, InChI)\n",
        "\n",
        "        # get image id (passed through to output)  \n",
        "        image_id = data['image_id'] \n",
        "        \n",
        "        # generate predictions\n",
        "        targets, predictions, probs = self(data, training=False)\n",
        "\n",
        "        # convert back to string\n",
        "        generated_predictions, generated_predictions_parsed = \\\n",
        "            self.tokens_to_string(predictions) \n",
        "\n",
        "        return image_id, generated_predictions\n",
        "\n",
        "    def train_step(self, data):\n",
        "        print('start')  # only shows upon function tracing\n",
        "\n",
        "        # get loss and grads\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            targets, predictions, probs = self(data, training=True)       \n",
        "            loss = self.compiled_loss(targets, probs)\n",
        "\n",
        "            # add any regularization losses\n",
        "            loss += tf.math.reduce_sum(self.losses) * self.regularization_factor\n",
        "\n",
        "        gradients = tape.gradient(loss, self.trainable_variables) \n",
        "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
        "        \n",
        "        # update metrics\n",
        "        self.compiled_metrics.update_state(targets, probs)\n",
        "        \n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "    # validation step\n",
        "    def test_step(self, data):\n",
        "        \n",
        "        # Compute predictions\n",
        "        targets, predictions, probs = self(data, training=False)       \n",
        "        \n",
        "        # record loss\n",
        "        self.compiled_loss(targets, probs)\n",
        "\n",
        "        # Update the metrics.\n",
        "        self.compiled_metrics.update_state(targets, probs)\n",
        "\n",
        "        return {m.name: m.result() for m in self.metrics}\n",
        "\n",
        "\n",
        "    # Full Generation Loop\n",
        "    def generation_loop(self, use_preds, use_beam, encoder_attention, inchi, target):\n",
        "        \n",
        "        # containers\n",
        "        # note: JIT/XLA not compatible with dynamic size TensorArrays. Update def if using JIT\n",
        "        tokens_array = tf.TensorArray(size=1, dtype=tf.int64, dynamic_size=True, tensor_array_name='tokens_array')\n",
        "        probs_array = tf.TensorArray(size=1, dtype=tf.float32, dynamic_size=True, tensor_array_name='probs_array')\n",
        "        \n",
        "        # \"while loop\" function\n",
        "        def loop_fn(step, continue_cond, inchi_step, tokens_array, probs_array):\n",
        "\n",
        "            # create mask\n",
        "            mask = tf.ones((step + 1, step + 1))\n",
        "            mask = tf.pad(mask, [[0, self.max_len - step - 1], \n",
        "                                 [0, self.max_len - step - 1]])\n",
        "            mask = tf.expand_dims(mask, 0)\n",
        "            mask = tf.tile(mask, [self.batch_size, 1, 1])\n",
        "\n",
        "            # attention update\n",
        "            decoder_att = self.decoder_attention([inchi_step, mask])\n",
        "            joint_attention = self.joint_attention([encoder_attention, decoder_att])\n",
        "            \n",
        "            # get char probs\n",
        "            # (use correct decoder for position in sequence)\n",
        "            if tf.math.less(step, self.start1):\n",
        "                probs = self.decoder_0([joint_attention])  \n",
        "\n",
        "            else:  # tf.math.less(step, self.start2):\n",
        "                probs = self.decoder_1([joint_attention])       \n",
        "\n",
        "            # select current step's probabilities and predictions\n",
        "            probs = probs[:, step:step+1, :]\n",
        "            predictions = tf.argmax(probs, axis=-1)\n",
        "\n",
        "            # save results\n",
        "            tokens_array = tokens_array.write(step, predictions)\n",
        "            probs_array = probs_array.write(step, probs)\n",
        "\n",
        "            # check early stopping criteria\n",
        "            if use_preds: \n",
        "                predictions = tf.expand_dims(predictions, axis=1)\n",
        "                continue_cond = tf.math.reduce_any(predictions != self.tokenized_EOS)\n",
        "                predictions = tf.squeeze(predictions, axis=1)\n",
        "            \n",
        "            # update continue condition\n",
        "            if continue_cond:\n",
        "                continue_cond = tf.math.less(step + 1, self.max_len)\n",
        "            \n",
        "            # prepare next input if continuing\n",
        "            if continue_cond:\n",
        "        \n",
        "                step = step + 1\n",
        "                if use_preds:\n",
        "                    predictions = self.embedding_layer(predictions)\n",
        "\n",
        "                    # pad to match inchi_step shape\n",
        "                    predictions = tf.pad(predictions, \n",
        "                        [[0,0], [step, tf.math.maximum(0, self.max_len - step - 1)], [0,0]])\n",
        "\n",
        "                    inchi_step = predictions + inchi_step\n",
        "                \n",
        "            # caution: make sure input is masked during decoder attention!\n",
        "\n",
        "            return [step, continue_cond, inchi_step, tokens_array, probs_array]\n",
        "\n",
        "        # stopping condition function\n",
        "        def cond_fn(step, continue_cond, inchi_step, tokens_array, probs_array):\n",
        "            return continue_cond\n",
        "\n",
        "        # generation loop\n",
        "        step = 0\n",
        "        continue_cond = True\n",
        "\n",
        "        inchi_step = inchi  # add step for final prediction\n",
        "        # note: inchi_step masking done during decoder attention\n",
        "\n",
        "        step, continue_cond, inchi_step, tokens_array, probs_array \\\n",
        "            = tf.while_loop(\n",
        "                    cond=cond_fn, \n",
        "                    body=loop_fn, \n",
        "                    loop_vars=[step, continue_cond, inchi_step, tokens_array, probs_array], \n",
        "                    maximum_iterations=self.max_len,                \n",
        "                    shape_invariants=[tf.TensorShape([]), # step\n",
        "                                      tf.TensorShape([]), # continue_cond\n",
        "                                      tf.TensorShape([None, self.max_len, self.embedding_dim]), # inchi_step\n",
        "                                      None, #tokens_array\n",
        "                                      None] #probs_array  # attention scores\n",
        "                    )\n",
        "        \n",
        "        # unpack token arrays\n",
        "        predicted_tokens = tokens_array.stack()  # predicted characters\n",
        "        predicted_tokens = tf.squeeze(predicted_tokens)\n",
        "        predicted_tokens = tf.transpose(predicted_tokens, perm=[1, 0])   \n",
        "\n",
        "        # unpack probs_array (no beam update)\n",
        "        predicted_probs = probs_array.stack()  # predicted logits\n",
        "        predicted_probs = tf.squeeze(predicted_probs)\n",
        "        predicted_probs = tf.transpose(predicted_probs, perm=[1, 0, 2])  \n",
        "\n",
        "        # (optional) beam update\n",
        "        if use_beam:\n",
        "            beam_inputs = predicted_probs\n",
        "\n",
        "            # pad/crop to uniform length and mask\n",
        "            mask_value = -1.0\n",
        "\n",
        "            # pad\n",
        "            beam_inputs = tf.pad(beam_inputs, constant_values=mask_value,\n",
        "                                 paddings=([[0, 0], [0, 200], [0, 0]]))\n",
        "            # crop\n",
        "            beam_inputs = beam_inputs[:, :self.max_len]\n",
        "            # mask\n",
        "            beam_inputs = tf.keras.layers.Masking(mask_value=mask_value)(beam_inputs)\n",
        "\n",
        "            # create initial RNN state\n",
        "            initial_state = tf.math.reduce_mean(encoder_attention, axis=1)\n",
        "\n",
        "            # get probs and predictions\n",
        "            predicted_probs = self.beam([beam_inputs, initial_state])\n",
        "            predicted_tokens = tf.argmax(predicted_probs, axis=-1)\n",
        "\n",
        "        return predicted_tokens, predicted_probs\n",
        "    \n",
        "    \n",
        "    def tokens_to_string(self, tokens):\n",
        "        parsed_string_vals = self.inverse_tokenizer(tf.constant(tokens))\n",
        "        string_vals = keras.layers.Lambda(lambda x: tf.strings.reduce_join(x, axis=-1))(parsed_string_vals)\n",
        "\n",
        "        # remove first EOS generated and everything after\n",
        "        pattern = ''.join([self.EOS, '.*$'])\n",
        "        string_vals = tf.strings.regex_replace(string_vals, pattern, rewrite='', \n",
        "                                               replace_global=True, name='remove_EOS')   \n",
        "\n",
        "        return string_vals, parsed_string_vals\n",
        "    \n",
        "    def update_max_len(self, new_value):\n",
        "        self.max_len = new_value\n",
        "\n",
        "        # update padding on tokenizer and layers with params depending on it\n",
        "        self.tokenizer_layer, self.inverse_tokenizer, self.tokenized_EOS = \\\n",
        "            Tokenizer(parameters=self.parameters, padded_length=self.max_len)\n",
        "\n",
        "        self.inchi_prep = InchiPrep(self.tokenizer_layer, self.vocab_size, \n",
        "                                    self.embedding_dim, name='InchiPrep')\n",
        "        \n",
        "        self.embedding_layer = self.inchi_prep.get_layer('Embedding')\n",
        "        return None\n",
        "    \n",
        "    def update_reg_factor(self, value):\n",
        "        # note: might need to recompile model before next training\n",
        "        self.regularization_factor = value\n",
        "\n",
        "    \"\"\"  \n",
        "    # Our Tensorflow metric calculates Levenshtein scores of the tokens.\n",
        "    # To calculate the true character-level score use this:\n",
        "    \n",
        "    !pip install levenshtein\n",
        "    from leven import levenshtein\n",
        "\n",
        "    def compute_levenshtein_scores(self, inchi_true, inchi_predicted):\n",
        "        scores = [levenshtein(pred, orig) for (pred, orig)\n",
        "                  in zip(inchi_predicted.numpy().tolist(), inchi_true.numpy().tolist())]\n",
        "        return tf.reduce_mean(scores)\n",
        "    \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5TuwjWo0RRB"
      },
      "source": [
        "class InchiGenerator(BaseTrainer):\n",
        "    \"\"\"\n",
        "    Beam updates turned on, training conducted using generated preds.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_model, name='BeamInchiTrainer', **kwargs):\n",
        "\n",
        "        super().__init__(beam_rnn_units=base_model.beam_rnn_units, \n",
        "                         attention_dim=base_model.attention_dim,\n",
        "                         use_dense_encoder_top=base_model.use_dense_encoder_top,\n",
        "                         use_convolutions=base_model.use_convolutions,\n",
        "                         use_dual_decoder= base_model.use_dual_decoder,\n",
        "                         max_len = base_model.max_len,\n",
        "                         parameters=base_model.parameters, \n",
        "                         name=name, **kwargs)\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "\n",
        "        # generation step options\n",
        "        use_beam = True\n",
        "        use_preds = True\n",
        "\n",
        "        # inputs\n",
        "        # note: dataset provided as (image, image_id, parsed_InChI, InChI)\n",
        "        image = inputs['image']      \n",
        "        parsed_inchi = inputs['parsed_InChI']\n",
        "        \n",
        "        # Encoder\n",
        "        encoder_attention, inchi, targets = self.encoding_step(image, parsed_inchi)\n",
        "\n",
        "        # Decoder\n",
        "        predictions, probs = self.generation_loop(use_preds, use_beam, \n",
        "                                                   encoder_attention,\n",
        "                                                   inchi, targets)\n",
        "        \n",
        "        return targets, predictions, probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtWg2jRlI1_W"
      },
      "source": [
        "class EditDistanceMetric(tf.keras.metrics.Metric):\n",
        "    def __init__(self, name='edit_distance', **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.edit_distance = self.add_weight(name='edit_distance', initializer='zeros')\n",
        "        self.batch_counter = self.add_weight(name='batch_counter', initializer='zeros')\n",
        "    \n",
        "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "        y_true = tf.sparse.from_dense(y_true)\n",
        "        y_pred = tf.sparse.from_dense(tf.argmax(y_pred, axis=-1))  # convert probs to preds\n",
        "\n",
        "        # compute edit distance (of parsed tokens)\n",
        "        edit_distance = tf.edit_distance(y_pred, y_true, normalize=False)\n",
        "        self.edit_distance.assign_add(tf.reduce_mean(edit_distance))\n",
        "\n",
        "        self.batch_counter.assign_add(1.)\n",
        "    \n",
        "    def result(self):\n",
        "        return self.edit_distance / self.batch_counter\n",
        "\n",
        "    def reset_state(self):\n",
        "        # The state of the metric will be reset at the start of each epoch.\n",
        "        self.edit_distance.assign(0.0)\n",
        "        self.batch_counter.assign(0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zt3klX0SYnVw"
      },
      "source": [
        "# Learning rate schedule used in \"Attention is All You Need\"\n",
        "\n",
        "class LRScheduleAIAYN(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "\n",
        "    def __init__(self, scale_factor=1, warmup_steps=4000):  # defaults reflect paper's values\n",
        "        self.warmup_steps = tf.constant(warmup_steps, dtype=tf.float32)\n",
        "        dim = tf.constant(352, dtype=tf.float32)\n",
        "        self.scale = scale_factor * tf.math.pow(dim, -1.5)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        opt_1 = tf.math.pow(step, -.5)\n",
        "        opt_2 = step * tf.math.pow(self.warmup_steps, -1.5)\n",
        "        return self.scale * tf.math.reduce_min([opt_1, opt_2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUtswyx2ru4O"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yhZAEoiSYsZb"
      },
      "source": [
        "Model compile options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDez379bi8Xy"
      },
      "source": [
        "NAME_MODIFIER = ''\n",
        "\n",
        "# build model\n",
        "ATTENTION_DIM = 208  # note: only used if USE_DENSE_ENCODER_TOP = True.\n",
        "                     # value from Darien Schettler public Kaggle notebook\n",
        "BEAM_RNN_UNITS = 128  # note: only used in beam_model\n",
        "USE_DENSE_ENCODER_TOP = True\n",
        "USE_CONVOLUTIONS = False\n",
        "USE_DUAL_DECODERS = False\n",
        "if USE_CONVOLUTIONS:\n",
        "    checkpoint_save_name = 'ConvAtt_model_weights' + NAME_MODIFIER\n",
        "else:\n",
        "    checkpoint_save_name = 'AISAYN_model_weights' + NAME_MODIFIER\n",
        "\n",
        "LOAD_CHECKPOINT_FILE = os.path.join(PARAMETERS.load_checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n",
        "SAVE_CHECKPOINT_FILE = os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7s5jU6OYwax"
      },
      "source": [
        "Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "n0OTg89vf1ht",
        "outputId": "053099fc-23c2-471d-b8c2-ea8ff589f765"
      },
      "source": [
        "# initialize models\n",
        "for val in train_ds.take(1):\n",
        "    with PARAMETERS.strategy().scope():  # (\"distribution strategy\" defined at top of notebook)\n",
        "        model_base = BaseTrainer(beam_rnn_units=BEAM_RNN_UNITS, # only used in full model\n",
        "                                 attention_dim=ATTENTION_DIM,  # value used in \"All You Need is Attention\"\n",
        "                                 use_dense_encoder_top=USE_DENSE_ENCODER_TOP,\n",
        "                                 use_convolutions=USE_CONVOLUTIONS,\n",
        "                                 use_dual_decoder=USE_DUAL_DECODERS,\n",
        "                                 max_len=175,  # 133 is recommended value from Darien Schettler Kaggle notebook\n",
        "                                 parameters=PARAMETERS, \n",
        "                                 name='BaseTrainer')\n",
        "        \n",
        "        # use larger batch size for inference\n",
        "        # (model needs to be rebuilt to change batch size)\n",
        "        model_inference = BaseTrainer(beam_rnn_units=BEAM_RNN_UNITS, # only used in full model\n",
        "                                 attention_dim=ATTENTION_DIM,  # value used in \"All You Need is Attention\"      \n",
        "                                 use_dense_encoder_top=USE_DENSE_ENCODER_TOP,                                \n",
        "                                 use_convolutions=USE_CONVOLUTIONS,\n",
        "                                 use_dual_decoder=USE_DUAL_DECODERS,\n",
        "                                 max_len=175,  # 133 is recommended value from Darien Schettler Kaggle notebook\n",
        "                                 parameters=PARAMETERS, \n",
        "                                 name='BaseInference')\n",
        "        \n",
        "        \"\"\" \n",
        "        # Beam-update model\n",
        "        model_beam = InchiGenerator(model_base, name='InchiGenerator_inference')  \n",
        "        \"\"\"\n",
        "\n",
        "    print('Models initialized.')\n",
        "\n",
        "# build / verify function call work\n",
        "for val in train_ds.take(1):\n",
        "    model_base(val, training=False)\n",
        "    model_base(val, training=True)\n",
        "    model_base.predict(val)\n",
        "\n",
        "    \"\"\"\n",
        "    model_beam(val, training=False)\n",
        "    model_beam(val, training=True)\n",
        "    model_beam.predict(val)\n",
        "    \"\"\"\n",
        "\n",
        "#for val2 in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(1):\n",
        "#    model_inference(val2)\n",
        "\n",
        "# sync weights\n",
        "# WARNING!: in Kaggle this loads from prev session saved weights\n",
        "try:\n",
        "    #model_base.load_weights(LOAD_CHECKPOINT_FILE)  \n",
        "    #model_inference.load_weights(LOAD_CHECKPOINT_FILE)\n",
        "except:\n",
        "    print('No weights loaded')\n",
        "\n",
        "print('\\n\\n')    \n",
        "model_base.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Models initialized.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Exception ignored in: <function IteratorResourceDeleter.__del__ at 0x7f8829c3d830>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 546, in __del__\n",
            "    handle=self._handle, deleter=self._deleter)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1264, in delete_iterator\n",
            "    _ctx, \"DeleteIterator\", name, handle, deleter)\n",
            "KeyboardInterrupt: \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-122-5cbe11cc7a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mmodel_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mmodel_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \"\"\"\n",
            "\u001b[0;32m<ipython-input-108-e316ffe7a11c>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# generate predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;31m# convert back to string\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-e316ffe7a11c>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    134\u001b[0m         predictions, probs = self.generation_loop(use_preds, use_beam, \n\u001b[1;32m    135\u001b[0m                                                    \u001b[0mencoder_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                                                    inchi, targets)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-e316ffe7a11c>\u001b[0m in \u001b[0;36mgeneration_loop\u001b[0;34m(self, use_preds, use_beam, encoder_attention, inchi, target)\u001b[0m\n\u001b[1;32m    272\u001b[0m                                       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# inchi_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                                       \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#tokens_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                                       None] #probs_array  # attention scores\n\u001b[0m\u001b[1;32m    275\u001b[0m                     )\n\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m                   \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__module__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'in a future version'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                   if date is None else ('after %s' % date), instructions)\n\u001b[0;32m--> 602\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     doc = _add_deprecated_arg_value_notice_to_docstring(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop_v2\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, maximum_iterations, name)\u001b[0m\n\u001b[1;32m   2539\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m       return_same_structure=True)\n\u001b[0m\u001b[1;32m   2542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   2775\u001b[0m                                               list(loop_vars))\n\u001b[1;32m   2776\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   2766\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   2767\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 2768\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2769\u001b[0m       \u001b[0mtry_to_pack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-108-e316ffe7a11c>\u001b[0m in \u001b[0;36mloop_fn\u001b[0;34m(step, continue_cond, inchi_step, tokens_array, probs_array)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0;31m# create mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             mask = tf.pad(mask, [[0, self.max_len - step - 1], \n\u001b[1;32m    203\u001b[0m                                  [0, self.max_len - step - 1]])\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mones\u001b[0;34m(shape, dtype, name)\u001b[0m\n\u001b[1;32m   3222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m       \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure it's a vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3224\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3225\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    174\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_arg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback_arg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# False values do not suppress exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yanU6T8tf1ht"
      },
      "source": [
        "#%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsmcfhvEvdUP"
      },
      "source": [
        "Test inference speed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EMrmPRzvTEr"
      },
      "source": [
        "#\"\"\"\n",
        "# Base Model: inference speed (no beam)\n",
        "\n",
        "%%timeit\n",
        "\n",
        "num_batches = 3  # processing time for 3 * 256 = 768 images\n",
        "for val2 in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(1):\n",
        "    im_id, preds = (model_inference.predict(val))\n",
        "#\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFBKRQowIWqq"
      },
      "source": [
        "\"\"\"\n",
        "# Full model: inference speed (with beam)\n",
        "%%timeit\n",
        "num_batches = 3\n",
        "\n",
        "for val in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(num_batches): \n",
        "    im_id, preds = (model_base.predict(val))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otxdN02mf1ht"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-9tox8gf1hu"
      },
      "source": [
        "# learning rate scheduler\n",
        "\n",
        "learning_rate = LRScheduleAIAYN(scale_factor=1, warmup_steps=4000)\n",
        "#learning_rate = 1e-3\n",
        "\n",
        "# optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate,\n",
        "                    beta_1=0.9, beta_2=0.98, epsilon=10e-9)  # params from \"Attention is All You Need\" paper\n",
        "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)  # speeds training on GPU\n",
        "\n",
        "# callbacks\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(CHECKPOINT_FILE, monitor='loss', \n",
        "                    save_weights_only=True, save_best_only=True, save_freq='epoch')\n",
        "\n",
        "#tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN8MF9xt4uqG"
      },
      "source": [
        "Train base model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdeuljeJf1h1"
      },
      "source": [
        "# Train base model (teacher-fed training, prediction-fed validation, no beam update)\n",
        "\n",
        "# Note: if training decoders segments, initialize second decoder weights using:\n",
        "# \"model_base.decoder_1.set_weights(model_base.decoder_0.get_weights())\"\n",
        "\n",
        "steps_per_epoch = 150\n",
        "epochs = len(train_labels_df) // steps_per_epoch  # one full pass through the dataset\n",
        "\n",
        "# choose variables to train\n",
        "model_base.image_encoder.get_layer('dense').trainable=True\n",
        "model_base.decoder_0.trainable = True\n",
        "#model_base.decoder_1.trainable = True\n",
        "\n",
        "# compile\n",
        "model_base.compile(optimizer=optimizer, \n",
        "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "            metrics=['sparse_categorical_accuracy', EditDistanceMetric()])\n",
        "\n",
        "# train\n",
        "model_base.fit(train_ds, \n",
        "               epochs=epochs, steps_per_epoch=steps_per_epoch, \n",
        "               validation_data=valid_ds, validation_steps=10, validation_freq=6,\n",
        "               callbacks=[checkpoint],#, tensorboard], \n",
        "               verbose=2, use_multiprocessing=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s1Lgn1e4wxz"
      },
      "source": [
        "Train beam update model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otz8JJdB3QZ6"
      },
      "source": [
        "\"\"\"\n",
        "# train beam model (prediction-fed training and inference, includes beam update mech)\n",
        "\n",
        "steps_per_epoch = 150\n",
        "epochs = len(train_labels_df) // steps_per_epoch  # one full pass through the dataset\n",
        "\n",
        "# sync weights\n",
        "model_beam.load_weights(PARAMETERS.load_checkpoint_dir() + 'checkpoints')\n",
        "print('Loaded saved weights')\n",
        "\n",
        "# choose variables to train\n",
        "model_base.decoder_0.trainable = True\n",
        "model_base.decoder_1.trainable = True  # if multiple decoders enabled\n",
        "\n",
        "# compile\n",
        "model_beam.compile(optimizer=optimizer, \n",
        "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['sparse_categorical_accuracy', EditDistanceMetric()])\n",
        "\n",
        "# train\n",
        "model_beam.fit(train_ds, epochs=30, steps_per_epoch=20, \n",
        "               validation_data=valid_ds, validation_steps=3, validation_freq=5,\n",
        "               callbacks=[checkpoint],#, tensorboard], \n",
        "               verbose=2, use_multiprocessing=True)\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbvzr5rdmjgs"
      },
      "source": [
        "# Inference\n",
        "\n",
        "Here we define function to conduct inference on the test set. Results are saved to \"submission.csv\".\n",
        "\n",
        "Intermediate results are saved at regular intervals to. This allows inference to be conducted in stages and is a safeguard in case of interruptions before the full set has been processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVm4YazBsFIB"
      },
      "source": [
        "def make_inference_progress(predictions_df, skip_recorded=True, save_freq=50, parameters=PARAMETERS):\n",
        "\n",
        "    # initialize model and build with inference batch size (test_ds)\n",
        "    model_base = BaseTrainer(embedding_dim=EMBEDDING_DIM, rnn_units=RNN_DIM, \n",
        "                             beam_rnn_units=BEAM_RNN_UNITS, \n",
        "                             parameters=PARAMETERS, name='BaseTrainer')\n",
        "    for val in test_ds.take(1):\n",
        "        model_base(val)\n",
        "    model_full = InchiGenerator(model_base, name='InchiGenerator')\n",
        "    for val in test_ds.take(1):\n",
        "        model_full(val)\n",
        "\n",
        "    # load saved weights\n",
        "    model_full.load_weights(PARAMETERS.load_checkpoint_dir() + 'checkpoints')\n",
        "    print('Loaded model')\n",
        "\n",
        "    batch_size = 1024\n",
        "    if skip_recorded:\n",
        "        existing_batches = len(predictions_df) // batch_size\n",
        "    else:\n",
        "        existing_batches = 0\n",
        "    i = 0\n",
        "\n",
        "    for val in test_ds.skip(existing_batches):\n",
        "        test_im_id, test_pred = model_full.predict(val, return_lev_score=False)\n",
        "\n",
        "        # decode bytestrings\n",
        "        test_im_id = [x.decode()[:-4] for x in test_im_id.numpy().tolist()]  # drops '.png'\n",
        "        test_pred = [x.decode() for x in test_pred.numpy().tolist()]\n",
        "\n",
        "        new_preds = pd.DataFrame({'image_id': test_im_id, \n",
        "                                  'InChI': test_pred})\n",
        "\n",
        "        \n",
        "        predictions_df = predictions_df.append(new_preds)\n",
        "\n",
        "        # save to CSV\n",
        "        if i % save_freq == 0:\n",
        "            predictions_df = predictions_df.drop_duplicates(subset='image_id', keep='last')\n",
        "            predictions_df.to_csv(PARAMETERS.csv_save_dir() + 'submission.csv', index=False)\n",
        "            print(f'iteration {i}')\n",
        "\n",
        "        i += 1\n",
        "\n",
        "    return predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xN9S2TyMy__W"
      },
      "source": [
        "Load previosuly generated predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6iSlSFmHy9mX"
      },
      "source": [
        "try:\n",
        "    predictions_df = pd.read_csv(PARAMETERS.csv_save_dir() + 'submission.csv')\n",
        "except:\n",
        "    predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlq0OYsqzHMA"
      },
      "source": [
        "Generate additional predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CplWdHecmiRQ"
      },
      "source": [
        "\"\"\" On first pass or to start from scratch, initialize the dataframe with:\n",
        "predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)\n",
        "\"\"\"\n",
        "\n",
        "predictions_df = make_inference_progress(predictions_df, save_freq=100, num_batches=1, starting_batch=0, parameters=PARAMETERS)\n",
        "predictions_df"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}