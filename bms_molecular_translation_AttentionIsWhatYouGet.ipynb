{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bms_molecular_translation_AttentionIsWhatYouGet.ipynb","provenance":[],"collapsed_sections":["htI5m-W2uJmx"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/mvenouziou/Project-Attention-Is-What-You-Get/blob/main/bms_molecular_translation_AttentionIsWhatYouGet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"HvUYADthV78M"},"source":["Load project"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"daKA-tX6MxXV","executionInfo":{"status":"ok","timestamp":1633625571884,"user_tz":240,"elapsed":2637,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"08ee32da-8434-437b-d55a-99e15df206c1"},"source":["!git clone https://github.com/mvenouziou/Project-Attention-Is-What-You-Get.git ./AttentionProject"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into './AttentionProject'...\n","remote: Enumerating objects: 69, done.\u001b[K\n","remote: Counting objects: 100% (69/69), done.\u001b[K\n","remote: Compressing objects: 100% (62/62), done.\u001b[K\n","remote: Total 69 (delta 32), reused 18 (delta 5), pack-reused 0\u001b[K\n","Unpacking objects: 100% (69/69), done.\n"]}]},{"cell_type":"markdown","metadata":{"id":"Zujis6hSUc0o"},"source":["# Attention is What You Get\n","\n","This is my entry into the [Bristol-Myers Squibb Molecular Translation](https://www.kaggle.com/c/bms-molecular-translation)  Kaggle competition.\n","\n","-----\n","\n","AUTHOR: \n","\n","Mo Venouziou\n","\n","- *Email: mvenouziou@gmail.com*\n","- *LinkedIn: www.linkedin.com/in/movenouziou/*\n","\n","Updates:\n","\n"," - *Original Posting: June 2, 2021*\n"," - *06/21/21: added TPU compatability*\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"jvew4SZuM33i"},"source":["### Our Goal: Predict the \"InChI\" value of any given chemical compound diagram. \n","\n","International Chemical Identifiers (\"InChI values\") are a standardized encoding to describe chemical compounds. They take the form of a string of letters, numbers and deliminators, often between 100 - 400 characters long. \n","\n","The chemical diagrams are provided as PNG files, often of such low quality that it may take a human several seconds to decipher. \n","\n","Label length and image quality become a serious challenge here, because we must predict labels for a very large quantity of images. There are 1.6 million images in the test set abd 2.4 million images available in the training set!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"-ELuJxxoUsQF","executionInfo":{"status":"ok","timestamp":1633625752298,"user_tz":240,"elapsed":184,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"5595439f-ccaf-4bb5-a318-82fb2d19e173"},"source":["print('Given a chemical compound image, can we determine its InChI value string?\\n')\n","with open('./AttentionProject/sample_inchi_value.txt') as f:\n","    print('Sample InChI Value:\\n', f.read(), '\\n', sep='')\n","\n","from PIL import Image\n","print('Sample InChI Diagram:')\n","Image.open('./AttentionProject/sample_InChI_diagram.png')"],"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Given a chemical compound image, can we determine its InChI value string?\n","\n","Sample InChI Value:\n","InChI=1S/C14H15BrN2O3/c1-3-4-10-12(18)16-14(20)17(13(10)19)11-7-9(15)6-5-8(11)2/h5-7,10H,3-4H2,1-2H3,(H,16,18,20)\n","\n","Sample InChI Diagram:\n"]},{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<PIL.PngImagePlugin.PngImageFile image mode=RGBA size=257x252 at 0x7F3DF72B2590>"]},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"xTzwOF_khY65"},"source":["## MODEL STRUCTURE: \n","\n","**Image CNN + Attention Features encoder --> text Attention + (optional )CNN feature layer decoder.**\n","\n","This is a hybrid approach with:\n"," \n"," - Overall strategy from [*Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*](https://proceedings.mlr.press/v37/xuc15.pdf).  Generate image feature vectors using a CNN then decode using an attention-aware process. (The paper uses an LSTM decoder, but this notebook uses Attention Transformers instead.)\n"," \n"," - Transformer encoder-decoder model from [*Attention Is All You Need*](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Self-attention feature extraction for both encoder and decoder, joint encoder-decoder attention feature interactions, and a dense prediction output block. Includes parameters to control number of encoder / decoder blocks.)\n","\n"," - ***PLUS*** *(optional):* Decoder Output Blocks placed in Series (not stacked). Increase the number of trainable parameters without adding inference computational complexity, while also allowing decoders to specialize on different regions of the output.\n"," \n"," - ***PLUS*** *(optional):* Is attention really all you need? Add a convolutional layer to enhance text features before decoder self-attention to experiment with performance differences with and without extra convolutional layer(s). Use of CNN's in NLP comes from [*Convolutional Sequence to Sequence Learning*](http://proceedings.mlr.press/v70/gehring17a.html.)\n","\n"," - ***PLUS*** *(optional):* Beam-Search Alternative, an extra decoding layer applied after the full logits prediction has been made. This takes the form of a bidirectional RNN with attention, applied to the full logits sequence. Because a full (initial) prediction has already been made, computations can be parallelized using statefull RNNs. (See more details below.)\n","\n","----\n","\n","## TPU Training:\n","\n"," - With over 1 million imgaes, training on TPU or distributed over multiple graphics cards is the only reasonable approach. \n"," - TPU requires data to stored in Google Cloud Storage buckets. My data is prepared and stored as TF Record Shards on GCS through Kaggle. \n"," - Kaggle periodically changes GCS dataset URL's. When this happens the URL must be manually updated in the parameters.py file.\n","\n","----\n","\n","## NEXT STEPS:\n","\n"," - Experiment with **\"Tokens-to-Token ViT\"** in place of the image CNN. (Technique from [*Training Vision Transformers from Scratch on ImageNet*](https://arxiv.org/pdf/2101.11986.pdf)\n","  \n"," - Train my **Beam-search Alternative**. \n","\n","    - Beam search is a technique to modify model predictions to reflect the (local) maximum likelihood estimate. However, it is *very* local in that computation expense increases quickly with the number of character steps taken into account. This is also a hard-coded algorithm, which is somewhat contrary to the philosophy of deep learning.\n","\n","    - A *Beam-search Alternative* would be an extra decoding layer applied *after* the full logits prediction has been made. This might be in the form of a stateful, bidirectional RNN that is computationally parallizable because it is applied to the full logits sequence.\n","\n","    - Need to revamp code to accept main model changes made for TPU support.\n","\n"," - Treat the number of convolutional layers (decoder feature extraction) and number of decoders places in series (decoder prediction output) as **new hyperparameters** to tune.\n","\n","----"]},{"cell_type":"markdown","metadata":{"id":"htI5m-W2uJmx"},"source":["### CITATIONS\n","\n","- \"Attention is All You Need.\" \n"," - Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. NIPS (2017). *https://research.google/pubs/pub46201/*\n","\n","- \"Convolutional Sequence to Sequence Learning.\"\n"," \n","  - Gehring, J., Auli, M., Grangier, D., Yarats, D. & Dauphin, Y.N.. (2017). Convolutional Sequence to Sequence Learning. Proceedings of the 34th International Conference on Machine Learning, in Proceedings of Machine Learning Research 70:1243-1252, *http://proceedings.mlr.press/v70/gehring17a.html.*\n","\n","\n","- \"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.\"\n"," \n","  - Mingxing Tan, Quoc V. Le (2019). Convolutional Sequence to Sequence Learning. International Conference on Machine Learning. *http://arxiv.org/abs/1905.11946.*\n","\n","\n","-  \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\"\n","  -  Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R. & Bengio, Y.. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. Proceedings of the 32nd International Conference on Machine Learning, in Proceedings of Machine Learning Research 37:2048-2057. *http://proceedings.mlr.press/v37/xuc15.html.* \n","            \n","\n","- \"Tokens-to-Token ViT: Training Vision Transformers from Scratch on ImageNet\"\n","\n","  - Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay, Jiashi Feng, Shuicheng Yan. Preprint (2021). *https://arxiv.org/abs/2101.11986*.\n","\n","- Tensorflow documentation tutorial \"Transformer model for language understanding.\" I found this after fully completing the model and found the attention mask was incorrect. My use of \"tf.linalg.band_part\" (only) is due to this tutorial. *www.tensorflow.org/text/tutorials/transformer#masking*\n","\n","- Special thanks to:\n","\n"," -  [Darien Schettler](https://www.kaggle.com/dschettler8845/bms-efficientnetv2-tpu-e2e-pipeline-in-3hrs/notebook.) for leading readers to the \"Show\" and \"Attention\" papers cited above, using *session.run()* to improve inference speed in distributed settings and providing detailed info on creating TF Records. I also utilized his scores as performance benchmarks, testing against his results and experimenting with models that sometimes included his hyper-parameter choices, most significantly by dropping the encoder attention component altogether. (My model allows this by setting encoder_heads=0.) This work is otherwise derived independently from his.\n","\n"," - [Qishen Ha Team](https://www.kaggle.com/c/bms-molecular-translation/discussion/243943) for sharing their structure results. I switched to larger input image dimensions based on their success with much larger values than I had previously used. However image and model size was *significantly* smaller than theirs due to Colab memory allowances vs their top of the line hardware.\n","\n","- It is possible my idea of a Beam Search Alternative is based on a lecture video from DeepLearning.ai's [Deep Learning Specialization](https://www.coursera.org/specializations/deep-learning)  on Coursera.\n","\n","- **Dataset / Kaggle Competition:** \"Bristol-Myers Squibb – Molecular Translation\" competition on Kaggle (2021). *https://www.kaggle.com/c/bms-molecular-translation*\n","\n","----\n"]},{"cell_type":"markdown","metadata":{"id":"csshw-ehbeQY"},"source":["## Contents\n","\n","1. [Imports](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=TjuUOVXao__C&line=4&uniqifier=1)\n","2. [Data Pipeline](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=lrLHKs5Ni7Sz)\n","3. [Model Layers](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=W0T-u0vZamI8)\n","    - [InChI Encoding](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=DYApmA2lf1hp&line=1&uniqifier=1)\n","    - [Image Encoding and Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=FESofcGdEaWF&line=1&uniqifier=1)\n","    - [Decoder Self-Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=6qFDs9RTjvod&line=1&uniqifier=1)\n","    - [Joint Encoder-Decoder Attention](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=jP-t1MkKnD5L)\n","    - [Decoder Head (Prediction Output)](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=38GA7wtNEhqW&line=1&uniqifier=1)\n","    - [Update Mechanism](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=_2UR1DLljD0S&line=1&uniqifier=1)\n","4. [Full Model](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=D6GIs3f3rpu0&line=1&uniqifier=1)\n","5. [Training](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=otxdN02mf1ht&line=1&uniqifier=1)\n","6. [Inference](https://colab.research.google.com/drive/1i6LMwu7BRfs955U4AdtV2oaI_9_A_Awq#scrollTo=Sbvzr5rdmjgs&line=5&uniqifier=1)\n","\n","---"]},{"cell_type":"code","metadata":{"id":"TjuUOVXao__C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625608273,"user_tz":240,"elapsed":17260,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"ac5bcd5c-45b7-4f50-c706-f2f69e224e9c"},"source":["#### PACKAGE IMPORTS ####\n","\n","# system management\n","import os\n","#os.environ['TF_ENABLE_ONEDNN.OPTS'] = '1'  # Intel's TF optimization\n","import sys\n","\n","# TF Model design\n","import tensorflow as tf\n","from tensorflow import keras\n","import tensorflow_hub as hub\n","from tensorflow.data import TFRecordDataset\n","from tensorflow.data.experimental import TFRecordWriter\n","\n","# Text processing\n","import re\n","import string\n","\n","# metric for Kaggle Competition\n","!pip install -q leven\n","from leven import levenshtein\n","\n","# Kaggle (for TPU)\n","#from kaggle_datasets import KaggleDatasets\n","\n","# Visualizations\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","from PIL import Image\n","\n","# Tensorboard Profiler\n","!pip install -U -q tensorboard\n","!pip install -U -q tensorboard_plugin_profile\n","!pip install --upgrade -q \"cloud-tpu-profiler>=2.3.0\"\n","%load_ext tensorboard\n","\n","# data management\n","import numpy as np\n","import pandas as pd\n","import itertools"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |██▏                             | 10 kB 29.4 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20 kB 15.1 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 40 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 51 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 61 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 71 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 92 kB 6.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 154 kB 5.4 MB/s \n","\u001b[?25h  Building wheel for leven (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.1 MB 4.6 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"iF4LtB61LYU4","executionInfo":{"status":"ok","timestamp":1633625608274,"user_tz":240,"elapsed":17,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"38c96c59-d5b8-4592-b2f0-a36f0886a467"},"source":["\"\"\"  if loading images from GDrive use this:\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\"\"\""],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"  if loading images from GDrive use this:\\nfrom google.colab import drive\\ndrive.mount('/content/drive')\\n\""]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"mdJxUTM2KyY7","executionInfo":{"status":"ok","timestamp":1633625608274,"user_tz":240,"elapsed":15,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["# Custom Imports\n","sys.path.insert(0, '../content/AttentionProject')\n","import parameters\n","import pipeline\n","import tf_record_creator\n","import tokenizers\n","import encoders\n","import transformers\n","import prediction_heads\n","import losses_metrics_learning_rates\n","import models"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUS3jc1o1Nq8"},"source":["## Model parameters\n","\n","The 'ModelParameters' class manages global hyperparamaters for portability between Colab and Kaggle notebook environments. Once set, all other cells will run on either platform.\n","\n","On Colab, connection to my personal Google Drive is required, as ModelParameters will extract the dataset from a zip file to the hosted environment. This process may take several minutes. (It would not be difficult for the reader to update the code to point to their own drive and download the zip dataset using the Kaggle API code below.)"]},{"cell_type":"code","metadata":{"id":"q6FUA_lvrd4Y","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1633625608275,"user_tz":240,"elapsed":16,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"555b4c17-f6a4-4f05-c667-bab36fe5590b"},"source":["\"\"\" Kaggle api for download the compressed dataset from Kaggle's servers.\n","\n","# imports\n","!pip uninstall -y kaggle\n","!pip install --upgrade pip\n","!pip install kaggle==1.5.6\n","\n","# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\n","# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\n","os.environ['KAGGLE_CONFIG_DIR'] = '/content/MyDrive/Kaggle'  # api token location\n","\"\"\""],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\" Kaggle api for download the compressed dataset from Kaggle's servers.\\n\\n# imports\\n!pip uninstall -y kaggle\\n!pip install --upgrade pip\\n!pip install kaggle==1.5.6\\n\\n# if needed, download data using '!kaggle competitions download -c bms-molecular-translation'\\n# then unzip with '! unzip bms-molecular-translation.zip -d datasets'\\nos.environ['KAGGLE_CONFIG_DIR'] = '/content/MyDrive/Kaggle'  # api token location\\n\""]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"8sFuCtyTUQwh"},"source":["Initialize Parameters, Mixed Precision and check for TPU"]},{"cell_type":"code","metadata":{"id":"_a301N_Gf1hl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625620185,"user_tz":240,"elapsed":11686,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"61830581-8898-4544-aca4-6a0012786d31"},"source":["PARAMETERS = parameters.ModelParameters(cloud_server='colab')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Clearing out eager caches\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.18.151.202:8470\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.18.151.202:8470\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Finished initializing TPU system.\n"]},{"output_type":"stream","name":"stdout","text":["All devices:  [LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:0/device:TPU:7', device_type='TPU')]\n","INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:Found TPU system:\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"]},{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]},{"output_type":"stream","name":"stderr","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"]}]},{"cell_type":"markdown","metadata":{"id":"lrLHKs5Ni7Sz"},"source":["# **Input Pipeline**"]},{"cell_type":"markdown","metadata":{"id":"kgq0W_hZUX-r"},"source":["Load train labels as DataFrame"]},{"cell_type":"code","metadata":{"id":"21sUOSyff1hm","executionInfo":{"status":"ok","timestamp":1633625620185,"user_tz":240,"elapsed":13,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["if not PARAMETERS.tpu():\n","    # Load CSV as dataframe\n","    train_labels_df = pd.read_csv(PARAMETERS.train_labels_csv())\n","    train_labels_df.head()"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QmcP1tFRrvyR"},"source":["Tokenizer\n","\n","Note: This must be kept outside the model (and used in dataset prep) for TPU compatability"]},{"cell_type":"code","metadata":{"id":"zw6ghpYe6l7W","executionInfo":{"status":"ok","timestamp":1633625620331,"user_tz":240,"elapsed":158,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["tokenizer_obj = tokenizers.Tokenizer(parameters=PARAMETERS)\n","\n","TOKENIZER_LAYER = tokenizer_obj.tokenizer_layer()\n","INVERSE_TOKENIZER = tokenizer_obj.inverse_tokenizer()\n","TOKENIZED_EOS = tokenizer_obj.tokenized_EOS()    "],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"No2ecfS_hR9k"},"source":["## Datasets\n","\n","Here we create efficient tf.data.Dataset train / validation / test sets.\n","\n","Out data pipeline will read our prepared CSV of (image filename, parsed InChI and standard InChI) tuples. (If this file is not found, it will be created from scratch. This may take several minutes)  Iterating through the list, it will load batches of corresponding images and labels.\n","\n","Our datasets contain the following information, accessible by dict keys: images, image_id, InChI, parsed_InChI. (The test set uses InChI = parsed_InChI = 'InChI=1S/', the known required stating value for any InChI code.)"]},{"cell_type":"markdown","metadata":{"id":"Ly1PolKXhNOy"},"source":["#### InChI Text Parsing\n","\n","We split each InChI label into its \"vocabulary\" of logical subunits, consisting of element abbreviations numbers, common symbols and the required string 'InChI=1S/', which is at the start of every InChI label. We want to narrow down this vocabulary to the smallest set represented in our training data. The functions below provide a system for finding this minimal set, as well as preparing a new CSV file with parsed labels ready to be fed into a tokenizer layer.\n","\n","(For clarity and to reduce reliance on loading external files, the true code has been commented out and replaced with corresponding hard-coded values.)"]},{"cell_type":"code","metadata":{"id":"wDRJElyOf1hn","executionInfo":{"status":"ok","timestamp":1633625620331,"user_tz":240,"elapsed":9,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["pipe = pipeline.Pipeline(parameters=PARAMETERS)\n","inchi_parser = pipe.inchi_parser"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KCaO3A_5f1ho"},"source":["Create Test, Train and Validation Datasets"]},{"cell_type":"code","metadata":{"id":"YvHSKTapob8V","executionInfo":{"status":"ok","timestamp":1633625620332,"user_tz":240,"elapsed":9,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["if not PARAMETERS.tpu():\n","    train_ds, valid_ds = pipe.data_generator('train', labels_df=train_labels_df, decode_images=True)\n","    \n","    # test dataset is slow to load because there are so many images\n","    #test_ds = pipe.data_generator('test', labels_df=None, decode_images=True)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juAkO-oQaUml"},"source":["Examine data shapes"]},{"cell_type":"code","metadata":{"id":"dKbpsKKrf1hp","executionInfo":{"status":"ok","timestamp":1633625620332,"user_tz":240,"elapsed":9,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["if not PARAMETERS.tpu():\n","\n","    print('Train DS')\n","    for val in train_ds.take(1):    \n","        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","\n","    print('\\nValidation DS')\n","    for val in valid_ds.take(1):\n","        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","\n","    try:\n","        print('\\nTest DS')\n","        for val in test_ds.take(1):\n","            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","    except:\n","        pass"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CeCYSm7N74i7"},"source":["### TF Records Implementation"]},{"cell_type":"code","metadata":{"id":"vQgYgaPoO4gp","executionInfo":{"status":"ok","timestamp":1633625620332,"user_tz":240,"elapsed":8,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["tf_rec = tf_record_creator.TFRecordCreator(PARAMETERS)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"VxuZInXxj8Fn","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1633625620333,"user_tz":240,"elapsed":9,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"04a73e7c-2ea0-405b-dec0-e9ecf6c6ce6a"},"source":["# To create TF_Records files\n","# note: can take 8+ hours for train set alone!\n","\"\"\"\n","serial_train_ds, serial_valid_ds = tf_rec.serialized_dataset_gen(set_type='train', labels_df=train_labels_df)\n","serial_test_ds = tf_rec.serialized_dataset_gen(set_type='test', labels_df=None)\n","\"\"\""],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\nserial_train_ds, serial_valid_ds = tf_rec.serialized_dataset_gen(set_type='train', labels_df=train_labels_df)\\nserial_test_ds = tf_rec.serialized_dataset_gen(set_type='test', labels_df=None)\\n\""]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"8SeQrwPyh1Ok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625640292,"user_tz":240,"elapsed":19967,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"d560a5a5-1b2e-4bbd-d542-3f835af79128"},"source":["# IF USING TF_RECORDS:\n","#NOTE: If no dataset loads, check if Kaggle GCS directories have changed. (This happends periodically)\n","if PARAMETERS.tpu():\n","    with PARAMETERS.strategy().scope(): \n","        train_ds = tf_rec.dataset_from_records('train')\n","        valid_ds = tf_rec.dataset_from_records('valid')\n","\n","    print('Train DS')\n","    for val in train_ds.take(1):    \n","        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","\n","    print('\\nValidation DS')\n","    for val in valid_ds.take(1):\n","        print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","              'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","\n","    try:\n","        print('\\nTest DS')\n","        for val in test_ds.take(1):\n","            print('image:', val['image'].shape, 'image_id:', val['image_id'].shape, \n","                'InChI:', val['InChI'].shape, 'tokenized_InChI:', val['tokenized_InChI'].shape)\n","    except:\n","        pass"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Train DS\n","image: (512, 320, 320, 3) image_id: (512,) InChI: (512,) tokenized_InChI: (512, 200)\n","\n","Validation DS\n","image: (512, 320, 320, 3) image_id: (512,) InChI: (512,) tokenized_InChI: (512, 200)\n","\n","Test DS\n"]}]},{"cell_type":"markdown","metadata":{"id":"W0T-u0vZamI8"},"source":["# **Model Layers**"]},{"cell_type":"code","metadata":{"id":"QzuTRK8JrxWx","executionInfo":{"status":"ok","timestamp":1633625640292,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["# sample params\n","temp_vocab_size = 50\n","temp_batch_size = 2\n","temp_num_chars = 100\n","temp_inchi_embedding_dim = 256\n","temp_image_target_size = [320,320]\n","temp_encoder_dim = 256\n","temp_encoder_att_heads = 8\n","temp_encoder_blocks = 2\n","temp_decoder_dim = 512\n","temp_decoder_att_heads = 5\n","temp_decoder_blocks = 2\n","\n","temp_inchi = tf.zeros([temp_batch_size, temp_num_chars])\n","temp_start_var = tf.zeros([temp_batch_size, 1, temp_inchi_embedding_dim])\n","temp_pos_encoding = tf.zeros([temp_batch_size, temp_num_chars, temp_inchi_embedding_dim])\n","temp_image = tf.zeros([temp_batch_size, 420,420,3])"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DYApmA2lf1hp"},"source":["## InChI Encoding\n","\n","Tokenizer and Embedding to convert parsed InChI strings to tensors of numbers. Includes (optional) masked convolutional layer as an extra preprocessing step"]},{"cell_type":"code","metadata":{"id":"wiiKyyo7Ev6J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625640442,"user_tz":240,"elapsed":163,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"ad53809b-bf12-46d2-c44e-4bf7b0c303a0"},"source":["inchi_encoder = encoders.InChIEncoder(vocab_size=temp_vocab_size, embedding_dim=temp_inchi_embedding_dim)\n","inchi_encoder(temp_inchi)\n","inchi_encoder.show_summary()"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"InChIEncoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","tokenized_inchi (InputLayer) [(None, 100)]             0         \n","_________________________________________________________________\n","tf.__operators__.getitem (Sl (None, 99)                0         \n","_________________________________________________________________\n","embedding (Embedding)        (None, 99, 256)           12800     \n","_________________________________________________________________\n","PrependStartVar (PrependStar (None, 100, 256)          256       \n","=================================================================\n","Total params: 13,056\n","Trainable params: 13,056\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"FESofcGdEaWF"},"source":["# Image CNN Encoder\n","\n","Feature Extraction Step 1: Run the images through a pre-trained image network, extracting features as the output of an intermediate convolutional layer. [Technique from \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention,\" cited at the top of this notebook.]  A dense layer is added for transfer learning and to control the dimension of the attention mechanism used later."]},{"cell_type":"code","metadata":{"id":"BdTKFUFKfHeU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625662124,"user_tz":240,"elapsed":21687,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"7ba1d93a-a42f-4c07-b7ed-c541e2b60706"},"source":["transfer_model = encoders.ImageEncoderBackbone(image_shape=temp_image_target_size)\n","transfer_model(temp_image)\n","transfer_model.show_summary()"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"ImageEncoderBackbone\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 448, 448, 3)]     0         \n","_________________________________________________________________\n","Resizing (Resizing)          (None, 320, 320, 3)       0         \n","_________________________________________________________________\n","tf.cast (TFOpLambda)         (None, 320, 320, 3)       0         \n","_________________________________________________________________\n","EfficientNet (Functional)    (None, 10, 10, 1536)      10783535  \n","=================================================================\n","Total params: 10,783,535\n","Trainable params: 10,696,232\n","Non-trainable params: 87,303\n","_________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"id":"SyEeXgr5f1hr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625662503,"user_tz":240,"elapsed":385,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"09a07031-6281-4576-a28e-d34e673ca9cc"},"source":["image_encoder = encoders.ImageDownscaler(encoder_dim=temp_encoder_dim)\n","image_encoder(tf.zeros([3, 10, 10, 1408]))\n","image_encoder.show_summary()"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"ImageDownscaler\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","image (InputLayer)           [(None, 10, 10, 1408)]    0         \n","_________________________________________________________________\n","Conv2D_1 (Conv2D)            (None, 10, 10, 256)       360704    \n","_________________________________________________________________\n","Conv2D_2 (Conv2D)            (None, 10, 10, 256)       65792     \n","=================================================================\n","Total params: 426,496\n","Trainable params: 426,496\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"XGYH_nP3Efsk"},"source":["## Encoder Attention\n","\n","Feature Extraction Step 2: Now that we have basic feature vectors, we use self-attention to generate more complex features. This is the encoding step used in \"Attention is All You Need,\" cited above. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n04-XEOvYM1H","executionInfo":{"status":"ok","timestamp":1633625662815,"user_tz":240,"elapsed":317,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"b9eca928-0df9-40e0-8caf-fb67ba5c9f85"},"source":["att = transformers.AttentionBlock(num_attention_heads=temp_encoder_att_heads)\n","att([tf.zeros([temp_batch_size,10,10,256]), # query\n","     tf.zeros([temp_batch_size,10,10,256]), # key\n","     tf.zeros([temp_batch_size,10,10,256])],  # value\n","    attention_mask=None)  # value\n","att.show_summary()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","query (InputLayer)              [(None, 10, 10, 256) 0                                            \n","__________________________________________________________________________________________________\n","key (InputLayer)                [(None, 10, 10, 256) 0                                            \n","__________________________________________________________________________________________________\n","value (InputLayer)              [(None, 10, 10, 256) 0                                            \n","__________________________________________________________________________________________________\n","AttentionLayer (MultiHeadAttent (None, 10, 10, 256)  263168      query[0][0]                      \n","                                                                 key[0][0]                        \n","                                                                 value[0][0]                      \n","__________________________________________________________________________________________________\n","Add (Add)                       (None, 10, 10, 256)  0           query[0][0]                      \n","                                                                 AttentionLayer[0][0]             \n","__________________________________________________________________________________________________\n","LayerNorm (LayerNormalization)  (None, 10, 10, 256)  512         Add[0][0]                        \n","==================================================================================================\n","Total params: 263,680\n","Trainable params: 263,680\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7GDukTbzZE2F","executionInfo":{"status":"ok","timestamp":1633625662987,"user_tz":240,"elapsed":174,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"04c8be39-9265-48f2-fa85-d0bc70a009c0"},"source":["feed = transformers.FeedForwardBlock()\n","feed(tf.zeros([temp_batch_size, 100, 256]))  # query\n","feed.show_summary()"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","features (InputLayer)           [(None, 100, 256)]   0                                            \n","__________________________________________________________________________________________________\n","DenseRelu (Dense)               (None, 100, 256)     65792       features[0][0]                   \n","__________________________________________________________________________________________________\n","DenseLinear (Dense)             (None, 100, 256)     65792       DenseRelu[0][0]                  \n","__________________________________________________________________________________________________\n","Dropout (Dropout)               (None, 100, 256)     0           DenseLinear[0][0]                \n","__________________________________________________________________________________________________\n","DenseAdd (Add)                  (None, 100, 256)     0           features[0][0]                   \n","                                                                 Dropout[0][0]                    \n","__________________________________________________________________________________________________\n","DenseLayerNorm (LayerNormalizat (None, 100, 256)     512         DenseAdd[0][0]                   \n","==================================================================================================\n","Total params: 132,096\n","Trainable params: 132,096\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0wkXgTcvYM87","executionInfo":{"status":"ok","timestamp":1633625663620,"user_tz":240,"elapsed":637,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"37a44e23-192b-400d-9d3d-6face8fcfa36"},"source":["enc_Att = transformers.EncoderAttention(num_attention_heads=temp_encoder_att_heads, num_blocks=temp_encoder_blocks)\n","enc_Att(tf.zeros([temp_batch_size, 10,10,256]))\n","enc_Att.show_summary()"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"EncoderAttention\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 10, 10, 256) 0                                            \n","__________________________________________________________________________________________________\n","Flatten2D (Reshape)             (None, 100, 256)     0           input_2[0][0]                    \n","__________________________________________________________________________________________________\n","AddPositional (AddPositional)   (None, 100, 256)     25600       Flatten2D[0][0]                  \n","__________________________________________________________________________________________________\n","SelfAttention_0 (AttentionBlock (None, 100, 256)     263680      AddPositional[0][0]              \n","                                                                 AddPositional[0][0]              \n","                                                                 AddPositional[0][0]              \n","__________________________________________________________________________________________________\n","FeedForward_0 (FeedForwardBlock (None, 100, 256)     132096      SelfAttention_0[0][0]            \n","__________________________________________________________________________________________________\n","SelfAttention_1 (AttentionBlock (None, 100, 256)     263680      FeedForward_0[0][0]              \n","                                                                 FeedForward_0[0][0]              \n","                                                                 FeedForward_0[0][0]              \n","__________________________________________________________________________________________________\n","FeedForward_1 (FeedForwardBlock (None, 100, 256)     132096      SelfAttention_1[0][0]            \n","==================================================================================================\n","Total params: 817,152\n","Trainable params: 817,152\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"6qFDs9RTjvod"},"source":["## Decoder Attention\n","\n","Text Feature extraction + Encoder/Decoder Joint Attention interaction. This is implemented as a subclassed model (not Functional API) for extra flexibility in input shapes. This will allow for a quicker inference loop."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gXqs2BTki9ro","executionInfo":{"status":"ok","timestamp":1633625664867,"user_tz":240,"elapsed":1251,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"c09d41fa-7fc7-45a0-955c-bcc7d752a36a"},"source":["dec_Att = transformers.DecoderAttention(num_attention_heads=temp_decoder_att_heads, num_blocks=temp_decoder_blocks)\n","dec_Att([tf.zeros([temp_batch_size, 100, temp_encoder_dim]), \n","         tf.zeros([temp_batch_size, temp_num_chars, temp_decoder_dim])])\n","dec_Att.show_summary()"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"DecoderAttention\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","decoding (InputLayer)           [(None, 100, 512)]   0                                            \n","__________________________________________________________________________________________________\n","AddPositional (AddPositional)   (None, 100, 512)     51200       decoding[0][0]                   \n","__________________________________________________________________________________________________\n","SelfAttention_0 (AttentionBlock (None, 100, 512)     1047546     AddPositional[0][0]              \n","                                                                 AddPositional[0][0]              \n","                                                                 AddPositional[0][0]              \n","__________________________________________________________________________________________________\n","encoding (InputLayer)           [(None, 100, 256)]   0                                            \n","__________________________________________________________________________________________________\n","JointAttention_0 (AttentionBloc (None, 100, 512)     786426      SelfAttention_0[0][0]            \n","                                                                 encoding[0][0]                   \n","                                                                 encoding[0][0]                   \n","__________________________________________________________________________________________________\n","FeedForward_0 (FeedForwardBlock (None, 100, 512)     526336      JointAttention_0[0][0]           \n","__________________________________________________________________________________________________\n","SelfAttention_1 (AttentionBlock (None, 100, 512)     1047546     FeedForward_0[0][0]              \n","                                                                 FeedForward_0[0][0]              \n","                                                                 FeedForward_0[0][0]              \n","__________________________________________________________________________________________________\n","JointAttention_1 (AttentionBloc (None, 100, 512)     786426      SelfAttention_1[0][0]            \n","                                                                 encoding[0][0]                   \n","                                                                 encoding[0][0]                   \n","__________________________________________________________________________________________________\n","FeedForward_1 (FeedForwardBlock (None, 100, 512)     526336      JointAttention_1[0][0]           \n","==================================================================================================\n","Total params: 4,771,816\n","Trainable params: 4,720,616\n","Non-trainable params: 51,200\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"38GA7wtNEhqW"},"source":["## Decoder Head (Prediction Output)\n","\n","This is where we use what was learned in the encoder-decoder attention to output predicted labels. It is the prediction step from \"Attention is All You Need.\""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Zk1fflvocG-","executionInfo":{"status":"ok","timestamp":1633625664951,"user_tz":240,"elapsed":127,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"5f7e8da9-362c-4682-fa5a-302ba2a9fcd3"},"source":["dec_head = prediction_heads.DecoderHead(vocab_size=temp_vocab_size, dual_heads_split_step=None)\n","dec_head(tf.zeros([temp_batch_size, temp_num_chars, temp_decoder_dim]))\n","print(dec_head.show_summary())\n","\n","print()\n","\n","dec_head = prediction_heads.DecoderHead(vocab_size=temp_vocab_size, dual_heads_split_step=25)\n","dec_head(tf.zeros([temp_batch_size, temp_num_chars, temp_decoder_dim]))\n","print(dec_head.show_summary())"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"DecoderHead\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","features (InputLayer)        [(None, 100, 512)]        0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 100, 50)           25650     \n","_________________________________________________________________\n","softmax_7 (Softmax)          (None, 100, 50)           0         \n","=================================================================\n","Total params: 25,650\n","Trainable params: 25,650\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Model: \"DecoderHead\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","features (InputLayer)        [(None, 100, 512)]        0         \n","_________________________________________________________________\n","SplitPred (SplitPred)        (None, 100, 50)           51300     \n","_________________________________________________________________\n","softmax_8 (Softmax)          (None, 100, 50)           0         \n","=================================================================\n","Total params: 51,300\n","Trainable params: 51,300\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}]},{"cell_type":"markdown","metadata":{"id":"_2UR1DLljD0S"},"source":["## Update Mechanism (Optional)\n","\n","*Note: this is fully coded but I have not had time to train parameters with it. I leave that as a future opportunity for exploration.*\n","\n","NLP technicques typically output logits to find the highest likelhood token prediction. This can be improved to a (local) maximum likelihood selection using a \"beam step\" that ay override the initial prediction choice. \n","\n","This layer is an alternative system for updating predictions. Unlike \"beam,\" it is trainable and includes longer-range dependencies (instead of the very \"local\" beam step.) The entire original prediction is passed through a bidirectional RNN (decoder feature extraction) followed by AIAYN stye attention blocks. No masking is needed since we have the full RNN output to work with."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2wySOkwkvEuJ","executionInfo":{"status":"ok","timestamp":1633625666311,"user_tz":240,"elapsed":1364,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"42b7c198-25a4-408f-92c1-f160f1cdf6e2"},"source":["beam = prediction_heads.BeamUpdate(num_attention_heads=6, vocab_size=temp_vocab_size)\n","beam([tf.zeros([temp_batch_size, temp_num_chars, temp_vocab_size]),\n","      tf.zeros([temp_batch_size, 100, temp_encoder_dim])])\n","beam.show_summary()"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","orig_predictions (InputLayer)   [(None, 100, 50)]    0                                            \n","__________________________________________________________________________________________________\n","BeamUnit (GRU)                  (None, 100, 50)      15300       orig_predictions[0][0]           \n","__________________________________________________________________________________________________\n","SelfAttention (AttentionBlock)  (None, 100, 50)      9894        BeamUnit[0][0]                   \n","                                                                 BeamUnit[0][0]                   \n","                                                                 BeamUnit[0][0]                   \n","__________________________________________________________________________________________________\n","encoder_features (InputLayer)   [(None, 100, 256)]   0                                            \n","__________________________________________________________________________________________________\n","JointAttention (AttentionBlock) (None, 100, 50)      29670       SelfAttention[0][0]              \n","                                                                 encoder_features[0][0]           \n","                                                                 encoder_features[0][0]           \n","__________________________________________________________________________________________________\n","FeedForwardLayer (FeedForwardBl (None, 100, 50)      5200        JointAttention[0][0]             \n","__________________________________________________________________________________________________\n","DecoderHead (DecoderHead)       (None, 100, 50)      2550        FeedForwardLayer[0][0]           \n","==================================================================================================\n","Total params: 62,614\n","Trainable params: 62,614\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"D6GIs3f3rpu0"},"source":["# **Full Model**\n","\n","All the components are combined into a full encoder/decoder model. This is implemented using the subclassing API with custom call, train,  evaluation and prediction steps. Once initialized, the models have full access to high-level model.fit(), model.compile() and model.save_weights() methods.\n","\n","An extra features implemented is having Decoder() elements in *series* (not stacked). This adds more trainable parameters without affecting inference speed, and allows decoders to specialize more on different regions of the text."]},{"cell_type":"markdown","metadata":{"id":"MUtswyx2ru4O"},"source":["## Build Model"]},{"cell_type":"markdown","metadata":{"id":"yhZAEoiSYsZb"},"source":["Model Parameters"]},{"cell_type":"code","metadata":{"id":"sDez379bi8Xy","executionInfo":{"status":"ok","timestamp":1633625666311,"user_tz":240,"elapsed":6,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["NAME_MODIFIER = ''\n","\n","# build model\n","ENCODER_DIM = 384\n","DECODER_DIM = 512  # # \"All You Need is Attention\" uses 512 units\n","\n","# Note: model has capacity for up to 6 encoder and 6 decoder blocks. (as in AISAYN base model)\n","ENCODER_BLOCKS = 2  # note: can set to 0 to skip encoder block\n","DECODER_BLOCKS = 3  # max 6 enc and 6 dec with Colab memory constraints and cuts elsewhere\n","DUAL_DECODERS_Step = 75\n","USE_CONVOLUTIONS = False\n","if USE_CONVOLUTIONS:\n","    checkpoint_save_name = 'ConvAtt_model_checkpoints' + NAME_MODIFIER\n","else:\n","    checkpoint_save_name = 'AISAYN_model_checkpoints' + NAME_MODIFIER\n","\n","LOAD_CHECKPOINT_FILE = os.path.join(PARAMETERS.load_checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n","SAVE_CHECKPOINT_FILE = os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, checkpoint_save_name)\n","\n","STEPS_PER_EXECUTION = 8\n","if PARAMETERS.tpu():  \n","    STEPS_PER_EXECUTION = 8 * PARAMETERS.strategy().num_replicas_in_sync\n","\n","# note: in Kaggle,\n","# LOAD_CHECKPOINT_FILE points to saved outputs from prev session\n","# SAVE_CHECKPOINT_FILE points to saved outputs from current session"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h7s5jU6OYwax"},"source":["Initialize model"]},{"cell_type":"code","metadata":{"id":"dNg-2SotL8YE","executionInfo":{"status":"ok","timestamp":1633625666311,"user_tz":240,"elapsed":5,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["# Update inputs: remove string keys, as they are not compatible with TPU\n","def remove_names(val):\n","    return val['image'], val['tokenized_InChI'], val['image_id'], val['InChI']\n","\n","train_ds_int_index = train_ds.map(remove_names)\n","valid_ds_int_index = valid_ds.map(remove_names)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"mR9NhMEoNDHi","executionInfo":{"status":"ok","timestamp":1633625666312,"user_tz":240,"elapsed":6,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["# callbacks\n","checkpoint = tf.keras.callbacks.ModelCheckpoint(SAVE_CHECKPOINT_FILE, monitor='loss', \n","        save_weights_only=True, save_best_only=False, save_freq=1000,\n","        options=tf.train.CheckpointOptions(experimental_io_device='/job:localhost'))\n","\n","nan_stop = tf.keras.callbacks.TerminateOnNaN()"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjQALfcqR2y9","executionInfo":{"status":"ok","timestamp":1633625666458,"user_tz":240,"elapsed":151,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["def compile_model(model=None, load_checkpoint=True, lr_scale_factor=1.0, label_smoothing=.1):\n","\n","    # compile using distribution strategy\n","    with PARAMETERS.strategy().scope():\n","     \n","        # initialize if no model provided\n","        if model is None:\n","\n","            model = models.MolecularTranslator(encoder_blocks=ENCODER_BLOCKS, \n","                                               encoder_dim=ENCODER_DIM, \n","                                               decoder_blocks=DECODER_BLOCKS, \n","                                               decoder_dim=DECODER_DIM, \n","                                               dual_heads_split_step=DUAL_DECODERS_Step,\n","                                               use_convolutions=USE_CONVOLUTIONS,\n","                                               parameters=PARAMETERS)\n","\n","            if PARAMETERS.tpu():  \n","        \n","                temp_ds = PARAMETERS.strategy().experimental_distribute_dataset(train_ds_int_index)\n","                temp_ds = iter(temp_ds)\n","                val = next(temp_ds)\n","\n","                # build with new val (inference and training modes)\n","                temp_func_train = tf.function(func=lambda x: model(x, True), experimental_relax_shapes=True,\n","                                        experimental_follow_type_hints=True)\n","                temp_func_inference = tf.function(func=lambda x: model(x, False), experimental_relax_shapes=True,\n","                                        experimental_follow_type_hints=True)\n","\n","                PARAMETERS.strategy().run(temp_func_train, args=[(val[0], val[1])])  # use strategy.run() on TPU\n","                    \n","            else:  \n","                # build with original val\n","                for val in train_ds_int_index.take(1): \n","                    model(val, training=True)\n","\n","            # show summary\n","            print(model.summary())\n","            print('Models initialized.')\n","        \n","        # compiler components\n","        # cyclic modification to AIAYN lr\n","        learning_rate = losses_metrics_learning_rates.LRScheduleAIAYN(\n","                                                        scale_factor=lr_scale_factor, \n","                                                        warmup_steps=5000)  \n","\n","        optimizer = tf.keras.optimizers.Adam(learning_rate,  # params from AIAYN\n","                                             beta_1=0.9, beta_2=0.98, epsilon=10e-9)\n","        \n","        # metrics\n","        edit_dist_metric = losses_metrics_learning_rates.EditDistanceMetric()\n","        \n","        # loss with label smoothing\n","        loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing)\n","\n","        # optimizations\n","        tf.config.optimizer.set_jit(\"autoclustering\")  # XLA compiler optimization\n","\n","        if not PARAMETERS.tpu():\n","            os.environ['TF_GPU_THREAD_MODE'] = 'gpu_private'  # better balances CPU / GPU interaction in tf.data    \n","            optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)  # required with mixed precision on GPU / CPU. Not used on TPU\n","\n","            # compile       \n","            model.compile(optimizer=optimizer, \n","                            loss=loss_fn,\n","                            metrics=['categorical_accuracy', edit_dist_metric],\n","                            steps_per_execution=STEPS_PER_EXECUTION)\n","        else:\n","            # compile (note: EditDistance metric not compatible with TPU)\n","            model.compile(optimizer=optimizer, \n","                          loss=loss_fn,\n","                          metrics=['categorical_accuracy'],\n","                          steps_per_execution=STEPS_PER_EXECUTION)\n","\n","    if load_checkpoint:\n","        # verify model calls & methods work\n","        if not PARAMETERS.tpu():\n","            model(val, training=False)\n","\n","            # sync weights\n","            # WARNING!: in Kaggle this loads from prev session saved weights\n","            try:\n","                with PARAMETERS.strategy().scope(): \n","                    model.load_weights(LOAD_CHECKPOINT_FILE)  \n","                    pass\n","            except:\n","                print('No weights loaded')  \n","\n","        else:\n","            # sync weights\n","            # WARNING!: in Kaggle this loads from prev session saved weights\n","            try:\n","                with PARAMETERS.strategy().scope(): \n","                    model.load_weights(LOAD_CHECKPOINT_FILE, \n","                                       options=tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\"))  \n","\n","            except:\n","                print('No weights loaded')    \n","\n","    return model"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"8WTIcK4vR5rv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625694905,"user_tz":240,"elapsed":28453,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"27caf23e-d695-4140-fd48-2d495fb16165"},"source":["base_model = compile_model(model=None, load_checkpoint=True, \n","                           lr_scale_factor=500.0, label_smoothing=.1)"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"MolecularTranslator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","text_vectorization (TextVect multiple                  0         \n","_________________________________________________________________\n","string_lookup_1 (StringLooku multiple                  0 (unused)\n","_________________________________________________________________\n","InChIEncoder (InChIEncoder)  multiple                  102400    \n","_________________________________________________________________\n","ImageEncoderBackbone (ImageE multiple                  10783535  \n","_________________________________________________________________\n","ImageDownscaler (ImageDownsc multiple                  738048    \n","_________________________________________________________________\n","EncoderAttention (EncoderAtt multiple                  1815552   \n","_________________________________________________________________\n","DecoderAttention (DecoderAtt multiple                  7598080   \n","_________________________________________________________________\n","DecoderHead (DecoderHead)    multiple                  204174    \n","=================================================================\n","Total params: 21,241,789\n","Trainable params: 21,052,086\n","Non-trainable params: 189,703\n","_________________________________________________________________\n","None\n","Models initialized.\n"]}]},{"cell_type":"markdown","metadata":{"id":"oL0ub_P2ovDL"},"source":["Inference Functions"]},{"cell_type":"code","metadata":{"id":"EGAGTWVKouN8"},"source":["def inference(model, dataset, take_num=None, skip_set_num=0, show_sample=False):\n","\n","    with model.parameters.strategy().scope():\n","\n","        generation_fn = model.call\n","               \n","        # select batches\n","        dataset = dataset.skip(skip_set_num)\n","        if take_num is not None:\n","            dataset = dataset.take(take_num)\n","        \n","        # distribute dataset\n","        distributed_ds = model.parameters.strategy().experimental_distribute_dataset(dataset)\n","        distributed_ds = iter(distributed_ds)\n","\n","        # initialize containers\n","        image_ids_list = []\n","        generated_predictions_list = []\n","        true_InChI_list = []\n","\n","        for val in distributed_ds:\n","            \n","            # unpack ds element (and distribute across replicas if needed)\n","            inputs = (val[0], val[1])  # (image, tokenized InChI)\n","            \n","            image_ids = val[2]\n","            if model.parameters.tpu():\n","                image_ids = model.parameters.strategy().gather(image_ids, axis=0)\n","            \n","            true_InChI = val[3]\n","            if model.parameters.tpu():\n","                true_InChI = model.parameters.strategy().gather(true_InChI, axis=0)\n","\n","            # generate probs\n","            generated_probs = model.parameters.strategy().run(generation_fn, args=[inputs])  # training=False\n","            if model.parameters.tpu():\n","                generated_probs = model.parameters.strategy().gather(generated_probs, axis=0)\n","                generated_probs = tf.squeeze(generated_probs)\n","\n","            # get predicted InChI\n","            generated_predictions = tf.argmax(generated_probs, axis=2)\n","            \n","            # convert predictions to strings\n","            generated_predictions = model.tokens_to_string(generated_predictions)\n","\n","            # decode bytestrings and update containers\n","            image_ids_list.extend([x.decode() for x in image_ids.numpy().tolist()])\n","            generated_predictions_list.extend([x.decode() for x in generated_predictions.numpy().tolist()])\n","            true_InChI_list.extend([x.decode() for x in true_InChI.numpy().tolist()])\n","\n","        # compute Levenshtein scores\n","        lev_score_list = [levenshtein(pred, orig) for (pred, orig)\n","                         in zip(generated_predictions_list, true_InChI_list)]\n","\n","    if show_sample:\n","        print(f'Mean Lev Score: {np.mean(lev_score_list)}\\n')\n","        for i in range(0, 100, 5):\n","            print(generated_predictions_list[i])\n","            print(true_InChI_list[i])\n","            print(lev_score_list[i])\n","            print()\n","\n","    return image_ids_list, generated_predictions_list, true_InChI_list, lev_score_list\n","\n","temp_results = inference(base_model, dataset=train_ds_int_index, take_num=1, show_sample=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xsmcfhvEvdUP"},"source":["Test inference speed"]},{"cell_type":"code","metadata":{"id":"m6BmY63-QSwJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633625803986,"user_tz":240,"elapsed":31135,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"1803989f-c20e-4797-9198-6a1c251c2b78"},"source":["#\"\"\"\n","# test inference speed - time for 'take_num' batches\n","%timeit inference(base_model, dataset=train_ds_int_index, take_num=2, skip_set_num=0)\n","#\"\"\""],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["1 loop, best of 5: 4.86 s per loop\n"]}]},{"cell_type":"code","metadata":{"id":"yanU6T8tf1ht","executionInfo":{"status":"ok","timestamp":1633625803986,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["if not PARAMETERS.tpu():\n","    tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs/')\n","    %tensorboard --logdir './logs/'"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFBKRQowIWqq","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1633625803987,"user_tz":240,"elapsed":15,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}},"outputId":"951addd6-bb72-4eec-ad9d-e4212975070a"},"source":["\"\"\"\n","# Full model: inference speed (with beam)\n","%%timeit\n","num_batches = 3\n","\n","for val in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(num_batches): \n","    im_id, preds = (base_model.predict(val))\n","\"\"\""],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n# Full model: inference speed (with beam)\\n%%timeit\\nnum_batches = 3\\n\\nfor val in train_ds.unbatch().batch(PARAMETERS.inference_batch_size()).take(num_batches): \\n    im_id, preds = (base_model.predict(val))\\n'"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"otxdN02mf1ht"},"source":["# Training"]},{"cell_type":"markdown","metadata":{"id":"Pnw2vkOhCFXM"},"source":["Prepare dataset for training"]},{"cell_type":"code","metadata":{"id":"xa48CRQtCFkN","executionInfo":{"status":"ok","timestamp":1633625803988,"user_tz":240,"elapsed":15,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["# note: one-hot needed so we can use label smoothing in CrossEntropy Loss\n","train_ds_prepared = train_ds_int_index\n","valid_ds_prepared = valid_ds_int_index\n","\n","# create one-hot encoded targets (allows label smoothing)\n","depth = base_model.vocab_size\n","one_hot = keras.layers.Lambda(lambda x: tf.one_hot(x, depth=depth)) \n","\n","train_ds_prepared = train_ds_prepared.map(lambda w, x, y, z: ((w, x), one_hot(x)),\n","                                          num_parallel_calls=tf.data.AUTOTUNE)\\\n","                                          .prefetch(tf.data.AUTOTUNE)\n","\n","valid_ds_prepared = valid_ds_int_index.map(lambda w, x, y, z: ((w, x), one_hot(x)),\n","                                          num_parallel_calls=tf.data.AUTOTUNE)\\\n","                                          .prefetch(tf.data.AUTOTUNE)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WN8MF9xt4uqG"},"source":["Train base model"]},{"cell_type":"code","metadata":{"id":"MdeuljeJf1h1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2bfe2bb1-821e-47a9-fa36-6af9a5d93e05"},"source":["\"\"\"\n","Note: validation results use inference mode (char-by-char generation), \n","while training uses teacher-fed inputs.\n","Note: graph can take a few minutes for initial build\n","\"\"\"\n","\n","# set training params\n","if not PARAMETERS.tpu():\n","    validation_steps = 512\n","    callbacks=[checkpoint, nan_stop, tensorboard]\n","    validation_freq = 4\n","\n","else:\n","    validation_steps = 512\n","    callbacks=[checkpoint, nan_stop]\n","    validation_freq = 4\n","\n","# set number of epochs\n","epochs = 13\n","steps_per_epoch = int(1.2 * 1e6) // PARAMETERS.batch_size()\n","\n","# Begin Training\n","history = base_model.fit(train_ds_prepared.repeat(),\n","                         validation_data=valid_ds_prepared.repeat(),\n","                         epochs=epochs,\n","                         steps_per_epoch=steps_per_epoch,\n","                         validation_freq=validation_freq, \n","                         validation_steps=validation_steps, \n","                         callbacks=callbacks,\n","                         verbose=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/13\n"]}]},{"cell_type":"markdown","metadata":{"id":"G7y-PoC1ztE7"},"source":["TPU-safe saving to local directory"]},{"cell_type":"code","metadata":{"id":"PDaC2XbXnSS4","executionInfo":{"status":"aborted","timestamp":1633625743558,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["base_model.save_weights(os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, 'saved_model'), \n","                        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oz66tXyM3LY6","executionInfo":{"status":"aborted","timestamp":1633625743558,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["base_model.load_weights(os.path.join(PARAMETERS.checkpoint_dir(), checkpoint_save_name, 'saved_model'), \n","                        options=tf.saved_model.SaveOptions(experimental_io_device='/job:localhost'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4s1Lgn1e4wxz"},"source":["Train beam update"]},{"cell_type":"code","metadata":{"id":"otz8JJdB3QZ6","executionInfo":{"status":"aborted","timestamp":1633625743558,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["\"\"\"\n","Not yet implemented\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sbvzr5rdmjgs"},"source":["# Inference\n","\n","Here we define function to conduct inference on the test set. Results are saved to \"submission.csv\".\n","\n","Intermediate results are saved at regular intervals to. This allows inference to be conducted in stages and is a safeguard in case of interruptions before the full set has been processed. "]},{"cell_type":"code","metadata":{"id":"mVm4YazBsFIB","executionInfo":{"status":"aborted","timestamp":1633625743559,"user_tz":240,"elapsed":14,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["def make_inference_progress(dataset, model, return_lev_score=True, save_freq=50, parameters=PARAMETERS):\n","\n","    batch_size = 1024\n","    est_num_batches = 2*10e7 // batch_size\n","    take_num = 100\n","\n","    #initialize new dataframe\n","    predictions_df = pd.DataFrame(columns=['image_id', 'InChI', 'lev_score'])\n","\n","\n","    for i in range(int(est_num_batches // take_num)):\n","        try:\n","\n","             # get predictions\n","            inference_outputs = run_inference(model, dataset, return_lev_score=True, \n","                                              take_num=take_num, skip_set_num=i)\n","            \n","            im_id, pred, true_val, lev_score  = inference_outputs[:]\n","\n","            # add to dataframe\n","            new_preds = pd.DataFrame({'image_id': im_id, 'InChI': pred, 'lev_score': lev_score})\n","            predictions_df = predictions_df.append(new_preds)\n","\n","            # save to CSV\n","            if i % save_freq == 0:\n","                predictions_df = predictions_df.drop_duplicates(subset='image_id', keep='last')\n","                predictions_df[['image_id', 'InChI']].to_csv(PARAMETERS.csv_save_dir() + 'submission.csv', index=False)\n","                print(f'iteration {i}')\n","\n","        except:\n","            print(f'completed at step {i}')\n","            break\n","\n","    return predictions_df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xN9S2TyMy__W"},"source":["Load previosuly generated predictions"]},{"cell_type":"code","metadata":{"id":"6iSlSFmHy9mX","executionInfo":{"status":"aborted","timestamp":1633625743560,"user_tz":240,"elapsed":15,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["try:\n","    predictions_df = pd.read_csv(PARAMETERS.csv_save_dir() + 'submission.csv')\n","except:\n","    predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xlq0OYsqzHMA"},"source":["Generate additional predictions"]},{"cell_type":"code","metadata":{"id":"CplWdHecmiRQ","executionInfo":{"status":"aborted","timestamp":1633625743560,"user_tz":240,"elapsed":15,"user":{"displayName":"Mo Venouziou","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02927845103616843460"}}},"source":["\"\"\" On first pass or to start from scratch, initialize the dataframe with:\n","predictions_df = pd.DataFrame({'image_id':[], 'InChI':[]}, dtype=str)\n","\"\"\"\n","\n","predictions_df = make_inference_progress(predictions_df, save_freq=100, \n","                                         num_batches=1, starting_batch=0, \n","                                         parameters=PARAMETERS)\n","predictions_df"],"execution_count":null,"outputs":[]}]}